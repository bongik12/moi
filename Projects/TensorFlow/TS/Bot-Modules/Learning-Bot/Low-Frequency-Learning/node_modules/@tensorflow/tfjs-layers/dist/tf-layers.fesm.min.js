/**
 * @license
 * Copyright 2021 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
import{backend as t,util as e,serialization as n,tidy as s,sqrt as i,sum as r,mul as a,clipByValue as o,div as l,add as u,relu as h,max as c,tensor1d as p,min as d,slice as f,slice4d as g,slice3d as m,slice2d as y,slice1d as b,gather as w,tile as k,randomNormal as x,elu as v,abs as S,dropout as I,concat as N,fused as A,concat4d as C,concat3d as z,concat2d as D,concat1d as T,zeros as E,ones as F,scalar as $,randomUniform as _,truncatedNormal as L,eye as R,linalg as M,variable as O,dispose as B,nextFrame as P,keep as W,log as U,sub as K,mean as j,softmax as V,neg as q,fill as G,maximum as H,floor as J,oneHot as Z,softplus as X,onesLike as Y,greater as Q,equal as tt,argMax as et,logicalAnd as nt,where as st,train as it,memory as rt,cast as at,Tensor as ot,Optimizer as lt,io as ut,selu as ht,minimum as ct,sigmoid as pt,tanh as dt,logSoftmax as ft,leakyRelu as gt,prelu as mt,transpose as yt,conv2dTranspose as bt,conv1d as wt,conv3d as kt,separableConv2d as xt,depthwiseConv2d as vt,expandDims as St,reverse as It,unstack as Nt,stack as At,split as Ct,conv2d as zt,any as Dt,notEqual as Tt,zerosLike as Et,all as Ft,greaterEqual as $t,moments as _t,batchNorm2d as Lt,batchNorm3d as Rt,batchNorm4d as Mt,pad as Ot,maxPool as Bt,avgPool as Pt,maxPool3d as Wt,avgPool3d as Ut,squeeze as Kt}from"@tensorflow/tfjs-core";function jt(t){throw new Error(`'${t}' not yet implemented or not found in the registry. This kernel may not be supported by the tfjs backend you have chosen`)}function Vt(t,e){if(!t)throw new Error("string"==typeof e?e:e())}function qt(t,e,n=""){Vt(Jt(t,e),()=>n+` Shapes ${t} and ${e} must match`)}function Gt(t,e=[],n=!1){if(null==e&&(e=[]),Array.isArray(t)||Qt(t)&&!n)for(let s=0;s<t.length;++s)Gt(t[s],e,n);else e.push(t);return e}function Ht(t){if(0===t.length)return 1;let e=t[0];for(let n=1;n<t.length;n++)e*=t[n];return e}function Jt(t,e){if(t===e)return!0;if(null==t||null==e)return!1;if(t.length!==e.length)return!1;for(let n=0;n<t.length;n++)if(t[n]!==e[n])return!1;return!0}function Zt(t){return t%1==0}function Xt(t,e){return e<=t.length?t:t+" ".repeat(e-t.length)}function Yt(t,e){const n=e.length;return Vt((t=null==t?e.map((t,e)=>e):[].concat(t)).every(t=>t>=-n&&t<n),()=>`All values in axis param must be in range [-${n}, ${n}) but got axis `+t),Vt(t.every(t=>Zt(t)),()=>"All values in axis param must be integers but got axis "+t),t.map(t=>t<0?n+t:t)}function Qt(t){return t instanceof Float32Array||t instanceof Int32Array||t instanceof Uint8Array}function te(t){return"string"==typeof t||t instanceof String}function ee(t){return Array.isArray(t)?ee(t[0]):t instanceof Float32Array?"float32":t instanceof Int32Array||t instanceof Uint8Array?"int32":"number"==typeof t?"float32":te(t)?"string":function(t){return"boolean"==typeof t}(t)?"bool":"float32"}function ne(t){return!!(t&&t.constructor&&t.call&&t.apply)}function se(t){const e=t.length;if(e<2)return[];const n=new Array(e-1);n[e-2]=t[e-1];for(let s=e-3;s>=0;--s)n[s]=n[s+1]*t[s+1];return n}function ie(t,e){if(0===t.length)return e[0];const n=t.reduce((t,e)=>t*e);if(0===n)return[];if(n!==e.length)throw new Error(`[${t}] does not match the input size ${e.length}.`);return function t(e,n,s){const i=new Array;if(1===n.length){const t=n[0];for(let n=0;n<t;n++)i[n]=s[e+n]}else{const r=n[0],a=n.slice(1),o=a.reduce((t,e)=>t*e);for(let n=0;n<r;n++)i[n]=t(e+n*o,a,s)}return i}(0,t,e)}function re(t,e){const n=ae(t,e);for(let t=0;t<n.length;t++)n[t]=1;return n}function ae(t,e){if(null==e||"float32"===e||"complex64"===e)return new Float32Array(t);if("int32"===e)return new Int32Array(t);if("bool"===e)return new Uint8Array(t);throw new Error("Unknown data type "+e)}function oe(t){return t&&t.then&&"function"==typeof t.then}class le{constructor(t){this.global=t,this.flags={},this.flagRegistry={},this.urlFlags={},this.populateURLFlags()}setPlatform(t,e){null!=this.platform&&console.warn(`Platform ${this.platformName} has already been set. Overwriting the platform with ${e}.`),this.platformName=t,this.platform=e}registerFlag(t,e,n){if(this.flagRegistry[t]={evaluationFn:e,setHook:n},null!=this.urlFlags[t]){const e=this.urlFlags[t];console.warn(`Setting feature override from URL ${t}: ${e}.`),this.set(t,e)}}async getAsync(t){return t in this.flags||(this.flags[t]=await this.evaluateFlag(t)),this.flags[t]}get(t){if(t in this.flags)return this.flags[t];const e=this.evaluateFlag(t);if(oe(e))throw new Error(`Flag ${t} cannot be synchronously evaluated. Please use getAsync() instead.`);return this.flags[t]=e,this.flags[t]}getNumber(t){return this.get(t)}getBool(t){return this.get(t)}getFlags(){return this.flags}get features(){return this.flags}set(t,e){if(null==this.flagRegistry[t])throw new Error(`Cannot set flag ${t} as it has not been registered.`);this.flags[t]=e,null!=this.flagRegistry[t].setHook&&this.flagRegistry[t].setHook(e)}evaluateFlag(t){if(null==this.flagRegistry[t])throw new Error(`Cannot evaluate flag '${t}': no evaluation function found.`);return this.flagRegistry[t].evaluationFn()}setFlags(t){this.flags=Object.assign({},t)}reset(){this.flags={},this.urlFlags={},this.populateURLFlags()}populateURLFlags(){if(void 0===this.global||void 0===this.global.location||void 0===this.global.location.search)return;const t=function(t){const e={};return t.replace(/[?&]([^=?&]+)(?:=([^&]*))?/g,(t,...n)=>(function(t,e,n){t[decodeURIComponent(e)]=decodeURIComponent(n||"")}(e,n[0],n[1]),n.join("="))),e}(this.global.location.search);if("tfjsflags"in t){t.tfjsflags.split(",").forEach(t=>{const[e,n]=t.split(":");this.urlFlags[e]=function(t,e){if("true"===(e=e.toLowerCase())||"false"===e)return"true"===e;if(""+ +e===e)return+e;throw new Error(`Could not parse value flag value ${e} for flag ${t}.`)}(e,n)})}}}function ue(){return ce}let he,ce=null;function pe(){if(null==he){let t;if("undefined"!=typeof window)t=window;else if("undefined"!=typeof global)t=global;else if("undefined"!=typeof process)t=process;else{if("undefined"==typeof self)throw new Error("Could not find a global object");t=self}he=t}return he}function de(t,e){const n=function(){const t=pe();return null==t._tfGlobals&&(t._tfGlobals=new Map),t._tfGlobals}();if(n.has(t))return n.get(t);{const s=e();return n.set(t,s),n.get(t)}}const fe=de("kernelRegistry",()=>new Map),ge=de("gradRegistry",()=>new Map);function me(t,e){const n=function(t,e){return`${e}_${t}`}(t,e);return fe.get(n)}function ye(t){return ge.get(t)}function be(t){const e=fe.entries(),n=[];for(;;){const{done:s,value:i}=e.next();if(s)break;const[r,a]=i,[o]=r.split("_");o===t&&n.push(a)}return n}function we(t){const{kernelName:e}=t;ge.has(e)&&ue().getBool("DEBUG")&&console.warn(`Overriding the gradient for '${e}'`),ge.set(e,t)}function ke(t,e){if("string"===e)throw new Error("Cannot convert a string[] to a TypedArray");if(Array.isArray(t)&&(t=Gt(t)),ue().getBool("DEBUG")&&function(t,e){for(let n=0;n<t.length;n++){const s=t[n];if(isNaN(s)||!isFinite(s))throw Error(`A tensor of type ${e} being uploaded contains ${s}.`)}}(t,e),function(t,e){return t instanceof Float32Array&&"float32"===e||t instanceof Int32Array&&"int32"===e||t instanceof Uint8Array&&"bool"===e}(t,e))return t;if(null==e||"float32"===e||"complex64"===e)return new Float32Array(t);if("int32"===e)return new Int32Array(t);if("bool"===e){const e=new Uint8Array(t.length);for(let n=0;n<e.length;++n)0!==Math.round(t[n])&&(e[n]=1);return e}throw new Error("Unknown data type "+e)}function xe(){return ue().platform.now()}function ve(t,e="utf-8"){return e=e||"utf-8",ue().platform.decode(t,e)}class Se{constructor(t,e){this.backendTimer=t,this.logger=e,null==e&&(this.logger=new Ne)}profileKernel(t,e,n){let s;const i=this.backendTimer.time(()=>{s=n()});if(ue().getBool("CHECK_COMPUTATION_FOR_ERRORS"))for(let e=0;e<s.length;e++){const n=s[e];n.data().then(e=>{Ie(e,n.dtype,t)})}return{kernelName:t,outputs:s,inputs:e,timeMs:i.then(t=>t.kernelMs),extraInfo:i.then(t=>null!=t.getExtraProfileInfo?t.getExtraProfileInfo():"")}}logKernelProfile(t){const{kernelName:e,outputs:n,timeMs:s,inputs:i,extraInfo:r}=t;n.forEach(t=>{Promise.all([t.data(),s,r]).then(n=>{this.logger.logKernelProfile(e,t,n[0],n[1],i,n[2])})})}}function Ie(t,e,n){if("float32"!==e)return!1;for(let e=0;e<t.length;e++){const s=t[e];if(isNaN(s)||!isFinite(s))return console.warn(`Found ${s} in the result of '${n}'`),!0}return!1}class Ne{logKernelProfile(t,e,n,s,i,r){const a="number"==typeof s?Xt(s+"ms",9):s.error,o=Xt(t,25),l=e.rank,u=e.size,h=Xt(e.shape.toString(),14);let c="";for(const t in i){const n=i[t];if(null!=n){const s=n.shape||e.shape,i=s.length;c+=`${t}: ${i}D ${i>0?s:""} `}}console.log(`%c${o}\t%c${a}\t%c${l}D ${h}\t%c${u}\t%c${c}\t%c${r}`,"font-weight:bold","color:red","color:blue","color: orange","color: green","color: steelblue")}}function Ae(t,e,n,s){const i=se(e),r=function(t,e,n,s){const i=Ht(e),r=s[s.length-1],a=new Array(r).fill(0),o=e.length,l="complex64"===n?De(t):t;if(o>1)for(let t=0;t<i/r;t++){const e=t*r;for(let t=0;t<r;t++)a[t]=Math.max(a[t],Ce(l[e+t],0,n).length)}return a}(t,e,n,i),a=e.length,o=function t(e,n,s,i,r,a=!0){const o="complex64"===s?2:1,l=n[0],u=n.length;if(0===u){if("complex64"===s){return[Ce(De(e)[0],0,s)]}return"bool"===s?[ze(e[0])]:[e[0].toString()]}if(1===u){if(l>20){const t=3*o;let n=Array.from(e.slice(0,t)),i=Array.from(e.slice((l-3)*o,l*o));return"complex64"===s&&(n=De(n),i=De(i)),["["+n.map((t,e)=>Ce(t,r[e],s)).join(", ")+", ..., "+i.map((t,e)=>Ce(t,r[l-3+e],s)).join(", ")+"]"]}return["["+("complex64"===s?De(e):Array.from(e)).map((t,e)=>Ce(t,r[e],s)).join(", ")+"]"]}const h=n.slice(1),c=i.slice(1),p=i[0]*o,d=[];if(l>20){for(let n=0;n<3;n++){const i=n*p,a=i+p;d.push(...t(e.slice(i,a),h,s,c,r,!1))}d.push("...");for(let n=l-3;n<l;n++){const i=n*p,a=i+p;d.push(...t(e.slice(i,a),h,s,c,r,n===l-1))}}else for(let n=0;n<l;n++){const i=n*p,a=i+p;d.push(...t(e.slice(i,a),h,s,c,r,n===l-1))}const f=2===u?",":"";d[0]="["+d[0]+f;for(let t=1;t<d.length-1;t++)d[t]=" "+d[t]+f;let g=",\n";for(let t=2;t<u;t++)g+="\n";return d[d.length-1]=" "+d[d.length-1]+"]"+(a?"":g),d}(t,e,n,i,r),l=["Tensor"];return s&&(l.push("  dtype: "+n),l.push("  rank: "+a),l.push(`  shape: [${e}]`),l.push("  values:")),l.push(o.map(t=>"    "+t).join("\n")),l.join("\n")}function Ce(t,e,n){let s;return s=Array.isArray(t)?parseFloat(t[0].toFixed(7))+" + "+parseFloat(t[1].toFixed(7))+"j":te(t)?`'${t}'`:"bool"===n?ze(t):parseFloat(t.toFixed(7)).toString(),Xt(s,e)}function ze(t){return 0===t?"false":"true"}function De(t){const e=[];for(let n=0;n<t.length;n+=2)e.push([t[n],t[n+1]]);return e}let Te=null;class Ee{constructor(t,e,n,s){this.kept=!1,this.isDisposedInternal=!1,this.shape=t.slice(),this.dtype=e||"float32",this.size=Ht(t),this.strides=se(t),this.dataId=n,this.id=s,this.rankType=this.rank<5?this.rank.toString():"higher"}get rank(){return this.shape.length}async buffer(){const t=await this.data();return null.buffer(this.shape,this.dtype,t)}bufferSync(){return null.buffer(this.shape,this.dtype,this.dataSync())}async array(){const t=await this.data();return ie(this.shape,t)}arraySync(){return ie(this.shape,this.dataSync())}async data(){this.throwIfDisposed();const t=Te().read(this.dataId);if("string"===this.dtype){const e=await t;try{return e.map(t=>ve(t))}catch(t){throw new Error("Failed to decode the string bytes into utf-8. To get the original bytes, call tensor.bytes().")}}return t}dataSync(){this.throwIfDisposed();const t=Te().readSync(this.dataId);if("string"===this.dtype)try{return t.map(t=>ve(t))}catch(t){throw new Error("Failed to decode the string bytes into utf-8. To get the original bytes, call tensor.bytes().")}return t}async bytes(){this.throwIfDisposed();const t=await Te().read(this.dataId);return"string"===this.dtype?t:new Uint8Array(t.buffer)}dispose(){this.isDisposed||(Te().disposeTensor(this),this.isDisposedInternal=!0)}get isDisposed(){return this.isDisposedInternal}throwIfDisposed(){if(this.isDisposed)throw new Error("Tensor is disposed.")}print(t=!1){return null.print(this,t)}clone(){return this.throwIfDisposed(),null.clone(this)}toString(t=!1){return Ae(this.dataSync(),this.shape,this.dtype,t)}cast(t){return this.throwIfDisposed(),null.cast(this,t)}variable(t=!0,e,n){return this.throwIfDisposed(),Te().makeVariable(this,t,e,n)}}function Fe(){return de("Tensor",()=>Ee)}Object.defineProperty(Ee,Symbol.hasInstance,{value:t=>!!t&&null!=t.data&&null!=t.dataSync&&null!=t.throwIfDisposed}),Fe();class $e extends Ee{constructor(t,e,n,s){super(t.shape,t.dtype,t.dataId,s),this.trainable=e,this.name=n}assign(t){if(t.dtype!==this.dtype)throw new Error(`dtype of the new value (${t.dtype}) and previous value (${this.dtype}) must match`);if(!Jt(t.shape,this.shape))throw new Error(`shape of the new value (${t.shape}) and previous value (${this.shape}) must match`);Te().disposeTensor(this),this.dataId=t.dataId,Te().incRef(this,null)}dispose(){Te().disposeVariable(this),this.isDisposedInternal=!0}}var _e,Le,Re,Me,Oe;Object.defineProperty($e,Symbol.hasInstance,{value:t=>t instanceof Ee&&null!=t.assign&&t.assign instanceof Function}),function(t){t.R0="R0",t.R1="R1",t.R2="R2",t.R3="R3",t.R4="R4",t.R5="R5",t.R6="R6"}(_e||(_e={})),function(t){t.float32="float32",t.int32="int32",t.bool="int32",t.complex64="complex64"}(Le||(Le={})),function(t){t.float32="float32",t.int32="int32",t.bool="bool",t.complex64="complex64"}(Re||(Re={})),function(t){t.float32="float32",t.int32="float32",t.bool="float32",t.complex64="complex64"}(Me||(Me={})),function(t){t.float32="complex64",t.int32="complex64",t.bool="complex64",t.complex64="complex64"}(Oe||(Oe={}));const Be={float32:Me,int32:Le,bool:Re,complex64:Oe};function Pe(t,e){if(t.dtype===e.dtype)return[t,e];const n=function(t,e){if("string"===t||"string"===e){if("string"===t&&"string"===e)return"string";throw new Error(`Can not upcast ${t} with ${e}`)}return Be[t][e]}(t.dtype,e.dtype);return[t.cast(n),e.cast(n)]}function We(t){const e=[];return function t(e,n,s){if(null==e)return;if(e instanceof Ee)return void n.push(e);if(i=e,!Array.isArray(i)&&"object"!=typeof i)return;var i;const r=e;for(const e in r){const i=r[e];s.has(i)||(s.add(i),t(i,n,s))}}(t,e,new Set),e}function Ue(t){return null!=t.kernelName}class Ke{constructor(){this.registeredVariables={},this.nextTapeNodeId=0,this.numBytes=0,this.numTensors=0,this.numStringTensors=0,this.numDataBuffers=0,this.gradientDepth=0,this.kernelDepth=0,this.scopeStack=[],this.numDataMovesStack=[],this.nextScopeId=0,this.tensorInfo=new WeakMap,this.profiling=!1,this.activeProfile={newBytes:0,newTensors:0,peakBytes:0,kernels:[],result:null,get kernelNames(){return Array.from(new Set(this.kernels.map(t=>t.name)))}}}dispose(){for(const t in this.registeredVariables)this.registeredVariables[t].dispose()}}class je{constructor(t){this.ENV=t,this.registry={},this.registryFactory={},this.pendingBackendInitId=0,this.state=new Ke}async ready(){if(null!=this.pendingBackendInit)return this.pendingBackendInit.then(()=>{});if(null!=this.backendInstance)return;const t=this.getSortedBackends();for(let e=0;e<t.length;e++){const n=t[e];if(await this.initializeBackend(n).success)return void await this.setBackend(n)}throw new Error("Could not initialize any backends, all backend initializations failed.")}get backend(){if(null!=this.pendingBackendInit)throw new Error(`Backend '${this.backendName}' has not yet been initialized. Make sure to await tf.ready() or await tf.setBackend() before calling other methods`);if(null==this.backendInstance){const{name:t,asyncInit:e}=this.initializeBackendsAndReturnBest();if(e)throw new Error(`The highest priority backend '${t}' has not yet been initialized. Make sure to await tf.ready() or await tf.setBackend() before calling other methods`);this.setBackend(t)}return this.backendInstance}backendNames(){return Object.keys(this.registryFactory)}findBackend(t){if(!(t in this.registry)){if(!(t in this.registryFactory))return null;{const{asyncInit:e}=this.initializeBackend(t);if(e)return null}}return this.registry[t]}findBackendFactory(t){return t in this.registryFactory?this.registryFactory[t].factory:null}registerBackend(t,e,n=1){return t in this.registryFactory?(console.warn(t+" backend was already registered. Reusing existing backend factory."),!1):(this.registryFactory[t]={factory:e,priority:n},!0)}async setBackend(t){if(null==this.registryFactory[t])throw new Error(`Backend name '${t}' not found in registry`);if(this.backendName=t,null==this.registry[t]){this.backendInstance=null;const{success:e,asyncInit:n}=this.initializeBackend(t);if(!(n?await e:e))return!1}return this.backendInstance=this.registry[t],this.setupRegisteredKernels(),this.profiler=new Se(this.backendInstance),!0}setupRegisteredKernels(){be(this.backendName).forEach(t=>{null!=t.setupFunc&&t.setupFunc(this.backendInstance)})}disposeRegisteredKernels(t){be(t).forEach(e=>{null!=e.disposeFunc&&e.disposeFunc(this.registry[t])})}initializeBackend(t){const e=this.registryFactory[t];if(null==e)throw new Error(`Cannot initialize backend ${t}, no registration found.`);try{const n=e.factory();if(!n||n instanceof class{decComplexRef(t){}time(t){return jt("time")}read(t){return jt("read")}readSync(t){return jt("readSync")}numDataIds(){return jt("numDataIds")}disposeData(t){return jt("disposeData")}write(t,e,n){return jt("write")}move(t,e,n,s){return jt("move")}memory(){return jt("memory")}floatPrecision(){return jt("floatPrecision")}epsilon(){return 32===this.floatPrecision()?1e-7:1e-4}dispose(){return jt("dispose")}}||"function"!=typeof n.then)return this.registry[t]=n,{success:!0,asyncInit:!1};{const e=++this.pendingBackendInitId,s=n.then(n=>!(e<this.pendingBackendInitId)&&(this.registry[t]=n,this.pendingBackendInit=null,!0)).catch(n=>(e<this.pendingBackendInitId||(this.pendingBackendInit=null,console.warn(`Initialization of backend ${t} failed`),console.warn(n.stack||n.message)),!1));return this.pendingBackendInit=s,{success:s,asyncInit:!0}}}catch(e){return console.warn(`Initialization of backend ${t} failed`),console.warn(e.stack||e.message),{success:!1,asyncInit:!1}}}removeBackend(t){if(!(t in this.registryFactory))throw new Error(t+" backend not found in registry");this.backendName===t&&null!=this.pendingBackendInit&&this.pendingBackendInitId++,t in this.registry&&(this.disposeRegisteredKernels(t),this.registry[t].dispose(),delete this.registry[t]),delete this.registryFactory[t],this.backendName===t&&(this.pendingBackendInit=null,this.backendName=null,this.backendInstance=null)}getSortedBackends(){if(0===Object.keys(this.registryFactory).length)throw new Error("No backend found in registry.");return Object.keys(this.registryFactory).sort((t,e)=>this.registryFactory[e].priority-this.registryFactory[t].priority)}initializeBackendsAndReturnBest(){const t=this.getSortedBackends();for(let e=0;e<t.length;e++){const n=t[e],{success:s,asyncInit:i}=this.initializeBackend(n);if(i||s)return{name:n,asyncInit:i}}throw new Error("Could not initialize any backends, all backend initializations failed.")}moveData(t,e){const n=this.state.tensorInfo.get(e),s=n.backend,i=this.readSync(e);s.disposeData(e),n.backend=t,t.move(e,i,n.shape,n.dtype),this.shouldCheckForMemLeaks()&&this.state.numDataMovesStack[this.state.numDataMovesStack.length-1]++}tidy(t,e){let n,s=null;if(null==e){if("function"!=typeof t)throw new Error("Please provide a function to tidy()");e=t}else{if("string"!=typeof t&&!(t instanceof String))throw new Error("When calling with two arguments, the first argument to tidy() must be a string");if("function"!=typeof e)throw new Error("When calling with two arguments, the 2nd argument to tidy() must be a function");s=t}return this.scopedRun(()=>this.startScope(s),()=>this.endScope(n),()=>(n=e(),n instanceof Promise&&console.error("Cannot return a Promise inside of tidy."),n))}scopedRun(t,e,n){t();try{const t=n();return e(),t}catch(t){throw e(),t}}nextTensorId(){return je.nextTensorId++}nextVariableId(){return je.nextVariableId++}clone(t){const e=this.makeTensorFromDataId(t.dataId,t.shape,t.dtype),n={x:t};return this.addTapeNode(this.state.activeScope.name,n,[e],t=>({x:()=>{const e={x:t},n={dtype:"float32"};return Ve.runKernel("Cast",e,n)}}),[],{}),e}runKernel(t,e,n){if(!(null!=me(t,this.backendName)))throw new Error(`Kernel '${t}' not registered for backend '${this.backendName}'`);return this.runKernelFunc({kernelName:t,inputs:e,attrs:n})}shouldCheckForMemLeaks(){return this.ENV.getBool("IS_TEST")}checkKernelForMemLeak(t,e,n){const s=this.backend.numDataIds();let i=0;n.forEach(t=>{i+="complex64"===t.dtype?3:1});const r=this.state.numDataMovesStack[this.state.numDataMovesStack.length-1],a=s-e-i-r;if(a>0)throw new Error(`Backend '${this.backendName}' has an internal memory leak (${a} data ids) after running '${t}'`)}runKernelFunc(t){let e,n=[];const s=this.isTapeOn(),i=this.state.numBytes,r=this.state.numTensors;let a,o;this.shouldCheckForMemLeaks()&&this.state.numDataMovesStack.push(0),null==this.backendName&&this.backend;const l=Ue(t)?t.kernelName:null!=this.state.activeScope?this.state.activeScope.name:"";if(Ue(t)){const{kernelName:e,inputs:i,attrs:r}=t;null==this.backendName&&this.backend;const l=me(e,this.backendName);Vt(null!=l,()=>`Cannot find registered kernel '${e}' for backend '${this.backendName}'`),a=()=>{const t=this.backend.numDataIds();o=l.kernelFunc({inputs:i,attrs:r,backend:this.backend});const a=Array.isArray(o)?o:[o];this.shouldCheckForMemLeaks()&&this.checkKernelForMemLeak(e,t,a);const u=a.map(t=>{if(null!=t.rank)return t;const{dataId:e,shape:n,dtype:s}=t;return this.makeTensorFromDataId(e,n,s)});if(s){const t=this.getTensorsForGradient(e,i,u);n=this.saveTensorsForBackwardMode(t)}return u}}else{const{forwardFunc:e}=t,i=t=>{s&&(n=t.map(t=>this.keep(this.clone(t))))};a=()=>{const t=this.backend.numDataIds();o=this.tidy(()=>e(this.backend,i));const n=Array.isArray(o)?o:[o];return this.shouldCheckForMemLeaks()&&this.checkKernelForMemLeak(l,t,n),n}}const{inputs:u,attrs:h}=t,c=Ue(t)?null:t.backwardsFunc;let p;return this.scopedRun(()=>this.state.kernelDepth++,()=>this.state.kernelDepth--,()=>{this.ENV.getBool("DEBUG")||this.state.profiling?(p=this.profiler.profileKernel(l,u,()=>a()),this.ENV.getBool("DEBUG")&&this.profiler.logKernelProfile(p),e=p.outputs):e=a()}),s&&this.addTapeNode(l,u,e,c,n,h),this.state.profiling&&this.state.activeProfile.kernels.push({name:l,bytesAdded:this.state.numBytes-i,totalBytesSnapshot:this.state.numBytes,tensorsAdded:this.state.numTensors-r,totalTensorsSnapshot:this.state.numTensors,inputShapes:Object.keys(u).map(t=>null!=u[t]?u[t].shape:null),outputShapes:e.map(t=>t.shape),kernelTimeMs:p.timeMs,extraInfo:p.extraInfo}),Array.isArray(o)?e:e[0]}saveTensorsForBackwardMode(t){return t.map(t=>this.keep(this.clone(t)))}getTensorsForGradient(t,e,n){const s=ye(t);if(null!=s){const t=s.inputsToSave||[],i=s.outputsToSave||[];let r;s.saveAllInputs?(Vt(Array.isArray(e),()=>"saveAllInputs is true, expected inputs to be an array."),r=Object.keys(e).map(t=>e[t])):r=t.map(t=>e[t]);const a=n.filter((t,e)=>i[e]);return r.concat(a)}return[]}makeTensor(t,e,n,s){if(null==t)throw new Error("Values passed to engine.makeTensor() are null");n=n||"float32",s=s||this.backend;let i=t;"string"===n&&te(t[0])&&(i=t.map(t=>function(t,e="utf-8"){return e=e||"utf-8",ue().platform.encode(t,e)}(t)));const r=s.write(i,e,n),a=new Ee(e,n,r,this.nextTensorId());if(this.incRef(a,s),"string"===n){const t=this.state.tensorInfo.get(r),e=function(t){if(null==t)return 0;let e=0;return t.forEach(t=>e+=t.length),e}(i);this.state.numBytes+=e-t.bytes,t.bytes=e}return a}makeTensorFromDataId(t,e,n,s){const i=new Ee(e,n=n||"float32",t,this.nextTensorId());return this.incRef(i,s),i}makeVariable(t,e=!0,n,s){n=n||this.nextVariableId().toString(),null!=s&&s!==t.dtype&&(t=t.cast(s));const i=new $e(t,e,n,this.nextTensorId());if(null!=this.state.registeredVariables[i.name])throw new Error(`Variable with name ${i.name} was already registered`);return this.state.registeredVariables[i.name]=i,this.incRef(i,this.backend),i}incRef(t,e){const n=this.state.tensorInfo.has(t.dataId)?this.state.tensorInfo.get(t.dataId).refCount:0;if(this.state.numTensors++,"string"===t.dtype&&this.state.numStringTensors++,0===n){this.state.numDataBuffers++;let n=0;"complex64"!==t.dtype&&"string"!==t.dtype&&(n=t.size*function(t){if("float32"===t||"int32"===t)return 4;if("complex64"===t)return 8;if("bool"===t)return 1;throw new Error("Unknown dtype "+t)}(t.dtype)),this.state.tensorInfo.set(t.dataId,{backend:e||this.backend,dtype:t.dtype,shape:t.shape,bytes:n,refCount:0}),this.state.numBytes+=n}this.state.tensorInfo.get(t.dataId).refCount++,t instanceof $e||this.track(t)}disposeTensor(t){if(!this.state.tensorInfo.has(t.dataId))return;this.state.numTensors--,"string"===t.dtype&&this.state.numStringTensors--;const e=this.state.tensorInfo.get(t.dataId);e.refCount<=1?("complex64"!==t.dtype&&(this.state.numBytes-=e.bytes),this.state.numDataBuffers--,e.backend.disposeData(t.dataId),this.state.tensorInfo.delete(t.dataId)):(e.backend.decComplexRef(t.dataId),this.state.tensorInfo.get(t.dataId).refCount--)}disposeVariables(){for(const t in this.state.registeredVariables){const e=this.state.registeredVariables[t];this.disposeVariable(e)}}disposeVariable(t){this.disposeTensor(t),null!=this.state.registeredVariables[t.name]&&delete this.state.registeredVariables[t.name]}memory(){const t=this.backend.memory();return t.numTensors=this.state.numTensors,t.numDataBuffers=this.state.numDataBuffers,t.numBytes=this.state.numBytes,this.state.numStringTensors>0&&(t.unreliable=!0,null==t.reasons&&(t.reasons=[]),t.reasons.push("Memory usage by string tensors is approximate (2 bytes per character)")),t}async profile(t){this.state.profiling=!0;const e=this.state.numBytes,n=this.state.numTensors;this.state.activeProfile.kernels=[],this.state.activeProfile.result=await t(),this.state.profiling=!1,this.state.activeProfile.peakBytes=Math.max(...this.state.activeProfile.kernels.map(t=>t.totalBytesSnapshot)),this.state.activeProfile.newBytes=this.state.numBytes-e,this.state.activeProfile.newTensors=this.state.numTensors-n;for(const t of this.state.activeProfile.kernels)t.kernelTimeMs=await t.kernelTimeMs,t.extraInfo=await t.extraInfo;return this.state.activeProfile}isTapeOn(){return this.state.gradientDepth>0&&0===this.state.kernelDepth}addTapeNode(t,e,n,s,i,r){const a={id:this.state.nextTapeNodeId++,kernelName:t,inputs:e,outputs:n,saved:i},o=ye(t);null!=o&&(s=o.gradFunc),null!=s&&(a.gradient=t=>(t=t.map((t,e)=>{if(null==t){const t=n[e],s=ae(t.size,t.dtype);return this.makeTensor(s,t.shape,t.dtype)}return t}),s(t.length>1?t:t[0],i,r))),this.state.activeTape.push(a)}keep(t){return t.kept=!0,t}startTape(){0===this.state.gradientDepth&&(this.state.activeTape=[]),this.state.gradientDepth++}endTape(){this.state.gradientDepth--}startScope(t){const e={track:[],name:"unnamed scope",id:this.state.nextScopeId++};t&&(e.name=t),this.state.scopeStack.push(e),this.state.activeScope=e}endScope(t){const e=We(t),n=new Set(e.map(t=>t.id));for(let t=0;t<this.state.activeScope.track.length;t++){const e=this.state.activeScope.track[t];e.kept||n.has(e.id)||e.dispose()}const s=this.state.scopeStack.pop();this.state.activeScope=0===this.state.scopeStack.length?null:this.state.scopeStack[this.state.scopeStack.length-1],e.forEach(t=>{t.kept||t.scopeId!==s.id||this.track(t)})}gradients(t,e,n,s=!1){if(Vt(e.length>0,()=>"gradients() received an empty list of xs."),null!=n&&"float32"!==n.dtype)throw new Error(`dy must have 'float32' dtype, but has '${n.dtype}'`);const i=this.scopedRun(()=>this.startTape(),()=>this.endTape(),()=>this.tidy("forward",t));Vt(i instanceof Ee,()=>"The result y returned by f() must be a tensor.");const r=function(t,e,n){const s={},i={};for(let t=0;t<e.length;t++)s[e[t].id]=!0;for(let n=0;n<t.length;n++){const r=t[n],a=r.inputs;for(const t in a){const n=a[t];let o=!1;for(let t=0;t<e.length;t++)if(s[n.id]){r.outputs.forEach(t=>s[t.id]=!0),o=!0,i[r.id]=!0;break}if(o)break}}const r={};r[n.id]=!0;const a={};for(let e=t.length-1;e>=0;e--){const n=t[e],s=n.inputs;for(let t=0;t<n.outputs.length;t++)if(r[n.outputs[t].id]){for(const t in s)r[s[t].id]=!0,a[n.id]=!0;break}}const o=[];for(let e=0;e<t.length;e++){const n=t[e];if(i[n.id]&&a[n.id]){const t={};for(const e in n.inputs){const i=n.inputs[e];s[i.id]&&(t[e]=i)}const e=Object.assign({},n);e.inputs=t,e.outputs=n.outputs,o.push(e)}}return o}(this.state.activeTape,e,i);if(!s&&0===r.length&&e.length>0)throw new Error("Cannot compute gradient of y=f(x) with respect to x. Make sure that the f you passed encloses all operations that lead from x to y.");return this.tidy("backward",()=>{const t={};t[i.id]=null==n?function(t){const e=re(Ht(t),"float32");return Ve.makeTensor(e,t,"float32")}(i.shape):n,function(t,e,n,s){for(let i=e.length-1;i>=0;i--){const r=e[i],a=[];if(r.outputs.forEach(e=>{const n=t[e.id];null!=n?a.push(n):a.push(null)}),null==r.gradient)throw new Error(`Cannot compute gradient: gradient function not found for ${r.kernelName}.`);const o=r.gradient(a);for(const e in r.inputs){if(!(e in o))throw new Error(`Cannot backprop through input ${e}. Available gradients found: ${Object.keys(o)}.`);const i=n(()=>o[e]());if("float32"!==i.dtype)throw new Error(`Error in gradient for op ${r.kernelName}. The gradient of input ${e} must have 'float32' dtype, but has '${i.dtype}'`);const a=r.inputs[e];if(!Jt(i.shape,a.shape))throw new Error(`Error in gradient for op ${r.kernelName}. The gradient of input '${e}' has shape '${i.shape}', which does not match the shape of the input '${a.shape}'`);if(null==t[a.id])t[a.id]=i;else{const e=t[a.id];t[a.id]=s(e,i),e.dispose()}}}}(t,r,t=>this.tidy(t),qe);const s=e.map(e=>t[e.id]);return 0===this.state.gradientDepth&&(this.state.activeTape.forEach(t=>{for(const e of t.saved)e.dispose()}),this.state.activeTape=null),{value:i,grads:s}})}customGrad(t){return Vt(ne(t),()=>"The f passed in customGrad(f) must be a function."),(...e)=>{let n;Vt(e.every(t=>t instanceof Ee),()=>"The args passed in customGrad(f)(x1, x2,...) must all be tensors");const s={};e.forEach((t,e)=>{s[e]=t});return this.runKernelFunc({forwardFunc:(s,i)=>(n=t(...e,i),Vt(n.value instanceof Ee,()=>"The function f passed in customGrad(f) must return an object where `obj.value` is a tensor"),Vt(ne(n.gradFunc),()=>"The function f passed in customGrad(f) must return an object where `obj.gradFunc` is a function."),n.value),backwardsFunc:(t,s)=>{const i=n.gradFunc(t,s),r=Array.isArray(i)?i:[i];Vt(r.length===e.length,()=>"The function f passed in customGrad(f) must return an object where `obj.gradFunc` is a function that returns the same number of tensors as inputs passed to f(...)."),Vt(r.every(t=>t instanceof Ee),()=>"The function f passed in customGrad(f) must return an object where `obj.gradFunc` is a function that returns a list of only tensors.");const a={};return r.forEach((t,e)=>{a[e]=()=>t}),a},inputs:s})}}readSync(t){return this.state.tensorInfo.get(t).backend.readSync(t)}read(t){return this.state.tensorInfo.get(t).backend.read(t)}async time(t){const e=xe(),n=await this.backend.time(t);return n.wallMs=xe()-e,n}track(t){return null!=this.state.activeScope&&(t.scopeId=this.state.activeScope.id,this.state.activeScope.track.push(t)),t}get registeredVariables(){return this.state.registeredVariables}reset(){this.pendingBackendInitId++,this.state.dispose(),this.ENV.reset(),this.state=new Ke;for(const t in this.registry)this.disposeRegisteredKernels(t),this.registry[t].dispose(),delete this.registry[t];this.backendName=null,this.backendInstance=null,this.pendingBackendInit=null}}je.nextTensorId=0,je.nextVariableId=0;const Ve=function(){const t=pe();if(null==t._tfengine){const e=new le(t);t._tfengine=new je(e)}var e;return e=t._tfengine.ENV,ce=e,Te=()=>t._tfengine,t._tfengine}();function qe(t,e){const n={a:t,b:e};return Ve.runKernel("Add",n)}function Ge(t,e){let n=t;if(Qt(t))return"string"===e?[]:[t.length];if(!Array.isArray(t))return[];const s=[];for(;Array.isArray(n)||Qt(n)&&"string"!==e;)s.push(n.length),n=n[0];return Array.isArray(t)&&ue().getBool("TENSORLIKE_CHECK_SHAPE_CONSISTENCY")&&function t(e,n,s){if(s=s||[],!Array.isArray(e)&&!Qt(e))return void Vt(0===n.length,()=>`Element arr[${s.join("][")}] is a primitive, but should be an array/TypedArray of ${n[0]} elements`);Vt(n.length>0,()=>`Element arr[${s.join("][")}] should be a primitive, but is an array of ${e.length} elements`),Vt(e.length===n[0],()=>`Element arr[${s.join("][")}] should have ${n[0]} elements, but has ${e.length} elements`);const i=n.slice(1);for(let n=0;n<e.length;++n)t(e[n],i,s.concat(n))}(t,s,[]),s}function He(t,e,n,s){if("string_or_numeric"!==t){if(null==t)throw new Error("Expected dtype cannot be null.");if("numeric"!==t&&t!==e||"numeric"===t&&"string"===e)throw new Error(`Argument '${n}' passed to '${s}' must be ${t} tensor, but got ${e} tensor`)}}function Je(t,e,n,s="numeric"){if(t instanceof Ee)return He(s,t.dtype,e,n),t;let i=ee(t);if("string"!==i&&["bool","int32","float32"].indexOf(s)>=0&&(i=s),He(s,i,e,n),null==t||!Qt(t)&&!Array.isArray(t)&&"number"!=typeof t&&"boolean"!=typeof t&&"string"!=typeof t){const s=null==t?"null":t.constructor.name;throw new Error(`Argument '${e}' passed to '${n}' must be a Tensor or TensorLike, but got '${s}'`)}const r=Ge(t,i);Qt(t)||Array.isArray(t)||(t=[t]);const a="string"!==i?ke(t,i):Gt(t,[],!0);return Ve.makeTensor(a,r,i)}function Ze(t,e,n,s="numeric"){if(!Array.isArray(t))throw new Error(`Argument ${e} passed to ${n} must be a \`Tensor[]\` or \`TensorLike[]\``);return t.map((t,i)=>Je(t,`${e}[${i}]`,n,s))}function Xe(t){const e=Object.keys(t);if(1!==e.length)throw new Error("Please provide an object with a single key (operation name) mapping to a function. Got an object with "+e.length+" keys.");let n=e[0];const s=t[n];n.endsWith("_")&&(n=n.substring(0,n.length-1)),n+="__op";const i=(...t)=>{Ve.startScope(n);try{const e=s(...t);return oe(e)&&console.error("Cannot return a Promise inside of tidy."),Ve.endScope(e),e}catch(t){throw Ve.endScope(null),t}};return Object.defineProperty(i,"name",{value:n,configurable:!0}),i}const Ye=Xe({abs_:function(t){const e=Je(t,"x","abs");if("complex64"===e.dtype){const t={x:e};return Ve.runKernel("ComplexAbs",t)}{const t={x:e};return Ve.runKernel("Abs",t)}}});const Qe=Xe({acos_:function(t){const e={x:Je(t,"x","acos")};return Ve.runKernel("Acos",e)}});const tn=Xe({acosh_:function(t){const e={x:Je(t,"x","acosh")};return Ve.runKernel("Acosh",e)}});const en=Xe({add_:function(t,e){let n=Je(t,"a","add"),s=Je(e,"b","add");[n,s]=Pe(n,s);const i={a:n,b:s};return Ve.runKernel("Add",i)}});const nn=Xe({all_:function(t,e=null,n=!1){const s={x:Je(t,"x","all","bool")},i={axis:e,keepDims:n};return Ve.runKernel("All",s,i)}});const sn=Xe({any_:function(t,e=null,n=!1){const s={x:Je(t,"x","any","bool")},i={axis:e,keepDims:n};return Ve.runKernel("Any",s,i)}});const rn=Xe({argMax_:function(t,e=0){const n={x:Je(t,"x","argMax")},s={axis:e};return Ve.runKernel("ArgMax",n,s)}});const an=Xe({argMin_:function(t,e=0){const n={x:Je(t,"x","argMin")},s={axis:e};return Ve.runKernel("ArgMin",n,s)}});const on=Xe({asin_:function(t){const e={x:Je(t,"x","asin")};return Ve.runKernel("Asin",e)}});const ln=Xe({asinh_:function(t){const e={x:Je(t,"x","asinh")};return Ve.runKernel("Asinh",e)}});const un=Xe({atan_:function(t){const e={x:Je(t,"x","atan")};return Ve.runKernel("Atan",e)}});const hn=Xe({atan2_:function(t,e){let n=Je(t,"a","atan2"),s=Je(e,"b","atan2");[n,s]=Pe(n,s);const i={a:n,b:s};return Ve.runKernel("Atan2",i)}});const cn=Xe({atanh_:function(t){const e={x:Je(t,"x","atanh")};return Ve.runKernel("Atanh",e)}});const pn=Xe({cast_:function(t,e){const n=Je(t,"x","cast");if(!function(t){return"bool"===t||"complex64"===t||"float32"===t||"int32"===t||"string"===t}(e))throw new Error("Failed to cast to unknown dtype "+e);if("string"===e&&"string"!==n.dtype||"string"!==e&&"string"===n.dtype)throw new Error("Only strings can be casted to strings");const s={x:n},i={dtype:e};return Ve.runKernel("Cast",s,i)}});function dn(t,e,n,s,i,r,a="channelsLast"){const[o,l]=fn(e);let u;if("channelsLast"===a)u=[o,l,t[3],t[3]];else{if("channelsFirst"!==a)throw new Error("Unknown dataFormat "+a);u=[o,l,t[1],t[1]]}return function(t,e,n,s,i,r,a=!1,o="channelsLast"){let[l,u,h,c]=[-1,-1,-1,-1];if("channelsLast"===o)[l,u,h,c]=t;else{if("channelsFirst"!==o)throw new Error("Unknown dataFormat "+o);[l,c,u,h]=t}const[p,d,,f]=e,[g,m]=fn(n),[y,b]=fn(s),w=gn(p,y),k=gn(d,b),{padInfo:x,outHeight:v,outWidth:S}=function(t,e,n,s,i,r,a,o,l){let u,h,c;if("number"==typeof t){u={top:t,bottom:t,left:t,right:t,type:0===t?"VALID":"NUMBER"};const i=function(t,e,n,s,i){null==s&&(s=function(t,e,n,s=1){const i=gn(e,s);return Math.floor((t[0]*(n-1)-n+i)/2)}(t,e,n));const r=t[0],a=t[1],o=mn((r-e+2*s)/n+1,i),l=mn((a-e+2*s)/n+1,i);return[o,l]}([e,n],r,s,t,o);h=i[0],c=i[1]}else if("same"===t){h=Math.ceil(e/s),c=Math.ceil(n/i);const t=Math.max(0,(h-1)*s+r-e),o=Math.max(0,(c-1)*i+a-n),l=Math.floor(t/2),p=t-l,d=Math.floor(o/2);u={top:l,bottom:p,left:d,right:o-d,type:"SAME"}}else if("valid"===t)u={top:0,bottom:0,left:0,right:0,type:"VALID"},h=Math.ceil((e-r+1)/s),c=Math.ceil((n-a+1)/i);else{if("object"!=typeof t)throw Error("Unknown padding parameter: "+t);{const p="channelsLast"===l?t[1][0]:t[2][0],d="channelsLast"===l?t[1][1]:t[2][1],f="channelsLast"===l?t[2][0]:t[3][0],g="channelsLast"===l?t[2][1]:t[3][1];u={top:p,bottom:d,left:f,right:g,type:0===p&&0===d&&0===f&&0===g?"VALID":"EXPLICIT"},h=mn((e-r+p+d)/s+1,o),c=mn((n-a+f+g)/i+1,o)}}return{padInfo:u,outHeight:h,outWidth:c}}(i,u,h,g,m,w,k,r,o),I=a?f*c:f;let N;"channelsFirst"===o?N=[l,I,v,S]:"channelsLast"===o&&(N=[l,v,S,I]);return{batchSize:l,dataFormat:o,inHeight:u,inWidth:h,inChannels:c,outHeight:v,outWidth:S,outChannels:I,padInfo:x,strideHeight:g,strideWidth:m,filterHeight:p,filterWidth:d,effectiveFilterHeight:w,effectiveFilterWidth:k,dilationHeight:y,dilationWidth:b,inShape:t,outShape:N,filterShape:e}}(t,u,n,s,i,r,!1,a)}function fn(t){return"number"==typeof t?[t,t,t]:2===t.length?[t[0],t[1],1]:t}function gn(t,e){return e<=1?t:t+(t-1)*(e-1)}function mn(t,e){if(!e)return Math.trunc(t);switch(e){case"round":return Math.round(t);case"ceil":return Math.ceil(t);case"floor":return Math.floor(t);default:throw new Error("Unknown roundingMode "+e)}}function yn(t){const[e,n,s]=fn(t);return 1===e&&1===n&&1===s}function bn(t,e){return yn(t)||yn(e)}const wn=Xe({reshape_:function(t,e){const n={x:Je(t,"x","reshape","string_or_numeric")},s={shape:e};return Ve.runKernel("Reshape",n,s)}});const kn=Xe({avgPool_:function(t,e,n,s,i){const r=Je(t,"x","avgPool","float32");Vt(bn(n,1),()=>`Error in avgPool: Either strides or dilations must be 1. Got strides ${n} and dilations '1'`);let a=r,o=!1;3===r.rank&&(o=!0,a=wn(r,[1,r.shape[0],r.shape[1],r.shape[2]])),Vt(4===a.rank,()=>`Error in avgPool: x must be rank 4 but got rank ${a.rank}.`),null!=i&&Vt(Zt(s),()=>`Error in avgPool: pad must be an integer when using, dimRoundingMode ${i} but got pad ${s}.`);const l={x:a},u={filterSize:e,strides:n,pad:s,dimRoundingMode:i};let h=Ve.runKernel("AvgPool",l,u);return h=pn(h,r.dtype),o?wn(h,[h.shape[1],h.shape[2],h.shape[3]]):h}});const xn=Xe({clone_:function(t){const e={x:Je(t,"x","clone","string_or_numeric")};return Ve.runKernel("Identity",e)}});const vn=Xe({concat_:function(t,e=0){Vt(t.length>=1,()=>"Pass at least one tensor to concat");const n=Ze(t,"tensors","concat","string_or_numeric");if("complex64"===n[0].dtype&&n.forEach(t=>{if("complex64"!==t.dtype)throw new Error(`Cannot concatenate complex64 tensors with a tensor\n          with dtype ${t.dtype}. `)}),1===n.length)return xn(n[0]);const s=n,i={axis:e};return Ve.runKernel("Concat",s,i)}});const Sn=Xe({matMul_:function(t,e,n=!1,s=!1){let i=Je(t,"a","matMul"),r=Je(e,"b","matMul");[i,r]=Pe(i,r);const a={a:i,b:r},o={transposeA:n,transposeB:s};return Ve.runKernel("BatchMatMul",a,o)}});const In=Xe({mul_:function(t,e){let n=Je(t,"a","mul"),s=Je(e,"b","mul");[n,s]=Pe(n,s);const i={a:n,b:s};return Ve.runKernel("Multiply",i)}});const Nn=Xe({sigmoid_:function(t){const e={x:Je(t,"x","sigmoid")};return Ve.runKernel("Sigmoid",e)}});const An=Xe({slice_:function(t,e,n){const s=Je(t,"x","slice","string_or_numeric");if(0===s.rank)throw new Error("Slicing scalar is not possible");const i={x:s},r={begin:e,size:n};return Ve.runKernel("Slice",i,r)}});const Cn=Xe({tanh_:function(t){const e={x:Je(t,"x","tanh")};return Ve.runKernel("Tanh",e)}});const zn=Xe({batchToSpaceND_:function(t,e,n){const s=Je(t,"x","batchToSpaceND"),i=e.reduce((t,e)=>t*e);Vt(s.rank>=1+e.length,()=>`input rank is ${s.rank} but should be > than blockShape.length ${e.length}`),Vt(n.length===e.length,()=>`crops.length is ${n.length} but should be equal to blockShape.length  ${e.length}`),Vt(s.shape[0]%i==0,()=>`input tensor batch is ${s.shape[0]} but is not divisible by the product of the elements of blockShape ${e.join(" * ")} === ${i}`);const r={x:s},a={blockShape:e,crops:n};return Ve.runKernel("BatchToSpaceND",r,a)}});const Dn=Xe({batchNorm_:function(t,e,n,s,i,r){null==r&&(r=.001);const a=Je(t,"x","batchNorm"),o=Je(e,"mean","batchNorm"),l=Je(n,"variance","batchNorm");let u,h;null!=i&&(u=Je(i,"scale","batchNorm")),null!=s&&(h=Je(s,"offset","batchNorm")),Vt(o.rank===l.rank,()=>"Batch normalization gradient requires mean and variance to have equal ranks."),Vt(null==h||o.rank===h.rank,()=>"Batch normalization gradient requires mean and offset to have equal ranks."),Vt(null==u||o.rank===u.rank,()=>"Batch normalization gradient requires mean and scale to have equal ranks.");const c={x:function(t){let e;return e=0===t.rank||1===t.rank?wn(t,[1,1,1,t.size]):2===t.rank?wn(t,[1,1,t.shape[0],t.shape[1]]):3===t.rank?wn(t,[1,t.shape[0],t.shape[1],t.shape[2]]):t,e}(a),scale:u,offset:h,mean:o,variance:l},p={varianceEpsilon:r},d=Ve.runKernel("FusedBatchNorm",c,p);return wn(d,a.shape)}});const Tn=Xe({broadcastTo_:function(t,e){let n=Je(t,"broadcastTo","x");const s=n.shape;if(e.some(t=>!(t>0)||t%1!=0))throw new Error(`broadcastTo(): Invalid broadcast shape [${e}].`);if(e.length<n.rank)throw new Error(`broadcastTo(): shape.length=${e.length} < input.rank=${n.rank}.`);if(e.length>n.rank){const t=n.shape.slice();for(;t.length<e.length;)t.unshift(1);n=wn(n,t)}const i=n.shape,r=Array.from(e);for(let t=e.length-1;t>=0;t--)if(i[t]===e[t])r[t]=1;else if(1!==n.shape[t])throw new Error(`broadcastTo(): [${s}] cannot be broadcast to [${e}].`);if(0===r.map((t,e)=>t>1?e:-1).filter(t=>t>=0).length)return xn(n);const a={x:n},o={reps:r};return Ve.runKernel("Tile",a,o)}});const En=Xe({ceil_:function(t){const e={x:Je(t,"x","ceil")};return Ve.runKernel("Ceil",e)}});const Fn=Xe({clipByValue_:function(t,e,n){const s=Je(t,"x","clipByValue");Vt(e<=n,()=>`Error in clip: min (${e}) must be less than or equal to max (${n}).`);const i={x:s},r={clipValueMin:e,clipValueMax:n};return Ve.runKernel("ClipByValue",i,r)}});const $n=Xe({complex_:function(t,e){const n=Je(t,"real","complex"),s=Je(e,"imag","complex");qt(n.shape,s.shape,`real and imag shapes, ${n.shape} and ${s.shape}, must match in call to tf.complex().`);const i={real:n,imag:s};return Ve.runKernel("Complex",i)}});const _n=Xe({conv2d_:function(t,e,n,s,i="NHWC",r=[1,1],a){const o=Je(t,"x","conv2d"),l=Je(e,"filter","conv2d");let u=o,h=!1;3===o.rank&&(h=!0,u=wn(o,[1,o.shape[0],o.shape[1],o.shape[2]])),Vt(4===u.rank,()=>`Error in conv2d: input must be rank 4, but got rank ${u.rank}.`),Vt(4===l.rank,()=>"Error in conv2d: filter must be rank 4, but got rank "+l.rank+"."),null!=a&&Vt(Zt(s),()=>`Error in conv2d: pad must be an integer when using, dimRoundingMode ${a} but got pad ${s}.`);const c="NHWC"===i?u.shape[3]:u.shape[1];Vt(c===l.shape[2],()=>`Error in conv2d: depth of input (${c}) must match input depth for filter ${l.shape[2]}.`),Vt(bn(n,r),()=>`Error in conv2D: Either strides or dilations must be 1. Got strides ${n} and dilations '${r}'`);const p={x:u,filter:l},d={strides:n,pad:s,dataFormat:i,dilations:r,dimRoundingMode:a},f=Ve.runKernel("Conv2D",p,d);return h?wn(f,[f.shape[1],f.shape[2],f.shape[3]]):f}});const Ln=Xe({conv1d_:function(t,e,n,s,i="NWC",r=1,a){const o=Je(t,"x","conv1d"),l=Je(e,"filter","conv1d");let u=o,h=!1;2===o.rank&&(h=!0,u=wn(o,[1,o.shape[0],o.shape[1]])),Vt(3===u.rank,()=>`Error in conv1d: input must be rank 3, but got rank ${u.rank}.`),Vt(3===l.rank,()=>"Error in conv1d: filter must be rank 3, but got rank "+l.rank+"."),null!=a&&Vt(Zt(s),()=>`Error in conv1d: pad must be an integer when using, dimRoundingMode ${a} but got pad ${s}.`),Vt(u.shape[2]===l.shape[1],()=>`Error in conv1d: depth of input (${u.shape[2]}) must match input depth for filter ${l.shape[1]}.`),Vt(bn(n,r),()=>`Error in conv1D: Either stride or dilation must be 1. Got stride ${n} and dilation '${r}'`),Vt("NWC"===i,()=>`Error in conv1d: got dataFormat of ${i} but only NWC is currently supported.`);const c=wn(l,[1,l.shape[0],l.shape[1],l.shape[2]]),p=wn(u,[u.shape[0],1,u.shape[1],u.shape[2]]),d=_n(p,c,[1,n],s,"NHWC",[1,r],a);return wn(d,h?[d.shape[2],d.shape[3]]:[d.shape[0],d.shape[2],d.shape[3]])}});const Rn=Xe({conv2DBackpropInput_:function(t,e,n,s,i,r="NHWC",a){Vt(t.length===e.rank,()=>`Length of inShape (${t.length}) and rank of dy (${e.rank}) must match`);let o=t,l=e,u=!1;3===e.rank&&(u=!0,l=wn(e,[1,e.shape[0],e.shape[1],e.shape[2]]),o=[1,t[0],t[1],t[2]]),Vt(4===o.length,()=>"Error in conv2dDerInput: inShape must be length 4, but got length "+o.length+"."),Vt(4===l.rank,()=>"Error in conv2dDerInput: dy must be rank 4, but got rank "+l.rank),Vt(4===n.rank,()=>"Error in conv2dDerInput: filter must be rank 4, but got rank "+n.rank);const h="NHWC"===r?o[3]:o[1],c="NHWC"===r?l.shape[3]:l.shape[1];Vt(h===n.shape[2],()=>`Error in conv2dDerInput: depth of input (${h}) must match input depth for filter ${n.shape[2]}.`),Vt(c===n.shape[3],()=>`Error in conv2dDerInput: depth of output (${c}) must match output depth for filter ${n.shape[3]}.`),null!=a&&Vt(Zt(i),()=>`Error in conv2dDerInput: pad must be an integer when using, dimRoundingMode ${a} but got pad ${i}.`);const p={dy:l,filter:n},d={strides:s,pad:i,dataFormat:r,dimRoundingMode:a,inputShape:o},f=Ve.runKernel("Conv2DBackpropInput",p,d);return u?wn(f,[f.shape[1],f.shape[2],f.shape[3]]):f}});const Mn=Xe({conv2dTranspose_:function(t,e,n,s,i,r){const a=Je(t,"x","conv2dTranspose"),o=Je(e,"filter","conv2dTranspose");return Rn(n,a,o,s,i,"NHWC",r)}});const On=Xe({conv3DBackpropInput_:function(t,e,n,s,i){Vt(t.length===e.rank,()=>`Length of inShape (${t.length}) and rank of dy (${e.rank}) must match`);let r=t,a=e,o=!1;4===e.rank&&(o=!0,a=wn(e,[1,e.shape[0],e.shape[1],e.shape[2],e.shape[3]]),r=[1,t[0],t[1],t[2],t[3]]);const l=r[4],u=a.shape[4];Vt(5===r.length,()=>"Error in conv3dDerInput: inShape must be length 5, but got length "+r.length+"."),Vt(5===a.rank,()=>"Error in conv3dDerInput: dy must be rank 5, but got rank "+a.rank),Vt(5===n.rank,()=>"Error in conv3dDerInput: filter must be rank 5, but got rank "+n.rank),Vt(l===n.shape[3],()=>`Error in conv3dDerInput: depth of input (${l}) must match input depth for filter ${n.shape[3]}.`),Vt(u===n.shape[4],()=>`Error in conv3dDerInput: depth of output (${u}) must match output depth for filter ${n.shape[4]}.`);const h={dy:a,filter:n},c={pad:i,strides:s,inputShape:r},p=Ve.runKernel("Conv3DBackpropInputV2",h,c);return o?wn(p,[p.shape[1],p.shape[2],p.shape[3],p.shape[4]]):p}});const Bn=Xe({cos_:function(t){const e={x:Je(t,"x","cos")};return Ve.runKernel("Cos",e)}});const Pn=Xe({cosh_:function(t){const e={x:Je(t,"x","cosh")};return Ve.runKernel("Cosh",e)}});const Wn=Xe({cumsum_:function(t,e=0,n=!1,s=!1){const i={x:Je(t,"x","cumsum")},r={axis:e,exclusive:n,reverse:s};return Ve.runKernel("Cumsum",i,r)}});const Un=Xe({depthToSpace_:function(t,e,n="NHWC"){const s=Je(t,"x","depthToSpace"),i="NHWC"===n?s.shape[1]:s.shape[2],r="NHWC"===n?s.shape[2]:s.shape[3],a="NHWC"===n?s.shape[3]:s.shape[1];Vt(i*e>=0,()=>`Negative dimension size caused by overflow when multiplying\n    ${i} and ${e}  for depthToSpace with input shape\n    ${s.shape}`),Vt(r*e>=0,()=>`Negative dimension size caused by overflow when multiplying\n    ${r} and ${e} for depthToSpace with input shape\n        ${s.shape}`),Vt(a%(e*e)==0,()=>`Dimension size must be evenly divisible by ${e*e} but is ${a} for depthToSpace with input shape ${s.shape}`);const o={x:s},l={blockSize:e,dataFormat:n};return Ve.runKernel("DepthToSpace",o,l)}});const Kn=Xe({depthwiseConv2d_:function(t,e,n,s,i="NHWC",r=[1,1],a){const o=Je(t,"x","depthwiseConv2d"),l=Je(e,"filter","depthwiseConv2d");let u=o,h=!1;3===o.rank&&(h=!0,u=wn(o,[1,o.shape[0],o.shape[1],o.shape[2]])),Vt(4===u.rank,()=>`Error in depthwiseConv2d: input must be rank 4, but got rank ${u.rank}.`),Vt(4===l.rank,()=>"Error in depthwiseConv2d: filter must be rank 4, but got rank "+l.rank+"."),Vt(u.shape[3]===l.shape[2],()=>`Error in depthwiseConv2d: number of input channels (${u.shape[3]}) must match the inChannels dimension in filter ${l.shape[2]}.`),null!=a&&Vt(Zt(s),()=>`Error in depthwiseConv2d: pad must be an integer when using, dimRoundingMode ${a} but got pad ${s}.`);const c={x:u,filter:l},p={strides:n,pad:s,dataFormat:i,dilations:r,dimRoundingMode:a},d=Ve.runKernel("DepthwiseConv2dNative",c,p);return h?wn(d,[d.shape[1],d.shape[2],d.shape[3]]):d}});const jn=Xe({dilation2d_:function(t,e,n,s,i=[1,1],r="NHWC"){const a=Je(t,"x","dilation2d"),o=Je(e,"filter","dilation2d");Vt(3===a.rank||4===a.rank,()=>"Error in dilation2d: input must be rank 3 or 4, but got rank "+a.rank+"."),Vt(3===o.rank,()=>"Error in dilation2d: filter must be rank 3, but got rank "+o.rank+"."),Vt("NHWC"===r,()=>"Error in dilation2d: Only NHWC is currently supported, but got dataFormat of "+r);let l=a,u=!1;3===a.rank&&(l=wn(a,[1,a.shape[0],a.shape[1],a.shape[2]]),u=!0);const h={x:l,filter:o},c={strides:n,pad:s,dilations:i},p=Ve.runKernel("Dilation2D",h,c);return u?wn(p,[p.shape[1],p.shape[2],p.shape[3]]):p}});const Vn=Xe({floorDiv_:function(t,e){let n=Je(t,"a","floorDiv"),s=Je(e,"b","floorDiv");[n,s]=Pe(n,s);const i={a:n,b:s};return Ve.runKernel("FloorDiv",i)}});const qn=Xe({div_:function(t,e){let n=Je(t,"a","div"),s=Je(e,"b","div");if([n,s]=Pe(n,s),"int32"===n.dtype&&"int32"===s.dtype)return Vn(n,s);const i={a:n,b:s};return Ve.runKernel("RealDiv",i,{})}});function Gn(t,e){const n=[];for(let s=0;s<e.length;s++){const i=t[t.length-s-1],r=e.length-s-1,a=e[r];(null==i||1===i&&a>1)&&n.unshift(r)}return n}function Hn(t,e){const n=[],s=Math.max(t.length,e.length);for(let i=0;i<s;i++){let s=t[t.length-i-1];null==s&&(s=1);let r=e[e.length-i-1];if(null==r&&(r=1),1===s)n.unshift(r);else if(1===r)n.unshift(s);else{if(s!==r){throw Error(`Operands could not be broadcast together with shapes ${t} and ${e}.`)}n.unshift(s)}}return n}const Jn=Xe({equal_:function(t,e){let n=Je(t,"a","equal"),s=Je(e,"b","equal");[n,s]=Pe(n,s),Hn(n.shape,s.shape);const i={a:n,b:s};return Ve.runKernel("Equal",i)}});const Zn=Xe({where_:function(t,e,n){const s=Je(e,"a","where"),i=Je(n,"b","where"),r=Je(t,"condition","where","bool"),a=Hn(s.shape,i.shape),o=Tn(s,a),l=Tn(i,a);1===r.rank&&Vt(r.shape[0]===s.shape[0],()=>"The first dimension of `a` must match the size of `condition`."),1!==r.rank&&qt(r.shape,l.shape,"Error in where: ");const u={condition:r,t:o,e:l};return Ve.runKernel("Select",u)}});const Xn=Xe({zerosLike_:function(t){const e={x:Je(t,"x","zerosLike")};return Ve.runKernel("ZerosLike",e)}});const Yn=Xe({divNoNan_:function(t,e){let n=Je(t,"a","div"),s=Je(e,"b","div");[n,s]=Pe(n,s);const i=qn(n,s),r=Xn(i),a=Jn(s,r);return Zn(a,r,i)}});const Qn=Xe({dot_:function(t,e){const n=Je(t,"t1","dot"),s=Je(e,"t2","dot");Vt(!(1!==n.rank&&2!==n.rank||1!==s.rank&&2!==s.rank),()=>`Error in dot: inputs must all be rank 1 or 2, but got ranks ${n.rank} and ${s.rank}.`);const i=1===n.rank?n.size:n.shape[1],r=1===s.rank?s.size:s.shape[0];if(Vt(i===r,()=>`Error in dot: inner dimensions of inputs must match, but got ${i} and ${r}.`),1===n.rank&&1===s.rank){const t=wn(n,[1,-1]),e=wn(s,[-1,1]),i=Sn(t,e);return wn(i,[])}if(1===n.rank&&2===s.rank){const t=wn(n,[1,-1]),e=wn(s,[s.shape[0],s.shape[1]]),i=Sn(t,e);return wn(i,[i.size])}if(2===n.rank&&1===s.rank){const t=wn(s,[-1,1]),e=Sn(n,t);return wn(e,[e.size])}{const t=wn(s,[s.shape[0],s.shape[1]]);return Sn(n,t)}}});const ts=Xe({elu_:function(t){const e={x:Je(t,"x","elu")};return Ve.runKernel("Elu",e)}});const es=Xe({erf_:function(t){let e=Je(t,"x","erf");Vt("int32"===e.dtype||"float32"===e.dtype,()=>"Input dtype must be `int32` or `float32`."),"int32"===e.dtype&&(e=pn(e,"float32"));const n={x:e};return Ve.runKernel("Erf",n)}});const ns=Xe({exp_:function(t){const e={x:Je(t,"x","exp")};return Ve.runKernel("Exp",e)}});const ss=Xe({expandDims_:function(t,e=0){const n=Je(t,"x","expandDims","string_or_numeric");Vt(e<=n.rank,()=>"Axis must be <= rank of the tensor");const s={input:n},i={dim:e};return Ve.runKernel("ExpandDims",s,i)}});const is=Xe({expm1_:function(t){const e={x:Je(t,"x","expm1")};return Ve.runKernel("Expm1",e)}});const rs=Xe({tile_:function(t,e){const n=Je(t,"x","tile","string_or_numeric");Vt(n.rank===e.length,()=>`Error in transpose: rank of input ${n.rank} must match length of reps ${e}.`);const s={x:n},i={reps:e};return Ve.runKernel("Tile",s,i)}});const as=Xe({floor_:function(t){const e={x:Je(t,"x","floor")};return Ve.runKernel("Floor",e)}});const os=Xe({gather_:function(t,e,n=0,s=0){const i={x:Je(t,"x","gather"),indices:Je(e,"indices","gather","int32")},r={axis:n,batchDims:s};return Ve.runKernel("GatherV2",i,r)}});const ls=Xe({greater_:function(t,e){let n=Je(t,"a","greater"),s=Je(e,"b","greater");[n,s]=Pe(n,s),Hn(n.shape,s.shape);const i={a:n,b:s};return Ve.runKernel("Greater",i)}});const us=Xe({greaterEqual_:function(t,e){let n=Je(t,"a","greaterEqual"),s=Je(e,"b","greaterEqual");[n,s]=Pe(n,s),Hn(n.shape,s.shape);const i={a:n,b:s};return Ve.runKernel("GreaterEqual",i)}});const hs=Xe({imag_:function(t){const e={input:Je(t,"input","imag")};return Ve.runKernel("Imag",e)}});const cs=Xe({isFinite_:function(t){const e={x:Je(t,"x","isFinite")};return Ve.runKernel("IsFinite",e)}});const ps=Xe({isInf_:function(t){const e={x:Je(t,"x","isInf")};return Ve.runKernel("IsInf",e)}});const ds=Xe({isNaN_:function(t){const e={x:Je(t,"x","isNaN")};return Ve.runKernel("IsNan",e)}});const fs=Xe({leakyRelu_:function(t,e=.2){const n={x:Je(t,"x","leakyRelu")},s={alpha:e};return Ve.runKernel("LeakyRelu",n,s)}});const gs=Xe({less_:function(t,e){let n=Je(t,"a","less"),s=Je(e,"b","less");[n,s]=Pe(n,s),Hn(n.shape,s.shape);const i={a:n,b:s};return Ve.runKernel("Less",i)}});const ms=Xe({lessEqual_:function(t,e){let n=Je(t,"a","lessEqual"),s=Je(e,"b","lessEqual");[n,s]=Pe(n,s),Hn(n.shape,s.shape);const i={a:n,b:s};return Ve.runKernel("LessEqual",i)}});const ys=Xe({localResponseNormalization_:function(t,e=5,n=1,s=1,i=.5){const r=Je(t,"x","localResponseNormalization");Vt(4===r.rank||3===r.rank,()=>`Error in localResponseNormalization: x must be rank 3 or 4 but got\n               rank ${r.rank}.`),Vt(Zt(e),()=>`Error in localResponseNormalization: depthRadius must be an integer but got depthRadius ${e}.`);let a=r,o=!1;3===r.rank&&(o=!0,a=wn(r,[1,r.shape[0],r.shape[1],r.shape[2]]));const l={x:a},u={depthRadius:e,bias:n,alpha:s,beta:i},h=Ve.runKernel("LRN",l,u);return o?wn(h,[h.shape[1],h.shape[2],h.shape[3]]):h}});const bs=Xe({log_:function(t){const e={x:Je(t,"x","log")};return Ve.runKernel("Log",e)}});const ws=Xe({log1p_:function(t){const e={x:Je(t,"x","log1p")};return Ve.runKernel("Log1p",e)}});function ks(t){return Ve.customGrad(t)}const xs=Xe({neg_:function(t){const e={x:Je(t,"x","neg")};return Ve.runKernel("Neg",e)}});const vs=Xe({softplus_:function(t){const e={x:Je(t,"x","softplus")};return Ve.runKernel("Softplus",e)}});const Ss=Xe({logSigmoid_:function(t){const e=Je(t,"x","logSigmoid");return ks(t=>({value:xs(vs(xs(t))),gradFunc:e=>In(e,Nn(xs(t)))}))(e)}});const Is=Xe({max_:function(t,e=null,n=!1){const s={x:Je(t,"x","max")},i={reductionIndices:e,keepDims:n};return Ve.runKernel("Max",s,i)}});const Ns=Xe({sub_:function(t,e){let n=Je(t,"a","sub"),s=Je(e,"b","sub");[n,s]=Pe(n,s);const i={a:n,b:s};return Ve.runKernel("Sub",i)}});const As=Xe({sum_:function(t,e=null,n=!1){let s=Je(t,"x","sum");"bool"===s.dtype&&(s=pn(s,"int32"));const i={x:s},r={axis:e,keepDims:n};return Ve.runKernel("Sum",i,r)}});const Cs=Xe({logSoftmax_:function(t,e=-1){const n=Je(t,"logits","logSoftmax");if(-1===e&&(e=n.rank-1),e!==n.rank-1)throw Error(`Log Softmax along a non-last dimension is not yet supported. Logits was rank ${n.rank} and axis was ${e}`);return ks((t,n)=>{const s=Is(t,e,!0),i=Ns(t,s),r=Ns(pn(i,"float32"),bs(As(ns(i),e,!0)));n([r]);return{value:r,gradFunc:(t,n)=>{const[s]=n,i=ns(s);return Ns(t,In(As(t,e,!0),i))}}})(n)}});function zs(t,e){return function(t,e,n){const s=t.length+e.length,i=[];let r=0,a=0;for(let o=0;o<s;o++)-1===n.indexOf(o)?i.push(t[r++]):i.push(e[a++]);return i}(t,e.map(t=>1),e)}function Ds(t){return t.map((t,e)=>[e,t]).sort((t,e)=>t[1]-e[1]).map(t=>t[0])}const Ts=Xe({logSumExp_:function(t,e=null,n=!1){const s=Je(t,"x","logSumExp"),i=Yt(e,s.shape),r=Is(s,i,!0),a=Ns(s,r),o=ns(a),l=As(o,i),u=bs(l),h=en(wn(r,u.shape),u);if(n){const t=zs(h.shape,i);return wn(h,t)}return h}});const Es=Xe({logicalAnd_:function(t,e){const n=Je(t,"a","logicalAnd","bool"),s=Je(e,"b","logicalAnd","bool");Hn(n.shape,s.shape);const i={a:n,b:s};return Ve.runKernel("LogicalAnd",i)}});const Fs=Xe({logicalNot_:function(t){const e={x:Je(t,"x","logicalNot","bool")};return Ve.runKernel("LogicalNot",e)}});const $s=Xe({logicalOr_:function(t,e){const n=Je(t,"a","logicalOr","bool"),s=Je(e,"b","logicalOr","bool");Hn(n.shape,s.shape);const i={a:n,b:s};return Ve.runKernel("LogicalOr",i)}});const _s=Xe({logicalXor_:function(t,e){const n=Je(t,"a","logicalXor","bool"),s=Je(e,"b","logicalXor","bool");return Hn(n.shape,s.shape),Es($s(t,e),Fs(Es(t,e)))}});const Ls=Xe({maxPool_:function(t,e,n,s,i){const r=Je(t,"x","maxPool");let a=r,o=!1;3===r.rank&&(o=!0,a=wn(r,[1,r.shape[0],r.shape[1],r.shape[2]])),Vt(4===a.rank,()=>`Error in maxPool: input must be rank 4 but got rank ${a.rank}.`),Vt(bn(n,1),()=>`Error in maxPool: Either strides or dilations must be 1. Got strides ${n} and dilations '1'`),null!=i&&Vt(Zt(s),()=>`Error in maxPool: pad must be an integer when using, dimRoundingMode ${i} but got pad ${s}.`);const l={x:a},u={filterSize:e,strides:n,pad:s,dimRoundingMode:i},h=Ve.runKernel("MaxPool",l,u);return o?wn(h,[h.shape[1],h.shape[2],h.shape[3]]):h}});const Rs=Xe({maximum_:function(t,e){let n=Je(t,"a","maximum"),s=Je(e,"b","maximum");[n,s]=Pe(n,s),"bool"===n.dtype&&(n=pn(n,"int32"),s=pn(s,"int32")),Hn(n.shape,s.shape);const i={a:n,b:s};return Ve.runKernel("Maximum",i)}});const Ms=Xe({mean_:function(t,e=null,n=!1){const s={x:Je(t,"x","mean")},i={axis:e,keepDims:n};return Ve.runKernel("Mean",s,i)}});const Os=Xe({min_:function(t,e=null,n=!1){const s={x:Je(t,"x","min")},i={axis:e,keepDims:n};return Ve.runKernel("Min",s,i)}});const Bs=Xe({minimum_:function(t,e){let n=Je(t,"a","minimum"),s=Je(e,"b","minimum");[n,s]=Pe(n,s),"bool"===n.dtype&&(n=pn(n,"int32"),s=pn(s,"int32")),Hn(n.shape,s.shape);const i={a:n,b:s};return Ve.runKernel("Minimum",i)}});const Ps=Xe({mirrorPad_:function(t,e,n){Vt("reflect"===n||"symmetric"===n,()=>`Invalid mode. Mode must be either reflect or symmetric. Got ${n}.`);const s=Je(t,"x","mirrorPad");if(0===s.rank)throw new Error("mirrorPad(scalar) is not defined. Pass non-scalar to mirrorPad");Vt(e.length===s.rank,()=>`Padding doesn't match input. Must be ${s.rank}. Got ${e.length}.`);const i="reflect"===n?1:0;for(let t=0;t<s.rank;t++)Vt(2===e[t].length,()=>"Invalid number of paddings. Must be length of 2 each."),Vt(e[t][0]>=0&&e[t][0]<=s.shape[t]-i&&e[t][1]>=0&&e[t][1]<=s.shape[t]-i,()=>`Padding in dimension ${t} cannot be greater than or equal to ${s.shape[t]-i} or less than 0 for input of shape `+s.shape);const r={paddings:e,mode:n},a={x:s};return Ve.runKernel("MirrorPad",a,r)}});const Ws=Xe({mod_:function(t,e){let n=Je(t,"a","mod"),s=Je(e,"b","mod");[n,s]=Pe(n,s);const i={a:n,b:s};return Ve.runKernel("Mod",i)}});const Us=Xe({square_:function(t){const e=Je(t,"x","square");return Ve.runKernel("Square",{x:e},{})}});const Ks=Xe({notEqual_:function(t,e){let n=Je(t,"a","notEqual"),s=Je(e,"b","notEqual");[n,s]=Pe(n,s),Hn(n.shape,s.shape);const i={a:n,b:s};return Ve.runKernel("NotEqual",i)}});const js=Xe({oneHot_:function(t,e,n=1,s=0){if(e<2)throw new Error("Error in oneHot: depth must be >=2, but it is "+e);const i={indices:Je(t,"indices","oneHot","int32")},r={depth:e,onValue:n,offValue:s};return Ve.runKernel("OneHot",i,r)}});function Vs(t,e="float32"){if("complex64"===e){const e=Vs(t,"float32"),n=Vs(t,"float32");return $n(e,n)}const n=ae(Ht(t),e);return Ve.makeTensor(n,t,e)}function qs(t,e="float32"){if("complex64"===e){const e=qs(t,"float32"),n=Vs(t,"float32");return $n(e,n)}const n=re(Ht(t),e);return Ve.makeTensor(n,t,e)}const Gs=Xe({onesLike_:function(t){const e={x:Je(t,"x","onesLike")};return Ve.runKernel("OnesLike",e)}});const Hs=Xe({pad_:function(t,e,n=0){const s=Je(t,"x","pad");if(0===s.rank)throw new Error("pad(scalar) is not defined. Pass non-scalar to pad");const i={paddings:e,constantValue:n},r={x:s};return Ve.runKernel("PadV2",r,i)}});const Js=Xe({spaceToBatchND_:function(t,e,n){const s=Je(t,"x","spaceToBatchND");Vt(s.rank>=1+e.length,()=>`input rank ${s.rank} should be > than [blockShape] ${e.length}`),Vt(n.length===e.length,()=>`paddings.shape[0] ${n.length} must be equal to [blockShape] ${e.length}`),Vt(s.shape.reduce((t,s,i)=>i>0&&i<=e.length?t&&(s+n[i-1][0]+n[i-1][1])%e[i-1]==0:t,!0),()=>`input spatial dimensions ${s.shape.slice(1)} with paddings ${n.toString()} must be divisible by blockShapes ${e.toString()}`);const i={x:s},r={blockShape:e,paddings:n};return Ve.runKernel("SpaceToBatchND",i,r)}});const Zs=Xe({pool_:function(t,e,n,s,i,r){null==i&&(i=[1,1]),null==r&&(r=1),0===s&&(s="valid");const a=Je(t,"x","maxPool");let o=a,l=!1;3===a.rank&&(l=!0,o=wn(a,[1,a.shape[0],a.shape[1],a.shape[2]])),Vt(bn(r,i),()=>`Error in pool: Either strides or dilations must be 1. Got strides ${r} and dilations '${i}'`);const u=dn(o.shape,e,r,i,s),h=[u.dilationHeight,u.dilationWidth];let c;c="same"===s?function(t,e){const n=t.map((t,n)=>t+(t-1)*(e[n]-1)).map(t=>t-1),s=n.map(t=>Math.floor(t/2)),i=n.map((t,e)=>t-s[e]);return n.map((t,e)=>[s[e],i[e]])}([u.filterHeight,u.filterWidth],h):[[0,0],[0,0]];const p=1===h[0]&&1===h[1],[d,f]=function(t,e,n){const s=n.map(t=>t[0]),i=n.map(t=>t[1]),r=t.concat(s,i),a=e.map((t,e)=>(t-r[e]%t)%t),o=i.map((t,e)=>t+a[e]),l=e.map((t,e)=>[s[e],o[e]]),u=e.map((t,e)=>[0,a[e]]);return[l,u]}([u.inHeight,u.inWidth],h,c),g=p?s:"valid",m=p?o:Js(o,h,d),y=("avg"===n?()=>kn(m,e,r,g):()=>Ls(m,e,r,g))(),b=p?y:zn(y,h,f);return l?wn(b,[b.shape[1],b.shape[2],b.shape[3]]):b}});const Xs=Xe({pow_:function(t,e){let n=Je(t,"base","pow"),s=Je(e,"exp","pow");[n,s]=Pe(n,s);const i={a:n,b:s};return Ve.runKernel("Pow",i)}});const Ys=Xe({prelu_:function(t,e){const n={x:Je(t,"x","prelu"),alpha:Je(e,"alpha","prelu")};return Ve.runKernel("Prelu",n)}});const Qs=Xe({prod_:function(t,e=null,n=!1){let s=Je(t,"x","prod");"bool"===s.dtype&&(s=pn(s,"int32"));const i={x:s},r={axis:e,keepDims:n};return Ve.runKernel("Prod",i,r)}});const ti=Xe({real_:function(t){const e={input:Je(t,"input","real")};return Ve.runKernel("Real",e)}});const ei=Xe({reciprocal_:function(t){const e={x:Je(t,"x","reciprocal")};return Ve.runKernel("Reciprocal",e)}});const ni=Xe({relu_:function(t){const e={x:Je(t,"x","relu")};return Ve.runKernel("Relu",e)}});const si=Xe({relu6_:function(t){const e={x:Je(t,"x","relu6")};return Ve.runKernel("Relu6",e)}});const ii=Xe({reverse_:function(t,e){const n={x:Je(t,"x","reverse")},s={dims:e};return Ve.runKernel("Reverse",n,s)}});const ri=Xe({round_:function(t){const e={x:Je(t,"x","round")};return Ve.runKernel("Round",e)}});const ai=Xe({rsqrt_:function(t){const e={x:Je(t,"x","rsqrt")};return Ve.runKernel("Rsqrt",e)}});function oi(t,e,n,s){if(null==s&&(s=ee(t)),"complex64"===s)throw new Error("Cannot construct a complex64 tensor directly. Please use tf.complex(real, imag).");if(!Qt(t)&&!Array.isArray(t)&&"number"!=typeof t&&"boolean"!=typeof t&&"string"!=typeof t)throw new Error("values passed to tensor(values) must be a number/boolean/string or an array of numbers/booleans/strings, or a TypedArray");if(null!=e){!function(t){t.forEach(e=>{Vt(Number.isInteger(e)&&e>=0,()=>`Tensor must have a shape comprised of positive integers but got shape [${t}].`)})}(e);const t=Ht(e),s=Ht(n);Vt(t===s,()=>`Based on the provided shape, [${e}], the tensor should have ${t} values but has ${s}`);for(let t=0;t<n.length;++t){const s=n[t],i=t!==n.length-1||s!==Ht(e.slice(t));Vt(n[t]===e[t]||!i,()=>`Error creating a new Tensor. Inferred shape (${n}) does not match the provided shape (${e}). `)}}return Qt(t)||Array.isArray(t)||(t=[t]),e=e||n,t="string"!==s?ke(t,s):Gt(t,[],!0),Ve.makeTensor(t,e,s)}function li(t,e){if((Qt(t)&&"string"!==e||Array.isArray(t))&&"complex64"!==e)throw new Error("Error creating a new Scalar: value must be a primitive (number|boolean|string)");if("string"===e&&Qt(t)&&!(t instanceof Uint8Array))throw new Error("When making a scalar from encoded string, the value must be `Uint8Array`.");return oi(t,[],[],e)}const ui=Xe({selu_:function(t){const e={x:Je(t,"x","selu")};return Ve.runKernel("Selu",e)}});const hi=Xe({separableConv2d_:function(t,e,n,s,i,r=[1,1],a="NHWC"){const o=Je(t,"x","separableConv2d"),l=Je(e,"depthwiseFilter","separableConv2d"),u=Je(n,"pointwiseFilter","separableConv2d");let h=o,c=!1;if(3===o.rank&&(c=!0,h=wn(o,[1,o.shape[0],o.shape[1],o.shape[2]])),"NCHW"===a)throw new Error("separableConv2d currently does not support dataFormat NCHW; only NHWC is supported");Vt(4===h.rank,()=>`Error in separableConv2d: input must be rank 4, but got rank ${h.rank}.`),Vt(4===l.rank,()=>`Error in separableConv2d: depthwise filter must be rank 4, but got rank ${l.rank}.`),Vt(4===u.rank,()=>`Error in separableConv2d: pointwise filter must be rank 4, but got rank ${l.rank}.`),Vt(1===u.shape[0],()=>`Error in separableConv2d: the first dimension of pointwise filter  must be 1, but got ${u.shape[0]}.`),Vt(1===u.shape[1],()=>`Error in separableConv2d: the second dimension of pointwise filter must be 1, but got ${u.shape[1]}.`);const p=l.shape[2],d=l.shape[3];Vt(u.shape[2]===p*d,()=>`Error in separableConv2d: the third dimension of pointwise filter must be ${p*d}, but got ${u.shape[2]}.`);const f=Kn(h,l,s,i,a,r),g=_n(f,u,1,"valid",a);return c?wn(g,[g.shape[1],g.shape[2],g.shape[3]]):g}});const ci=Xe({sign_:function(t){const e={x:Je(t,"x","sign")};return Ve.runKernel("Sign",e)}});const pi=Xe({sin_:function(t){const e={x:Je(t,"x","sin")};return Ve.runKernel("Sin",e)}});const di=Xe({sinh_:function(t){const e={x:Je(t,"x","sinh")};return Ve.runKernel("Sinh",e)}});const fi=Xe({softmax_:function(t,e=-1){const n=Je(t,"logits","softmax","float32");if(-1===e&&(e=n.rank-1),e!==n.rank-1)throw Error(`Softmax along a non-last dimension is not yet supported. Logits was rank ${n.rank} and dim was ${e}`);const s={logits:n},i={dim:e};return Ve.runKernel("Softmax",s,i)}});const gi=Xe({fft_:function(t){Vt("complex64"===t.dtype,()=>`The dtype for tf.spectral.fft() must be complex64 but got ${t.dtype}.`);const e={input:t};return Ve.runKernel("FFT",e)}});const mi=Xe({ifft_:function(t){Vt("complex64"===t.dtype,()=>`The dtype for tf.spectral.ifft() must be complex64 but got ${t.dtype}.`);const e={input:t};return Ve.runKernel("IFFT",e)}});const yi=Xe({irfft_:function(t){const e=t.shape[t.shape.length-1],n=t.size/e;let s;if(e<=2){const i=wn(t,[n,e]);s=mi(i)}else{const i=[n,2*(e-1)],r=wn(ti(t),[n,e]),a=wn(hs(t),[n,e]),o=ii(An(r,[0,1],[n,e-2]),1),l=In(ii(An(a,[0,1],[n,e-2]),1),li(-1)),u=vn([r,o],1),h=vn([a,l],1),c=wn($n(u,h),[i[0],i[1]]);s=mi(c)}if(s=ti(s),3===t.rank&&0!==t.shape[0]){const e=s,n=t.shape[0];s=wn(s,[n,s.shape[0]/n,s.shape[1]]),e.dispose()}return s}});const bi=Xe({split_:function(t,e,n=0){const s={x:Je(t,"x","split")},i={numOrSizeSplits:e,axis:n};return Ve.runKernel("SplitV",s,i)}});const wi=Xe({rfft_:function(t,e){Vt("float32"===t.dtype,()=>"The dtype for rfft() must be real value but got "+t.dtype);let n=t.shape[t.shape.length-1];const s=t.size/n;let i;if(null!=e&&e<n){const s=t.shape.map(t=>0),r=t.shape.map(t=>t);r[t.shape.length-1]=e,i=An(t,s,r),n=e}else if(null!=e&&e>n){const s=t.shape.map(t=>t);s[t.shape.length-1]=e-n,i=vn([t,Vs(s)],t.shape.length-1),n=e}else i=t;const r=Xn(i),a=wn($n(i,r),[s,n]),o=gi(a),l=Math.floor(n/2)+1,u=ti(o),h=hs(o),c=bi(u,[l,n-l],u.shape.length-1),p=bi(h,[l,n-l],h.shape.length-1),d=i.shape.slice();return d[i.shape.length-1]=l,wn($n(c[0],p[0]),d)}});const ki=Xe({sqrt_:function(t){const e={x:Je(t,"x","sqrt")};return Ve.runKernel("Sqrt",e)}});const xi=Xe({squaredDifference_:function(t,e){let n=Je(t,"a","squaredDifference"),s=Je(e,"b","squaredDifference");[n,s]=Pe(n,s),Hn(n.shape,s.shape);const i={a:n,b:s};return Ve.runKernel("SquaredDifference",i,{})}});const vi=Xe({squeeze_:function(t,e){const n=Je(t,"x","squeeze");return wn(n,function(t,e){const n=[],s=[],i=null!=e&&Array.isArray(e)&&0===e.length,r=null==e||i?null:Yt(e,t).sort();let a=0;for(let e=0;e<t.length;++e){if(null!=r){if(r[a]===e&&1!==t[e])throw new Error(`Can't squeeze axis ${e} since its dim '${t[e]}' is not 1`);(null==r[a]||r[a]>e)&&1===t[e]&&(n.push(t[e]),s.push(e)),r[a]<=e&&a++}1!==t[e]&&(n.push(t[e]),s.push(e))}return{newShape:n,keptDims:s}}(n.shape,e).newShape)}});const Si=Xe({stack_:function(t,e=0){const n=Ze(t,"tensors","stack","string_or_numeric");Vt(n.length>=1,()=>"Pass at least one tensor to tf.stack"),n.length>0&&Vt(e<=n[0].rank,()=>"Axis must be <= rank of the tensor");const s=n,i={axis:e};return Ve.runKernel("Pack",s,i)}});const Ii=Xe({step_:function(t,e=0){const n={x:Je(t,"x","step")},s={alpha:e};return Ve.runKernel("Step",n,s)}});const Ni=Xe({stridedSlice_:function(t,e,n,s,i=0,r=0,a=0,o=0,l=0){const u={x:Je(t,"x","stridedSlice")},h={begin:e,end:n,strides:s,beginMask:i,endMask:r,ellipsisMask:a,newAxisMask:o,shrinkAxisMask:l};return Ve.runKernel("StridedSlice",u,h)}});const Ai=Xe({tan_:function(t){const e={x:Je(t,"x","tan")};return Ve.runKernel("Tan",e)}});const Ci=Xe({topk_:function(t,e=1,n=!0){const s=Je(t,"x","topk");if(0===s.rank)throw new Error("topk() expects the input to be of rank 1 or higher");const i=s.shape[s.shape.length-1];if(e>i)throw new Error(`'k' passed to topk() must be <= the last dimension (${i}) but got `+e);const r={x:s},a={k:e,sorted:n},[o,l]=Ve.runKernel("TopK",r,a);return{values:o,indices:l}}});const zi=Xe({unique_:function(t,e=0){const n=Je(t,"x","unique","string_or_numeric");Vt(n.rank>0,()=>"The input tensor must be at least 1D");const s={x:n},i={axis:e},[r,a]=Ve.runKernel("Unique",s,i);return{values:r,indices:a}}});const Di=Xe({unsortedSegmentSum_:function(t,e,n){const s=Je(t,"x","unsortedSegmentSum"),i=Je(e,"segmentIds","unsortedSegmentSum","int32");Vt(Zt(n),()=>"numSegments must be of dtype int");const r={x:s,segmentIds:i},a={numSegments:n};return Ve.runKernel("UnsortedSegmentSum",r,a)}});const Ti=Xe({unstack_:function(t,e=0){const n=Je(t,"x","unstack","string_or_numeric");Vt(e>=-n.shape.length&&e<n.shape.length,()=>`Axis = ${e} is not in [-${n.shape.length}, ${n.shape.length})`);const s={value:n},i={axis:e};return Ve.runKernel("Unpack",s,i)}});const Ei=Xe({transpose_:function(t,e){const n=Je(t,"x","transpose");if(null==e&&(e=n.shape.map((t,e)=>e).reverse()),Vt(n.rank===e.length,()=>`Error in transpose: rank of input ${n.rank} must match length of perm ${e}.`),e.forEach(t=>{Vt(t>=0&&t<n.rank,()=>"All entries in 'perm' must be between 0 and "+(n.rank-1)+" but got "+e)}),n.rank<=1)return n.clone();const s={x:n},i={perm:e};return Ve.runKernel("Transpose",s,i)}});const Fi=Xe({norm_:function(t,e="euclidean",n=null,s=!1){const i=function t(e,n,s=null){if(0===e.rank)return Ye(e);if(1!==e.rank&&null===s)return t(wn(e,[-1]),n,s);if(1===e.rank||"number"==typeof s||Array.isArray(s)&&1===s.length){if(1===n)return As(Ye(e),s);if(n===1/0)return Is(Ye(e),s);if(n===-1/0)return Os(Ye(e),s);if("euclidean"===n||2===n)return ki(As(Xs(Ye(e),li(2,"int32")),s));throw new Error("Error in norm: invalid ord value: "+n)}if(Array.isArray(s)&&2===s.length){if(1===n)return Is(As(Ye(e),s[0]),s[1]-1);if(n===1/0)return Is(As(Ye(e),s[1]),s[0]);if(n===-1/0)return Os(As(Ye(e),s[1]),s[0]);if("fro"===n||"euclidean"===n)return ki(As(Us(e),s));throw new Error("Error in norm: invalid ord value: "+n)}throw new Error("Error in norm: invalid axis: "+s)}(t=Je(t,"x","norm"),e,n);let r=i.shape;if(s){const e=Yt(n,t.shape);r=zs(i.shape,e)}return wn(i,r)}});const $i=Xe({conv2DBackpropFilter_:function(t,e,n,s,i,r="NHWC",a){let o=t;3===t.rank&&(o=wn(t,[1,t.shape[0],t.shape[1],t.shape[2]]));let l=e;3===l.rank&&(l=wn(e,[1,e.shape[0],e.shape[1],e.shape[2]])),Vt(4===o.rank,()=>"Error in conv2dDerFilter: input must be rank 4, but got shape "+o.shape+"."),Vt(4===l.rank,()=>"Error in conv2dDerFilter: dy must be rank 4, but got shape "+l.shape+"."),Vt(4===n.length,()=>"Error in conv2dDerFilter: filterShape must be length 4, but got "+n+".");const u="NHWC"===r?o.shape[3]:o.shape[1],h="NHWC"===r?l.shape[3]:l.shape[1];Vt(u===n[2],()=>`Error in conv2dDerFilter: depth of input ${u}) must match input depth in filter (${n[2]}.`),Vt(h===n[3],()=>`Error in conv2dDerFilter: depth of dy (${h}) must match output depth for filter (${n[3]}).`),null!=a&&Vt(Zt(i),()=>`Error in conv2dDerFilter: pad must be an integer when using, dimRoundingMode ${a} but got pad ${i}.`);const c={x:o,dy:l},p={strides:s,pad:i,dataFormat:r,dimRoundingMode:a,filterShape:n};return Ve.runKernel("Conv2DBackpropFilter",c,p)}});const _i=Xe({depthwiseConv2dNativeBackpropFilter_:function(t,e,n,s,i,r=[1,1],a){let o=t;3===t.rank&&(o=wn(t,[1,t.shape[0],t.shape[1],t.shape[2]]));let l=e;3===l.rank&&(l=wn(e,[1,e.shape[0],e.shape[1],e.shape[2]]));const u={x:o,dy:l},h={strides:s,pad:i,dimRoundingMode:a,dilations:r,filterShape:n};return Ve.runKernel("DepthwiseConv2dNativeBackpropFilter",u,h)}});const Li=Xe({depthwiseConv2dNativeBackpropInput_:function(t,e,n,s,i,r=[1,1],a){let o=e,l=!1;3===e.rank&&(l=!0,o=wn(e,[1,e.shape[0],e.shape[1],e.shape[2]]));const u={dy:o,filter:n},h={strides:s,pad:i,dimRoundingMode:a,dilations:r,inputShape:t},c=Ve.runKernel("DepthwiseConv2dNativeBackpropInput",u,h);return l?wn(c,[c.shape[1],c.shape[2],c.shape[3]]):c}});const Ri=Xe({resizeBilinear_:function(t,e,n=!1,s=!1){const i=Je(t,"images","resizeBilinear");Vt(3===i.rank||4===i.rank,()=>`Error in resizeBilinear: x must be rank 3 or 4, but got rank ${i.rank}.`),Vt(2===e.length,()=>"Error in resizeBilinear: new shape must 2D, but got shape "+e+"."),Vt(!1===s||!1===n,()=>"Error in resizeBilinear: If halfPixelCenters is true, alignCorners must be false.");let r=i,a=!1;3===i.rank&&(a=!0,r=wn(i,[1,i.shape[0],i.shape[1],i.shape[2]]));const o={images:r},l={alignCorners:n,halfPixelCenters:s,size:e},u=Ve.runKernel("ResizeBilinear",o,l);return a?wn(u,[u.shape[1],u.shape[2],u.shape[3]]):u}});const Mi=Xe({resizeNearestNeighbor_:function(t,e,n=!1,s=!1){const i=Je(t,"images","resizeNearestNeighbor");Vt(3===i.rank||4===i.rank,()=>`Error in resizeNearestNeighbor: x must be rank 3 or 4, but got rank ${i.rank}.`),Vt(2===e.length,()=>"Error in resizeNearestNeighbor: new shape must 2D, but got shape "+e+"."),Vt("float32"===i.dtype||"int32"===i.dtype,()=>"`images` must have `int32` or `float32` as dtype"),Vt(!1===s||!1===n,()=>"Error in resizeNearestNeighbor: If halfPixelCenters is true, alignCorners must be false.");let r=i,a=!1;3===i.rank&&(a=!0,r=wn(i,[1,i.shape[0],i.shape[1],i.shape[2]]));const o={images:r},l={alignCorners:n,halfPixelCenters:s,size:e},u=Ve.runKernel("ResizeNearestNeighbor",o,l);return a?wn(u,[u.shape[1],u.shape[2],u.shape[3]]):u}});Fe().prototype.abs=function(){return this.throwIfDisposed(),Ye(this)},Fe().prototype.acos=function(){return this.throwIfDisposed(),Qe(this)},Fe().prototype.acosh=function(){return this.throwIfDisposed(),tn(this)},Fe().prototype.add=function(t){return this.throwIfDisposed(),en(this,t)},Fe().prototype.all=function(t,e){return this.throwIfDisposed(),nn(this,t,e)},Fe().prototype.any=function(t,e){return this.throwIfDisposed(),sn(this,t,e)},Fe().prototype.argMax=function(t){return this.throwIfDisposed(),rn(this,t)},Fe().prototype.argMin=function(t){return this.throwIfDisposed(),an(this,t)},Fe().prototype.asScalar=function(){return this.throwIfDisposed(),Vt(1===this.size,()=>"The array must have only 1 element."),wn(this,[])},Fe().prototype.asType=function(t){return this.throwIfDisposed(),pn(this,t)},Fe().prototype.as1D=function(){return this.throwIfDisposed(),wn(this,[this.size])},Fe().prototype.as2D=function(t,e){return this.throwIfDisposed(),wn(this,[t,e])},Fe().prototype.as3D=function(t,e,n){return this.throwIfDisposed(),wn(this,[t,e,n])},Fe().prototype.as4D=function(t,e,n,s){return this.throwIfDisposed(),wn(this,[t,e,n,s])},Fe().prototype.as5D=function(t,e,n,s,i){return this.throwIfDisposed(),wn(this,[t,e,n,s,i])},Fe().prototype.asin=function(){return this.throwIfDisposed(),on(this)},Fe().prototype.asinh=function(){return this.throwIfDisposed(),ln(this)},Fe().prototype.atan=function(){return this.throwIfDisposed(),un(this)},Fe().prototype.atan2=function(t){return this.throwIfDisposed(),hn(this,t)},Fe().prototype.atanh=function(){return this.throwIfDisposed(),cn(this)},Fe().prototype.avgPool=function(t,e,n,s){return this.throwIfDisposed(),kn(this,t,e,n,s)},Fe().prototype.batchToSpaceND=function(t,e){return this.throwIfDisposed(),zn(this,t,e)},Fe().prototype.batchNorm=function(t,e,n,s,i){return this.throwIfDisposed(),Dn(this,t,e,n,s,i)},Fe().prototype.broadcastTo=function(t){return this.throwIfDisposed(),Tn(this,t)},Fe().prototype.cast=function(t){return this.throwIfDisposed(),pn(this,t)},Fe().prototype.ceil=function(){return this.throwIfDisposed(),En(this)},Fe().prototype.clipByValue=function(t,e){return this.throwIfDisposed(),Fn(this,t,e)},Fe().prototype.concat=function(t,e){return this.throwIfDisposed(),t instanceof Ee&&(t=[t]),vn([this,...t],e)},Fe().prototype.conv1d=function(t,e,n,s,i,r){return this.throwIfDisposed(),Ln(this,t,e,n,s,i,r)},Fe().prototype.conv2dTranspose=function(t,e,n,s,i){return this.throwIfDisposed(),Mn(this,t,e,n,s,i)},Fe().prototype.conv2d=function(t,e,n,s,i,r){return this.throwIfDisposed(),_n(this,t,e,n,s,i,r)},Fe().prototype.cos=function(){return this.throwIfDisposed(),Bn(this)},Fe().prototype.cosh=function(){return this.throwIfDisposed(),Pn(this)},Fe().prototype.cumsum=function(t,e,n){return this.throwIfDisposed(),Wn(this,t,e,n)},Fe().prototype.depthToSpace=function(t,e){return this.throwIfDisposed(),Un(this,t,e)},Fe().prototype.depthwiseConv2d=function(t,e,n,s,i,r){return this.throwIfDisposed(),Kn(this,t,e,n,s,i,r)},Fe().prototype.dilation2d=function(t,e,n,s,i){return this.throwIfDisposed(),jn(this,t,e,n,s,i)},Fe().prototype.divNoNan=function(t){return this.throwIfDisposed(),Yn(this,t)},Fe().prototype.div=function(t){return this.throwIfDisposed(),qn(this,t)},Fe().prototype.dot=function(t){return this.throwIfDisposed(),Qn(this,t)},Fe().prototype.elu=function(){return this.throwIfDisposed(),ts(this)},Fe().prototype.equal=function(t){return this.throwIfDisposed(),Jn(this,t)},Fe().prototype.erf=function(){return this.throwIfDisposed(),es(this)},Fe().prototype.exp=function(){return this.throwIfDisposed(),ns(this)},Fe().prototype.expandDims=function(t){return this.throwIfDisposed(),ss(this,t)},Fe().prototype.expm1=function(){return this.throwIfDisposed(),is(this)},Fe().prototype.fft=function(){return this.throwIfDisposed(),gi(this)},Fe().prototype.flatten=function(){return this.throwIfDisposed(),wn(this,[this.size])},Fe().prototype.floor=function(){return this.throwIfDisposed(),as(this)},Fe().prototype.floorDiv=function(t){return this.throwIfDisposed(),Vn(this,t)},Fe().prototype.gather=function(t,e){return this.throwIfDisposed(),os(this,t,e)},Fe().prototype.greaterEqual=function(t){return this.throwIfDisposed(),us(this,t)},Fe().prototype.greater=function(t){return this.throwIfDisposed(),ls(this,t)},Fe().prototype.ifft=function(){return this.throwIfDisposed(),mi(this)},Fe().prototype.irfft=function(){return this.throwIfDisposed(),yi(this)},Fe().prototype.isFinite=function(){return this.throwIfDisposed(),cs(this)},Fe().prototype.isInf=function(){return this.throwIfDisposed(),ps(this)},Fe().prototype.isNaN=function(){return this.throwIfDisposed(),ds(this)},Fe().prototype.leakyRelu=function(t){return this.throwIfDisposed(),fs(this,t)},Fe().prototype.lessEqual=function(t){return this.throwIfDisposed(),ms(this,t)},Fe().prototype.less=function(t){return this.throwIfDisposed(),gs(this,t)},Fe().prototype.localResponseNormalization=function(t,e,n,s){return this.throwIfDisposed(),ys(this,t,e,n,s)},Fe().prototype.logSigmoid=function(){return this.throwIfDisposed(),Ss(this)},Fe().prototype.logSoftmax=function(t){return this.throwIfDisposed(),Cs(this,t)},Fe().prototype.logSumExp=function(t,e){return this.throwIfDisposed(),Ts(this,t,e)},Fe().prototype.log=function(){return this.throwIfDisposed(),bs(this)},Fe().prototype.log1p=function(){return this.throwIfDisposed(),ws(this)},Fe().prototype.logicalAnd=function(t){return this.throwIfDisposed(),Es(this,t)},Fe().prototype.logicalNot=function(){return this.throwIfDisposed(),Fs(this)},Fe().prototype.logicalOr=function(t){return this.throwIfDisposed(),$s(this,t)},Fe().prototype.logicalXor=function(t){return this.throwIfDisposed(),_s(this,t)},Fe().prototype.matMul=function(t,e,n){return this.throwIfDisposed(),Sn(this,t,e,n)},Fe().prototype.maxPool=function(t,e,n,s){return this.throwIfDisposed(),Ls(this,t,e,n,s)},Fe().prototype.max=function(t,e){return this.throwIfDisposed(),Is(this,t,e)},Fe().prototype.maximum=function(t){return this.throwIfDisposed(),Rs(this,t)},Fe().prototype.mean=function(t,e){return this.throwIfDisposed(),Ms(this,t,e)},Fe().prototype.min=function(t,e){return this.throwIfDisposed(),Os(this,t,e)},Fe().prototype.minimum=function(t){return this.throwIfDisposed(),Bs(this,t)},Fe().prototype.mirrorPad=function(t,e){return this.throwIfDisposed(),Ps(this,t,e)},Fe().prototype.mod=function(t){return this.throwIfDisposed(),Ws(this,t)},Fe().prototype.mul=function(t){return this.throwIfDisposed(),In(this,t)},Fe().prototype.neg=function(){return this.throwIfDisposed(),xs(this)},Fe().prototype.norm=function(t,e,n){return this.throwIfDisposed(),Fi(this,t,e,n)},Fe().prototype.notEqual=function(t){return this.throwIfDisposed(),Ks(this,t)},Fe().prototype.oneHot=function(t,e=1,n=0){return this.throwIfDisposed(),js(this,t,e,n)},Fe().prototype.onesLike=function(){return this.throwIfDisposed(),Gs(this)},Fe().prototype.pad=function(t,e){return this.throwIfDisposed(),Hs(this,t,e)},Fe().prototype.pool=function(t,e,n,s,i){return this.throwIfDisposed(),Zs(this,t,e,n,s,i)},Fe().prototype.pow=function(t){return this.throwIfDisposed(),Xs(this,t)},Fe().prototype.prelu=function(t){return this.throwIfDisposed(),Ys(this,t)},Fe().prototype.prod=function(t,e){return this.throwIfDisposed(),Qs(this,t,e)},Fe().prototype.reciprocal=function(){return this.throwIfDisposed(),ei(this)},Fe().prototype.relu=function(){return this.throwIfDisposed(),ni(this)},Fe().prototype.relu6=function(){return this.throwIfDisposed(),si(this)},Fe().prototype.reshapeAs=function(t){return this.throwIfDisposed(),wn(this,t.shape)},Fe().prototype.reshape=function(t){return this.throwIfDisposed(),wn(this,t)},Fe().prototype.resizeBilinear=function(t,e,n){return this.throwIfDisposed(),Ri(this,t,e,n)},Fe().prototype.resizeNearestNeighbor=function(t,e,n){return this.throwIfDisposed(),Mi(this,t,e,n)},Fe().prototype.reverse=function(t){return this.throwIfDisposed(),ii(this,t)},Fe().prototype.rfft=function(){return this.throwIfDisposed(),wi(this)},Fe().prototype.round=function(){return this.throwIfDisposed(),ri(this)},Fe().prototype.rsqrt=function(){return this.throwIfDisposed(),ai(this)},Fe().prototype.selu=function(){return this.throwIfDisposed(),ui(this)},Fe().prototype.separableConv2d=function(t,e,n,s,i,r){return this.throwIfDisposed(),hi(this,t,e,n,s,i,r)},Fe().prototype.sigmoid=function(){return this.throwIfDisposed(),Nn(this)},Fe().prototype.sign=function(){return this.throwIfDisposed(),ci(this)},Fe().prototype.sin=function(){return this.throwIfDisposed(),pi(this)},Fe().prototype.sinh=function(){return this.throwIfDisposed(),di(this)},Fe().prototype.slice=function(t,e){return this.throwIfDisposed(),An(this,t,e)},Fe().prototype.softmax=function(t){return this.throwIfDisposed(),fi(this,t)},Fe().prototype.softplus=function(){return this.throwIfDisposed(),vs(this)},Fe().prototype.spaceToBatchND=function(t,e){return this.throwIfDisposed(),Js(this,t,e)},Fe().prototype.split=function(t,e){return this.throwIfDisposed(),bi(this,t,e)},Fe().prototype.sqrt=function(){return this.throwIfDisposed(),ki(this)},Fe().prototype.square=function(){return this.throwIfDisposed(),Us(this)},Fe().prototype.squaredDifference=function(t){return this.throwIfDisposed(),xi(this,t)},Fe().prototype.squeeze=function(t){return this.throwIfDisposed(),vi(this,t)},Fe().prototype.stack=function(t,e){this.throwIfDisposed();const n=t instanceof Ee?[this,t]:[this,...t];return Si(n,e)},Fe().prototype.step=function(t){return this.throwIfDisposed(),Ii(this,t)},Fe().prototype.stridedSlice=function(t,e,n,s,i,r,a,o){return this.throwIfDisposed(),Ni(this,t,e,n,s,i,r,a,o)},Fe().prototype.sub=function(t){return this.throwIfDisposed(),Ns(this,t)},Fe().prototype.sum=function(t,e){return this.throwIfDisposed(),As(this,t,e)},Fe().prototype.tan=function(){return this.throwIfDisposed(),Ai(this)},Fe().prototype.tanh=function(){return this.throwIfDisposed(),Cn(this)},Fe().prototype.tile=function(t){return this.throwIfDisposed(),rs(this,t)},Fe().prototype.toBool=function(){return this.throwIfDisposed(),pn(this,"bool")},Fe().prototype.toFloat=function(){return this.throwIfDisposed(),pn(this,"float32")},Fe().prototype.toInt=function(){return this.throwIfDisposed(),pn(this,"int32")},Fe().prototype.topk=function(t,e){return this.throwIfDisposed(),Ci(this,t,e)},Fe().prototype.transpose=function(t){return this.throwIfDisposed(),Ei(this,t)},Fe().prototype.unique=function(t){return this.throwIfDisposed(),zi(this,t)},Fe().prototype.unsortedSegmentSum=function(t,e){return this.throwIfDisposed(),Di(this,t,e)},Fe().prototype.unstack=function(t){return this.throwIfDisposed(),Ti(this,t)},Fe().prototype.where=function(t,e){return this.throwIfDisposed(),Zn(t,this,e)},Fe().prototype.zerosLike=function(){return this.throwIfDisposed(),Xn(this)};const Oi={kernelName:"Abs",inputsToSave:["x"],gradFunc:(t,e)=>{const[n]=e;return{x:()=>In(t,Ii(pn(n,"float32"),-1))}}},Bi={kernelName:"Acos",inputsToSave:["x"],gradFunc:(t,e)=>{const[n]=e;return{x:()=>{const e=Us(pn(n,"float32")),s=ki(Ns(li(1),e));return xs(qn(t,s))}}}},Pi={kernelName:"Acosh",inputsToSave:["x"],gradFunc:(t,e)=>{const[n]=e;return{x:()=>{const e=ki(Ns(Us(pn(n,"float32")),1));return qn(t,e)}}}},Wi={kernelName:"Add",inputsToSave:["a","b"],gradFunc:(t,e)=>{const[n,s]=e,i=Hn(n.shape,s.shape);return{a:()=>{let e=t;const s=Gn(n.shape,i);return s.length>0&&(e=As(e,s)),wn(e,n.shape)},b:()=>{let e=t;const n=Gn(s.shape,i);return n.length>0&&(e=As(e,n)),wn(e,s.shape)}}}},Ui={kernelName:"AddN",saveAllInputs:!0,gradFunc:(t,e)=>{const n={};return e.forEach((e,s)=>{n[s]=()=>t.clone()}),n}},Ki={kernelName:"ArgMax",inputsToSave:["x"],gradFunc:(t,e)=>{const[n]=e;return{x:()=>Xn(n)}}},ji={kernelName:"ArgMin",inputsToSave:["x"],gradFunc:(t,e)=>{const[n]=e;return{x:()=>Xn(n)}}},Vi={kernelName:"Asin",inputsToSave:["x"],gradFunc:(t,e)=>{const[n]=e;return{x:()=>qn(t,ki(Ns(li(1),Us(pn(n,"float32")))))}}},qi={kernelName:"Asinh",inputsToSave:["x"],gradFunc:(t,e)=>{const[n]=e;return{x:()=>{const e=ki(en(li(1),Us(pn(n,"float32"))));return qn(t,e)}}}},Gi={kernelName:"Atan2",inputsToSave:["a","b"],gradFunc:(t,e)=>{const[n,s]=e,i=Hn(n.shape,s.shape);return{a:()=>{const e=en(Us(n),Us(s));let r=In(t,qn(s,e));const a=Gn(n.shape,i);return a.length>0&&(r=As(r,a)),wn(r,n.shape)},b:()=>{const e=en(Us(n),Us(s));let r=xs(In(t,qn(n,e)));const a=Gn(s.shape,i);return a.length>0&&(r=As(r,a)),wn(r,s.shape)}}}},Hi={kernelName:"Atan",inputsToSave:["x"],gradFunc:(t,e)=>{const[n]=e;return{x:()=>qn(t,en(Us(pn(n,"float32")),1))}}},Ji={kernelName:"Atanh",inputsToSave:["x"],gradFunc:(t,e)=>{const[n]=e;return{x:()=>qn(t,Ns(li(1),Us(pn(n,"float32"))))}}};const Zi=Xe({avgPool3dGrad_:function(t,e,n,s,i,r){const a=Je(t,"dy","avgPool3dGrad"),o=Je(e,"input","avgPool3dGrad");let l=a,u=o,h=!1;4===o.rank&&(h=!0,l=wn(a,[1,a.shape[0],a.shape[1],a.shape[2],a.shape[3]]),u=wn(o,[1,o.shape[0],o.shape[1],o.shape[2],o.shape[3]])),Vt(5===l.rank,()=>"Error in avgPool3dGrad: dy must be rank 5 but got rank "+l.rank+"."),Vt(5===u.rank,()=>"Error in avgPool3dGrad: input must be rank 5 but got rank "+u.rank+"."),null!=r&&Vt(Zt(i),()=>`Error in avgPool3dGrad: pad must be an integer when using, dimRoundingMode ${r} but got pad ${i}.`);const c={dy:l,input:u},p={filterSize:n,strides:s,pad:i,dimRoundingMode:r},d=Ve.runKernel("AvgPool3DGrad",c,p);return h?wn(d,[d.shape[1],d.shape[2],d.shape[3],d.shape[4]]):d}}),Xi={kernelName:"AvgPool3D",inputsToSave:["x"],gradFunc:(t,e,n)=>{const[s]=e,{filterSize:i,strides:r,pad:a,dimRoundingMode:o}=n;return{x:()=>Zi(t,s,i,r,a,o)}}};const Yi=Xe({avgPoolGrad_:function(t,e,n,s,i){const r=Je(t,"dy","avgPoolGrad"),a=Je(e,"input","avgPoolGrad");Vt(a.rank===r.rank,()=>`Rank of input (${a.rank}) does not match rank of dy (${r.rank})`);let o=a,l=r,u=!1;3===a.rank&&(u=!0,o=wn(a,[1,a.shape[0],a.shape[1],a.shape[2]]),l=wn(r,[1,r.shape[0],r.shape[1],r.shape[2]])),Vt(4===l.rank,()=>"Error in avgPoolGrad: dy must be rank 4 but got rank "+l.rank+"."),Vt(4===o.rank,()=>"Error in avgPoolGrad: input must be rank 4 but got rank "+o.rank+".");const h={dy:l,input:o},c={filterSize:n,strides:s,pad:i},p=Ve.runKernel("AvgPoolGrad",h,c);return u?wn(p,[p.shape[1],p.shape[2],p.shape[3]]):p}}),Qi={kernelName:"AvgPool",inputsToSave:["x"],gradFunc:(t,e,n)=>{const[s]=e,{filterSize:i,strides:r,pad:a}=n;return{x:()=>Yi(t,s,i,r,a)}}},tr={kernelName:"BatchMatMul",inputsToSave:["a","b"],gradFunc:(t,e,n)=>{const[s,i]=e,{transposeA:r,transposeB:a}=n;return r||a?!r&&a?{a:()=>Sn(t,i,!1,!1),b:()=>Sn(t,s,!0,!1)}:r&&!a?{a:()=>Sn(i,t,!1,!0),b:()=>Sn(s,t,!1,!1)}:{a:()=>Sn(i,t,!0,!0),b:()=>Sn(t,s,!0,!0)}:{a:()=>Sn(t,i,!1,!0),b:()=>Sn(s,t,!0,!1)}}},er={kernelName:"BatchToSpaceND",gradFunc:(t,e,n)=>{const{blockShape:s,crops:i}=n;return{x:()=>Js(t,s,i)}}},nr={kernelName:"BroadcastTo",gradFunc:(t,e,n)=>{const s=n,i=s.inputShape,r=s.shape,a=Array.from(r);for(let t=i.length-1;t>=0;t--)if(i[t]===r[t])a[t]=1;else if(1!==i[t])throw new Error(`broadcastTo(): [${i}] cannot be broadcast to [${r}].`);const o=[];for(let t=0;t<a.length;t++)a[t]>1&&o.push(t);return{x:()=>As(t,o,!0)}}},sr={kernelName:"Cast",gradFunc:t=>({x:()=>t.clone()})},ir={kernelName:"Ceil",gradFunc:t=>({x:()=>Xn(t)})},rr={kernelName:"ClipByValue",inputsToSave:["x"],gradFunc:(t,e,n)=>{const[s]=e,{clipValueMin:i,clipValueMax:r}=n;return{x:()=>Zn(Es(us(s,i),ms(s,r)),t,Xn(t))}}},ar={kernelName:"ComplexAbs",inputsToSave:["x"],gradFunc:Oi.gradFunc},or={kernelName:"Concat",saveAllInputs:!0,gradFunc:(t,e,n)=>{const s=e.map(t=>t.shape),{axis:i}=n,r=Yt(i,e[0].shape)[0],a=s.map(t=>t[r]);return bi(t,a,r).map(t=>()=>t)}},lr={kernelName:"Conv2D",inputsToSave:["x","filter"],gradFunc:(t,e,n)=>{const[s,i]=e,{dilations:r,strides:a,pad:o,dataFormat:l}=n;return Vt(yn(r),()=>`Error in gradient of conv2D: dilation rates greater than 1 are not yet supported in gradients. Got dilations '${r}'`),{x:()=>Rn(s.shape,t,i,a,o,l),filter:()=>$i(s,t,i.shape,a,o,l)}}},ur={kernelName:"Conv2DBackpropInput",inputsToSave:["dy","filter"],gradFunc:(t,e,n)=>{const[s,i]=e,{strides:r,pad:a,dataFormat:o,dimRoundingMode:l}=n;return{dy:()=>_n(t,i,r,a,o,1,l),filter:()=>$i(t,s,i.shape,r,a,o,l)}}};const hr=Xe({conv3DBackpropFilter_:function(t,e,n,s,i){let r=t;4===t.rank&&(r=wn(t,[1,t.shape[0],t.shape[1],t.shape[2],t.shape[3]]));let a=e;4===a.rank&&(a=wn(e,[1,e.shape[0],e.shape[1],e.shape[2],e.shape[3]])),Vt(5===r.rank,()=>"Error in conv3dDerFilter: input must be rank 5, but got shape "+r.shape+"."),Vt(5===a.rank,()=>"Error in conv3dDerFilter: dy must be rank 5, but got shape "+a.shape+"."),Vt(5===n.length,()=>"Error in conv3dDerFilter: filterShape must be length 5, but got "+n+"."),Vt(r.shape[4]===n[3],()=>`Error in conv3dDerFilter: depth of input ${r.shape[4]}) must match input depth in filter (${n[3]}.`),Vt(a.shape[4]===n[4],()=>`Error in conv3dDerFilter: depth of dy (${a.shape[4]}) must match output depth for filter (${n[4]}).`);const o={x:r,dy:a},l={strides:s,pad:i,filterShape:n};return Ve.runKernel("Conv3DBackpropFilterV2",o,l)}}),cr={kernelName:"Conv3D",inputsToSave:["x","filter"],gradFunc:(t,e,n)=>{const{dilations:s,strides:i,pad:r}=n;Vt(yn(s),()=>`Error in gradient of conv3D: dilation rates greater than 1 are not yet supported in gradients. Got dilations '${s}'`);const[a,o]=e;return{x:()=>On(a.shape,t,o,i,r),filter:()=>hr(a,t,o.shape,i,r)}}},pr={kernelName:"Cos",inputsToSave:["x"],gradFunc:(t,e)=>{const[n]=e;return{x:()=>In(xs(pi(pn(n,"float32"))),t)}}},dr={kernelName:"Cosh",inputsToSave:["x"],gradFunc:(t,e)=>{const[n]=e;return{x:()=>In(di(pn(n,"float32")),t)}}},fr={kernelName:"Cumsum",inputsToSave:["x"],gradFunc:(t,e,n)=>{const[s]=e,{axis:i,exclusive:r,reverse:a}=n;return{x:()=>{const e=function(t,e){if(function(t,e){for(let n=0;n<t.length;++n)if(t[t.length-n-1]!==e-1-n)return!1;return!0}(t,e))return null;const n=[];for(let s=0;s<e;++s)-1===t.indexOf(s)&&n.push(s);return t.forEach(t=>n.push(t)),n}([i],s.rank);let n=Wn(t,i,r,!a);return null!=e&&(n=Ei(n,e)),n}}}},gr={kernelName:"DepthwiseConv2dNative",inputsToSave:["x","filter"],gradFunc:(t,e,n)=>{const{dilations:s,strides:i,pad:r,dimRoundingMode:a}=n,o=null==s?[1,1]:s;Vt(yn(o),()=>`Error in gradient of depthwiseConv2dNative: dilation rates greater than 1 are not yet supported. Got dilations '${o}'`);const[l,u]=e;return Vt(4===l.rank,()=>`Error in gradient of depthwiseConv2dNative: input must be rank 4, but got rank ${l.rank}.`),Vt(4===u.rank,()=>`Error in gradient of depthwiseConv2dNative: filter must be rank 4, but got rank ${u.rank}.`),Vt(l.shape[3]===u.shape[2],()=>`Error in gradient of depthwiseConv2d: number of input channels (${l.shape[3]}) must match the inChannels dimension in filter ${u.shape[2]}.`),Vt(bn(i,o),()=>`Error in gradient of depthwiseConv2d: Either strides or dilations must be  1. Got strides ${i} and dilations '${o}'.`),null!=a&&Vt(Zt(r),()=>`Error in depthwiseConv2d: pad must be an integer when using, dimRoundingMode ${a} but got pad ${r}.`),{x:()=>Li(l.shape,t,u,i,r,s,a),filter:()=>_i(l,t,u.shape,i,r,s,a)}}},mr={kernelName:"Dilation2D",inputsToSave:["x","filter"],gradFunc:(t,e,n)=>{const[s,i]=e,r={x:s,filter:i,dy:t},a={x:s,filter:i,dy:t};return{x:()=>Ve.runKernel("Dilation2DBackpropInput",r,n),filter:()=>Ve.runKernel("Dilation2DBackpropFilter",a,n)}}},yr={kernelName:"Elu",outputsToSave:[!0],gradFunc:(t,e)=>{const[n]=e,s={dy:t,y:n};return{x:()=>Ve.runKernel("EluGrad",s)}}},br={kernelName:"Erf",inputsToSave:["x"],gradFunc:(t,e)=>{const[n]=e,s=In(ns(xs(Us(n))),2/Math.sqrt(Math.PI));return{x:()=>In(t,s)}}},wr={kernelName:"Exp",outputsToSave:[!0],gradFunc:(t,e)=>{const[n]=e;return{x:()=>In(t,n)}}},kr={kernelName:"ExpandDims",inputsToSave:["input"],gradFunc:(t,e)=>{const[n]=e;return{input:()=>wn(t,n.shape)}}},xr={kernelName:"Expm1",inputsToSave:["x"],gradFunc:(t,e)=>{const[n]=e;return{x:()=>In(t,ns(n))}}},vr={kernelName:"Floor",gradFunc:t=>({x:()=>Xn(t)})},Sr={kernelName:"FloorDiv",inputsToSave:["a","b"],gradFunc:(t,e)=>{const[n,s]=e,i=Hn(n.shape,s.shape);return{a:()=>{const e=qn(t,pn(s,"float32")),r=Gn(n.shape,i);return r.length>0?wn(As(e,r),n.shape):e},b:()=>{let e=In(t,pn(n,"float32"));const r=Gn(s.shape,i);r.length>0&&(e=wn(As(e,r),s.shape));const a=Us(s);return xs(qn(e,pn(a,"float32")))}}}},Ir={kernelName:"FusedBatchNorm",inputsToSave:["x","mean","variance","scale"],gradFunc:(t,e,n)=>{const{varianceEpsilon:s}=n,[i,r,a,o]=e,l=null==o?li(1):o,u=Gn(r.shape,i.shape),h=[];if(1===r.rank){for(let t=0;t<i.shape.length-1;++t)h.push(i.shape[t]);h.push(1)}const c=Ns(i,r),p=In(t,l),d=ai(en(a,li(s))),f=In(In(In(d,d),d),li(-.5));return{x:()=>1===r.rank?wn(In(In(t,rs(wn(d,[1,1,1,r.shape[0]]),h)),l),i.shape):wn(In(In(t,d),l),i.shape),mean:()=>{let t=In(In(d,li(-1)),p);return 1===r.rank&&(t=As(t,u)),wn(t,r.shape)},variance:()=>{let t=In(In(f,c),p);return 1===r.rank&&(t=As(t,u)),wn(t,r.shape)},scale:()=>{const e=In(c,d);let n=In(t,e);return 1===r.rank&&(n=As(n,u)),wn(n,r.shape)},offset:()=>{let e=t;return 1===r.rank&&(e=As(e,u)),wn(e,r.shape)}}}},Nr={kernelName:"GatherV2",inputsToSave:["x","indices"],gradFunc:(t,e,n)=>{const[s,i]=e,{axis:r}=n,a=Yt(r,s.shape)[0];return{x:()=>{const e=s.shape,n=i.size,o=e.slice(0,a),l=o.length,u=e.slice(r,e.length).slice(1),h=u.length,c=Ar(0,l),p=Ar(l+1,l+1+h),d=Cr([o,[n],u]),f=wn(t,d),g=wn(i,[n]),m=Cr([[l],c,p]),y=Ei(f,m);let b=Di(y,g,s.shape[a]);const w=Ds(m);return b=Ei(b,w),b},indices:()=>i}}};function Ar(t,e){const n=[];for(let s=t;s<e;++s)n.push(s);return n}function Cr(t){const e=[];for(let n=0;n<t.length;++n)for(let s=0;s<t[n].length;++s)e.push(t[n][s]);return e}const zr={kernelName:"GreaterEqual",inputsToSave:["a","b"],gradFunc:(t,e)=>{const[n,s]=e;return{a:()=>Xn(n),b:()=>Xn(s)}}},Dr={kernelName:"Identity",gradFunc:t=>({x:()=>pn(t,"float32")})},Tr={kernelName:"IsFinite",gradFunc:t=>({x:()=>Xn(t)})},Er={kernelName:"IsInf",gradFunc:t=>({x:()=>Xn(t)})},Fr={kernelName:"IsNan",gradFunc:t=>({x:()=>Xn(t)})},$r={kernelName:"LeakyRelu",inputsToSave:["x"],gradFunc:(t,e,n)=>{const[s]=e,{alpha:i}=n,r=ls(s,0);return{x:()=>Zn(r,t,In(t,i))}}},_r={kernelName:"Log1p",inputsToSave:["x"],gradFunc:(t,e)=>{const[n]=e;return{x:()=>qn(t,en(n,1))}}},Lr={kernelName:"Log",inputsToSave:["x"],gradFunc:(t,e)=>{const[n]=e;return{x:()=>qn(t,pn(n,"float32"))}}},Rr={kernelName:"LogSoftmax",inputsToSave:[],outputsToSave:[!0],gradFunc:(t,e,n)=>{const[s]=e,{axis:i}=n;return{logits:()=>{const e=ns(s);return Ns(t,In(As(t,i,!0),e))}}}};const Mr=Xe({localResponseNormalizationBackprop_:function(t,e,n,s=5,i=1,r=1,a=.5){const o={x:t,y:e,dy:n},l={depthRadius:s,bias:i,alpha:r,beta:a};return Ve.runKernel("LRNGrad",o,l)}}),Or={kernelName:"LRN",inputsToSave:["x"],outputsToSave:[!0],gradFunc:(t,e,n)=>{const[s,i]=e,{depthRadius:r,bias:a,alpha:o,beta:l}=n;return{x:()=>Mr(s,i,t,r,a,o,l)}}};function Br(t,e,n,s){return e.rank<n.rank&&(e=wn(e,zs(e.shape,s))),t.rank<n.rank&&(t=wn(t,zs(t.shape,s))),{x:()=>In(t,pn(Jn(n,e),t.dtype))}}const Pr={kernelName:"Max",inputsToSave:["x"],outputsToSave:[!0],gradFunc:(t,e,n)=>{const s=n,{reductionIndices:i}=s,r=e[0],a=Br(t,e[1],r,Yt(i,r.shape));return{x:()=>a.x()}}},Wr={kernelName:"Maximum",inputsToSave:["a","b"],gradFunc:(t,e)=>{const[n,s]=e;return{a:()=>In(t,pn(us(n,s),"float32")),b:()=>In(t,pn(gs(n,s),"float32"))}}};const Ur=Xe({maxPool3dGrad_:function(t,e,n,s,i,r,a){const o=Je(t,"dy","maxPool3dGrad"),l=Je(e,"input","maxPool3dGrad"),u=Je(n,"output","maxPool3dGrad");let h=o,c=l,p=u,d=!1;4===l.rank&&(d=!0,h=wn(o,[1,o.shape[0],o.shape[1],o.shape[2],o.shape[3]]),c=wn(l,[1,l.shape[0],l.shape[1],l.shape[2],l.shape[3]]),p=wn(u,[1,u.shape[0],u.shape[1],u.shape[2],u.shape[3]])),Vt(5===h.rank,()=>"Error in maxPool3dGrad: dy must be rank 5 but got rank "+h.rank+"."),Vt(5===c.rank,()=>"Error in maxPool3dGrad: input must be rank 5 but got rank "+c.rank+"."),Vt(5===p.rank,()=>"Error in maxPool3dGrad: output must be rank 5 but got rank "+p.rank+"."),null!=a&&Vt(Zt(r),()=>`Error in maxPool3dGrad: pad must be an integer when using, dimRoundingMode ${a} but got pad ${r}.`);const f={dy:h,input:c,output:p},g={filterSize:s,strides:i,pad:r,dimRoundingMode:a},m=Ve.runKernel("MaxPool3DGrad",f,g);return d?wn(m,[m.shape[1],m.shape[2],m.shape[3],m.shape[4]]):m}}),Kr={kernelName:"MaxPool3D",inputsToSave:["x"],outputsToSave:[!0],gradFunc:(t,e,n)=>{const[s,i]=e,{filterSize:r,strides:a,pad:o,dimRoundingMode:l}=n;return{x:()=>Ur(t,s,i,r,a,o,l)}}};const jr=Xe({maxPoolGrad_:function(t,e,n,s,i,r,a){const o=Je(t,"dy","maxPoolGrad"),l=Je(e,"input","maxPoolGrad"),u=Je(n,"output","maxPoolGrad");Vt(l.rank===o.rank,()=>`Rank of input (${l.rank}) does not match rank of dy (${o.rank})`),Vt(4===o.rank,()=>"Error in maxPoolGrad: dy must be rank 4 but got rank "+o.rank+"."),Vt(4===l.rank,()=>"Error in maxPoolGrad: input must be rank 4 but got rank "+l.rank+"."),null!=a&&Vt(Zt(r),()=>`Error in maxPoolGrad: pad must be an integer when using, dimRoundingMode ${a} but got pad ${r}.`);const h={dy:o,input:l,output:u},c={filterSize:s,strides:i,pad:r,dimRoundingMode:a};return Ve.runKernel("MaxPoolGrad",h,c)}}),Vr={kernelName:"PadV2",inputsToSave:["x"],gradFunc:(t,e,n)=>{const s=e[0],{paddings:i}=n,r=i.map(t=>t[0]);return{x:()=>An(t,r,s.shape)}}};const qr={kernelName:"SpaceToBatchND",gradFunc:(t,e,n)=>{const{blockShape:s,paddings:i}=n;return{x:()=>zn(t,s,i)}}},Gr={kernelName:"SplitV",gradFunc:(t,e,n)=>{const{axis:s}=n;return{x:()=>vn(t,s)}}};const Hr=[Oi,Bi,Pi,Wi,Ui,Ki,ji,Vi,qi,Gi,Hi,Ji,Xi,Qi,tr,er,nr,sr,ir,rr,ar,or,ur,lr,cr,pr,dr,fr,gr,mr,{kernelName:"RealDiv",inputsToSave:["a","b"],gradFunc:(t,e)=>{const[n,s]=e,i=Hn(n.shape,s.shape);return{a:()=>{const e=qn(t,pn(s,"float32")),r=Gn(n.shape,i);return r.length>0?wn(As(e,r),n.shape):e},b:()=>{let e=In(t,pn(n,"float32"));const r=Gn(s.shape,i);r.length>0&&(e=wn(As(e,r),s.shape));const a=Us(s);return xs(qn(e,pn(a,"float32")))}}}},yr,br,wr,kr,xr,Sr,vr,Ir,Nr,zr,Dr,Tr,Er,Fr,$r,_r,Lr,Rr,Or,Pr,Pr,Wr,Kr,{kernelName:"MaxPool",inputsToSave:["x"],outputsToSave:[!0],gradFunc:(t,e,n)=>{const[s,i]=e,{filterSize:r,strides:a,pad:o}=n;return{x:()=>jr(t,s,i,r,a,o)}}},{kernelName:"Mean",inputsToSave:["x"],gradFunc:(t,e,n)=>{const[s]=e,{axis:i}=n,r=Yt(i,s.shape),a=Ht(function(t,e){const n=[],s=t.length;for(let i=0;i<s;i++)-1===e.indexOf(i)&&n.push(t[i]);return[n,e.map(e=>t[e])]}(s.shape,r)[1]);return{x:()=>{const e=s.shape.slice();r.forEach(t=>{e[t]=1});const n=wn(t,e);return qn(In(n,qs(s.shape,"float32")),a)}}}},{kernelName:"Min",inputsToSave:["x"],outputsToSave:[!0],gradFunc:(t,e,n)=>{const s=n,{axis:i}=s,[r,a]=e,o=Br(t,a,r,Yt(i,r.shape));return{x:()=>o.x()}}},{kernelName:"Minimum",inputsToSave:["a","b"],gradFunc:(t,e)=>{const[n,s]=e;return{a:()=>In(t,pn(ms(n,s),"float32")),b:()=>In(t,pn(ls(n,s),"float32"))}}},{kernelName:"MirrorPad",inputsToSave:["x"],gradFunc:(t,e,n)=>{const s=e[0],{paddings:i}=n,r=i.map(t=>t[0]);return{x:()=>An(t,r,s.shape)}}},{kernelName:"Mod",inputsToSave:["a","b"],gradFunc:(t,e)=>{const[n,s]=e,i=Hn(n.shape,s.shape);return{a:()=>{const e=Gn(n.shape,i);return e.length>0?wn(As(t,e),n.shape):t},b:()=>{const e=In(t,xs(as(qn(n,s)))),r=Gn(s.shape,i);return r.length>0?wn(As(e,r),s.shape):e}}}},{kernelName:"Multiply",inputsToSave:["a","b"],gradFunc:(t,e)=>{const[n,s]=e,i=Hn(n.shape,s.shape);return{a:()=>{const e=In(t,pn(s,"float32")),r=Gn(n.shape,i);return r.length>0?wn(As(e,r),n.shape):e},b:()=>{const e=In(t,pn(n,"float32")),r=Gn(s.shape,i);return r.length>0?wn(As(e,r),s.shape):e}}}},{kernelName:"Neg",gradFunc:t=>({x:()=>xs(t)})},{kernelName:"OneHot",inputsToSave:["indices"],gradFunc:(t,e)=>{const n=e[0];return{indices:()=>Vs(n.shape,"float32")}}},{kernelName:"OnesLike",gradFunc:t=>({x:()=>Xn(t)})},{kernelName:"Pack",saveAllInputs:!0,gradFunc:(t,e,n)=>{const{axis:s}=n;return Ti(t,s).map(t=>()=>t)}},Vr,Vr,{kernelName:"Pow",inputsToSave:["a","b"],outputsToSave:[!0],gradFunc:(t,e)=>{const[n,s,i]=e,r=n,a=s,o=Hn(r.shape,a.shape);return{a:()=>{const e=pn(a,"float32");let n=In(t,In(e,Xs(r,Ns(e,li(1)))));const s=Gn(r.shape,o);return s.length>0&&(n=As(n,s)),wn(n,r.shape)},b:()=>{const e=ls(r,0),n=Zn(e,bs(r),Xn(r));let s=In(t,In(i,n));const l=Gn(a.shape,o);return l.length>0&&(s=As(s,l)),wn(s,a.shape)}}}},{kernelName:"Prelu",inputsToSave:["x","alpha"],gradFunc:(t,e)=>{const[n,s]=e,i=ls(n,0);return{x:()=>Zn(i,t,In(t,s)),alpha:()=>{let e=Zn(i,Xn(t),In(t,n));const r=Gn(s.shape,t.shape);return r.length>0&&(e=As(e,r)),wn(e,s.shape)}}}},{kernelName:"Reciprocal",inputsToSave:["x"],gradFunc:(t,e)=>{const[n]=e;return{x:()=>qn(t,xs(Us(n)))}}},{kernelName:"Relu6",inputsToSave:["x"],gradFunc:(t,e)=>{const[n]=e,s=In(ms(n,6),Ii(n));return{x:()=>In(t,pn(s,"float32"))}}},{kernelName:"Relu",inputsToSave:["x"],gradFunc:(t,e)=>{const[n]=e;return{x:()=>In(t,pn(Ii(n),"float32"))}}},{kernelName:"Reshape",inputsToSave:["x"],gradFunc:(t,e)=>{const[n]=e;return{x:()=>wn(t,n.shape)}}},{kernelName:"ResizeBilinear",inputsToSave:["images"],gradFunc:(t,e,n)=>{const[s]=e,i={dy:t,images:s};return{images:()=>Ve.runKernel("ResizeBilinearGrad",i,n)}}},{kernelName:"ResizeNearestNeighbor",inputsToSave:["images"],gradFunc:(t,e,n)=>{const[s]=e,i={dy:t,images:s};return{images:()=>Ve.runKernel("ResizeNearestNeighborGrad",i,n)}}},{kernelName:"Reverse",gradFunc:(t,e,n)=>{const{dims:s}=n,i=Yt(s,t.shape);return{x:()=>ii(t,i)}}},{kernelName:"Round",gradFunc:t=>({x:()=>Xn(t)})},{kernelName:"Rsqrt",inputsToSave:["x"],gradFunc:(t,e)=>{const[n]=e;return{x:()=>xs(qn(t,In(Xs(n,1.5),2)))}}},{kernelName:"Select",inputsToSave:["condition"],gradFunc:(t,e)=>{const[n]=e;return{condition:()=>pn(Xn(n),"float32"),t:()=>In(t,pn(n,t.dtype)),e:()=>In(t,pn(Fs(n),t.dtype))}}},{kernelName:"Selu",inputsToSave:["x"],gradFunc:(t,e)=>{const[n]=e;return{x:()=>{const e=ls(n,li(0)),s=li(1.7580993408473768),i=li(1.0507009873554805),r=In(t,i),a=In(In(t,s),ns(pn(n,"float32")));return Zn(e,r,a)}}}},{kernelName:"Sigmoid",outputsToSave:[!0],gradFunc:(t,e)=>{const[n]=e;return{x:()=>In(t,In(n,Ns(li(1),n)))}}},{kernelName:"Sign",gradFunc:t=>({x:()=>Xn(t)})},{kernelName:"Sin",inputsToSave:["x"],gradFunc:(t,e)=>{const[n]=e;return{x:()=>In(Bn(pn(n,"float32")),t)}}},{kernelName:"Sinh",inputsToSave:["x"],gradFunc:(t,e)=>{const[n]=e;return{x:()=>In(Pn(pn(n,"float32")),t)}}},{kernelName:"Slice",inputsToSave:["x"],gradFunc:(t,e,n)=>{const[s]=e,{begin:i,size:r}=n,a=s.shape,[o,l]=function(t,e,n){let s;const i=t.shape.length;let r;return s="number"==typeof e?[e,...new Array(i-1).fill(0)]:e.length<i?e.concat(new Array(i-e.length).fill(0)):e.slice(),s.forEach(t=>{Vt(-1!==t,()=>"slice() does not support negative begin indexing.")}),r=null==n?new Array(i).fill(-1):"number"==typeof n?[n,...new Array(i-1).fill(-1)]:n.length<i?n.concat(new Array(i-n.length).fill(-1)):n,r=r.map((e,n)=>e>=0?e:(Vt(-1===e,()=>`Negative size values should be exactly -1 but got ${e} for the slice() size at index ${n}.`),t.shape[n]-s[n])),[s,r]}(s,i,r),u=[];for(let e=0;e<t.rank;e++)u.push([o[e],a[e]-o[e]-l[e]]);return{x:()=>Hs(t,u)}}},{kernelName:"Softmax",outputsToSave:[!0],gradFunc:(t,e,n)=>{const[s]=e,{dim:i}=n,r=In(t,s);return{logits:()=>Ns(r,In(As(r,[i],!0),s))}}},{kernelName:"Softplus",inputsToSave:["x"],gradFunc:(t,e)=>{const[n]=e;return{x:()=>In(t,Nn(n))}}},qr,qr,Gr,Gr,{kernelName:"Sqrt",inputsToSave:["x"],gradFunc:(t,e)=>{const[n]=e;return{x:()=>qn(t,In(ki(pn(n,"float32")),2))}}},{kernelName:"SquaredDifference",inputsToSave:["a","b"],gradFunc:(t,e)=>{const[n,s]=e,i=li(2);return{a:()=>In(t,In(i,Ns(n,s))),b:()=>In(t,In(i,Ns(s,n)))}}},{kernelName:"Square",inputsToSave:["x"],gradFunc:(t,e)=>{const[n]=e;return{x:()=>In(t,In(pn(n,"float32"),2))}}},{kernelName:"Step",gradFunc:t=>({x:()=>Xn(t)})},{kernelName:"Sub",inputsToSave:["a","b"],gradFunc:(t,e)=>{const[n,s]=e,i=Hn(n.shape,s.shape);return{a:()=>{let e=t;const s=Gn(n.shape,i);return s.length>0&&(e=As(e,s)),wn(e,n.shape)},b:()=>{let e=t;const n=Gn(s.shape,i);return n.length>0&&(e=As(e,n)),wn(xs(e),s.shape)}}}},{kernelName:"Sum",inputsToSave:["x"],gradFunc:(t,e,n)=>{const[s]=e,i=s.shape.slice(),{axis:r}=n;Yt(r,s.shape).forEach(t=>{i[t]=1});const a=wn(t,i),o=In(a,qs(s.shape,"float32"));return{x:()=>o}}},{kernelName:"Tan",inputsToSave:["x"],gradFunc:(t,e)=>{const[n]=e;return{x:()=>qn(t,Us(Bn(n)))}}},{kernelName:"Tanh",outputsToSave:[!0],gradFunc:(t,e)=>{const[n]=e;return{x:()=>In(Ns(li(1),Us(n)),t)}}},{kernelName:"Tile",inputsToSave:["x"],gradFunc:(t,e,n)=>{const[s]=e,{reps:i}=n;return{x:()=>{let e=Xn(s);if(1===s.rank)for(let n=0;n<i[0];++n)e=en(e,An(t,[n*s.shape[0]],[s.shape[0]]));else if(2===s.rank)for(let n=0;n<i[0];++n)for(let r=0;r<i[1];++r)e=en(e,An(t,[n*s.shape[0],r*s.shape[1]],[s.shape[0],s.shape[1]]));else if(3===s.rank)for(let n=0;n<i[0];++n)for(let r=0;r<i[1];++r)for(let a=0;a<i[2];++a)e=en(e,An(t,[n*s.shape[0],r*s.shape[1],a*s.shape[2]],[s.shape[0],s.shape[1],s.shape[2]]));else{if(4!==s.rank)throw new Error("Gradient for tile operation is not implemented for rank-"+s.rank+" tensors yet.");for(let n=0;n<i[0];++n)for(let r=0;r<i[1];++r)for(let a=0;a<i[2];++a)for(let o=0;o<i[3];++o)e=en(e,An(t,[n*s.shape[0],r*s.shape[1],a*s.shape[2],o*s.shape[3]],[s.shape[0],s.shape[1],s.shape[2],s.shape[3]]))}return e}}}},{kernelName:"Transpose",gradFunc:(t,e,n)=>{const s=n,{perm:i}=s,r=Ds(i);return{x:()=>Ei(t,r)}}},{kernelName:"Unpack",gradFunc:(t,e,n)=>{const s=n,{axis:i}=s;return{value:()=>Si(t,i)}}},{kernelName:"UnsortedSegmentSum",inputsToSave:["segmentIds"],gradFunc:(t,e)=>{const[n]=e;return{x:()=>function(t,e){const n=Rs(e,Xn(e)),s=os(t,n);let i=us(e,li(0,"int32"));const r=s.rank-i.rank;for(let t=0;t<r;++t)i=ss(i,t+1);i=Es(i,qs(s.shape,"bool"));const a=Xn(s);return Zn(i,s,a)}(t,n)}}},{kernelName:"ZerosLike",gradFunc:t=>({x:()=>Xn(t)})}];for(const t of Hr)we(t);let Jr;function Zr(){return null==Jr&&(Jr=t().epsilon()),Jr}class Xr extends Error{constructor(t){super(t),Object.setPrototypeOf(this,Xr.prototype)}}class Yr extends Error{constructor(t){super(t),Object.setPrototypeOf(this,Yr.prototype)}}class Qr extends Error{constructor(t){super(t),Object.setPrototypeOf(this,Qr.prototype)}}class ta extends Error{constructor(t){super(t),Object.setPrototypeOf(this,ta.prototype)}}class ea extends Error{constructor(t){super(t),Object.setPrototypeOf(this,ea.prototype)}}function na(t,e){if(Array.isArray(t)){let n=[];for(let s=0;s<e;s++)n=n.concat(t);return n}{const n=new Array(e);return n.fill(t),n}}function sa(t,e){if(!t)throw new ea(e)}function ia(t,e){let n=0;for(const s of t)s===e&&n++;return n}function ra(t){return 1===t.length?t[0]:t}function aa(t){return Array.isArray(t)?t:[t]}function oa(t){const e=t.replace(/(.)([A-Z][a-z0-9]+)/g,"$1_$2").replace(/([a-z])([A-Z])/g,"$1_$2").toLowerCase();return"_"!==e[0]?e:"private"+e}function la(t){return t.length<=1||-1===t.indexOf("_")?t:t.replace(/[_]+(\w|$)/g,(t,e)=>e.toUpperCase())}let ua={};function ha(t){if(null==t)return null;const e={};return e.className=t.getClassName(),e.config=t.getConfig(),e}function ca(t,e={},n={},s="object",i=!1){if("string"==typeof t){const i=t;let r;if(i in n)r=n[i];else if(i in ua)r=ua[i];else if(r=e[i],null==r)throw new Qr(`Unknown ${s}: ${t}. This may be due to one of the following reasons:\n1. The ${s} is defined in Python, in which case it needs to be ported to TensorFlow.js or your JavaScript code.\n2. The custom ${s} is defined in JavaScript, but is not registered properly with tf.serialization.registerClass().`);return r}{const r=t;if(null==r.className||null==r.config)throw new Qr(s+": Improper config format: "+JSON.stringify(r)+".\n'className' and 'config' must set.");const a=r.className;let o,l;if(a in n?[o,l]=n[a]:a in ua?[o,l]=ua.className:a in e&&([o,l]=e[a]),null==o)throw new Qr(`Unknown ${s}: ${a}. This may be due to one of the following reasons:\n1. The ${s} is defined in Python, in which case it needs to be ported to TensorFlow.js or your JavaScript code.\n2. The custom ${s} is defined in JavaScript, but is not registered properly with tf.serialization.registerClass().`);if(null!=l){const t={};for(const e of Object.keys(ua))t[e]=ua[e];for(const e of Object.keys(n))t[e]=n[e];r.config.customObjects=t;const e=Object.assign({},ua);for(const t of Object.keys(n))ua[t]=n[t];!function t(e){if(null!=e&&"object"==typeof e)if(Array.isArray(e))e.forEach(e=>t(e));else{const n=Object.keys(e);for(const s of n){const n=e[s];null!=n&&"object"==typeof n&&(Array.isArray(n)||"ndarray"!==n.type||"number"!=typeof n.value?t(n):e[s]=n.value)}}}(r.config);const s=l(o,r.config,n,i);return ua=Object.assign({},e),s}{const t=Object.assign({},ua);for(const t of Object.keys(n))ua[t]=n[t];const e=new o(r.config);return ua=Object.assign({},t),e}}}function pa(t,e){return-1*function(t,e){return t<e?-1:t>e?1:0}(t,e)}function da(t){if(null==t)return t;const e=[];for(const n of t)-1===e.indexOf(n)&&e.push(n);return e}function fa(t){if(null==t)throw new Qr("Invalid value in obj: "+JSON.stringify(t));for(const e in t)if(t.hasOwnProperty(e))return!1;return!0}function ga(t,e,n){if(null!=n&&t.indexOf(n)<0)throw new Qr(`${n} is not a valid ${e}.  Valid values are ${t} or null/undefined.`)}function ma(t,e,n=0,s=1/0){return sa(n>=0),sa(s>=n),Array.isArray(t)&&t.length>=n&&t.length<=s&&t.every(t=>typeof t===e)}function ya(t,n){Array.isArray(t)?(e.assert(t.length>0,()=>n+" is unexpectedly an empty array."),t.forEach((t,e)=>ya(t,`element ${e+1} of ${n}`))):e.assert(Number.isInteger(t)&&t>0,()=>`Expected ${n} to be a positive integer, but got `+function t(e){return null===e?"null":Array.isArray(e)?"["+e.map(e=>t(e)).join(",")+"]":"string"==typeof e?`"${e}"`:""+e}(t)+".")}function ba(t){return"relu"===t?"relu":"linear"===t?"linear":"elu"===t?"elu":null}function wa(t,e){return s(()=>i(r(a(t,t),e,!0)))}class ka extends n.Serializable{getConfig(){return{}}}class xa extends ka{constructor(t){super(),this.defaultMaxValue=2,this.defaultAxis=0,this.maxValue=null!=t.maxValue?t.maxValue:this.defaultMaxValue,this.axis=null!=t.axis?t.axis:this.defaultAxis}apply(t){return s(()=>{const e=wa(t,this.axis),n=o(e,0,this.maxValue);return a(t,l(n,u(Zr(),e)))})}getConfig(){return{maxValue:this.maxValue,axis:this.axis}}}xa.className="MaxNorm",n.registerClass(xa);class va extends ka{constructor(t){super(),this.defaultAxis=0,this.axis=null!=t.axis?t.axis:this.defaultAxis}apply(t){return s(()=>l(t,u(Zr(),wa(t,this.axis))))}getConfig(){return{axis:this.axis}}}va.className="UnitNorm",n.registerClass(va);class Sa extends ka{apply(t){return h(t)}}Sa.className="NonNeg",n.registerClass(Sa);class Ia extends ka{constructor(t){super(),this.defaultMinValue=0,this.defaultMaxValue=1,this.defaultRate=1,this.defaultAxis=0,this.minValue=null!=t.minValue?t.minValue:this.defaultMinValue,this.maxValue=null!=t.maxValue?t.maxValue:this.defaultMaxValue,this.rate=null!=t.rate?t.rate:this.defaultRate,this.axis=null!=t.axis?t.axis:this.defaultAxis}apply(t){return s(()=>{const e=wa(t,this.axis),n=u(a(this.rate,o(e,this.minValue,this.maxValue)),a(1-this.rate,e));return a(t,l(n,u(Zr(),e)))})}getConfig(){return{minValue:this.minValue,maxValue:this.maxValue,rate:this.rate,axis:this.axis}}}Ia.className="MinMaxNorm",n.registerClass(Ia);const Na={maxNorm:"MaxNorm",minMaxNorm:"MinMaxNorm",nonNeg:"NonNeg",unitNorm:"UnitNorm"};function Aa(t){return ha(t)}function Ca(t,e={}){return ca(t,n.SerializationMap.getMap().classNameMap,e,"constraint")}function za(t){if(null==t)return null;if("string"==typeof t){return Ca({className:t in Na?Na[t]:t,config:{}})}return t instanceof ka?t:Ca(t)}var Da=Object.freeze({__proto__:null,maxNorm:function(t){return new xa(t)},unitNorm:function(t){return new va(t)},nonNeg:function(){return new Sa},minMaxNorm:function(t){return new Ia(t)}});const Ta=["channelsFirst","channelsLast"],Ea=["nearest","bilinear"],Fa=["valid","same","causal"],$a=["max","avg"],_a=["sum","mul","concat","ave"],La=new Map;function Ra(t){ga(Ta,"DataFormat",t)}function Ma(t){ga(Fa,"PaddingMode",t)}function Oa(t){ga($a,"PoolMode",t)}const Ba=[];function Pa(t,e){Ba.push(t);try{const t=e();return Ba.pop(),t}catch(t){throw Ba.pop(),t}}function Wa(t){if(!ja(t))throw new Error("Not a valid tensor name: '"+t+"'");return(0===Ba.length?"":Ba.join("/")+"/")+t}function Ua(t){if(!ja(t))throw new Error("Not a valid tensor name: '"+t+"'");La.has(t)||La.set(t,0);const e=La.get(t);if(La.set(t,La.get(t)+1),e>0){const n=`${t}_${e}`;return La.set(n,1),n}return t}const Ka=new RegExp(/^[A-Za-z0-9][-A-Za-z0-9\._\/]*$/);function ja(t){return!!t.match(Ka)}function Va(t,e,n){null==e&&(e=0),null==n&&(n=t.length);let s=1;for(let i=e;i<n;++i)s*=t[i];return s}function qa(t){return t=Array.isArray(t)?new Float32Array(t):t,p(t)}function Ga(t){return d(qa(t)).dataSync()[0]}function Ha(t){return c(qa(t)).dataSync()[0]}function Ja(t,e){if(e<t)throw new Qr(`end (${e}) < begin (${t}) is forbidden.`);const n=[];for(let s=t;s<e;++s)n.push(s);return n}function Za(t,e){return t.asType(e)}function Xa(t,e=-1){const n=t.shape.slice();return e<0&&(e=n.length+e+1),n.splice(e,0,1),t.reshape(n)}function Ya(t,e,n){return s(()=>{switch(t.rank){case 1:return b(t,e,n);case 2:return y(t,[e,0],[n,t.shape[1]]);case 3:return m(t,[e,0,0],[n,t.shape[1],t.shape[2]]);case 4:return g(t,[e,0,0,0],[n,t.shape[1],t.shape[2],t.shape[3]]);case 5:return f(t,[e,0,0,0,0],[n,t.shape[1],t.shape[2],t.shape[3],t.shape[4]]);case 6:return f(t,[e,0,0,0,0,0],[n,t.shape[1],t.shape[2],t.shape[3],t.shape[4],t.shape[5]]);default:throw new Qr("sliceAlongFirstAxis() received an unsupported tensor rank: "+t.rank)}})}function Qa(t,e,n){return s(()=>{switch(t.rank){case 1:return b(t,e,n);case 2:return y(t,[0,e],[t.shape[0],n]);case 3:return m(t,[0,0,e],[t.shape[0],t.shape[1],n]);case 4:return g(t,[0,0,0,e],[t.shape[0],t.shape[1],t.shape[2],n]);default:throw new Qr("sliceAlongLastAxis() received an unsupported tensor rank: "+t.rank)}})}function to(t,e,n,i){return s(()=>{switch(t.rank){case 1:return b(t,e,n);case 2:switch(i){case 1:return Ya(t,e,n);case 2:return Qa(t,e,n);default:throw new Qr("The axis is not within the rank of the tensor "+i)}case 3:switch(i){case 1:return Ya(t,e,n);case 2:return m(t,[0,e,0],[t.shape[0],n,t.shape[2]]);case 3:return Qa(t,e,n);default:throw new Qr("The axis is not within the rank of the tensor "+i)}case 4:switch(i){case 1:return Ya(t,e,n);case 2:return g(t,[0,e,0,0],[t.shape[0],n,t.shape[2],t.shape[3]]);case 3:return g(t,[0,0,e,0],[t.shape[0],t.shape[1],n,t.shape[3]]);case 4:return Qa(t,e,n);default:throw new Qr("The axis is not within the rank of the tensor "+i)}default:throw new Qr("sliceAlongLastAxis() received an unsupported tensor rank: "+t.rank)}})}function eo(t,e=-1){let n;return e<0&&(n=t[0].rank,e=0!==n?n:0),e===t[0].rank&&(e=-1),N(t,e)}function no(t,e){switch(t.rank){case 1:return T([t,e]);case 2:return D([t,e],0);case 3:return z([t,e],0);case 4:return C([t,e],0);default:throw new Qr("concatAlongFirstAxis() received an unsupported tensor rank: "+t.rank)}}function so(t,e){if(Array.isArray(e)||(e=[e]),t.rank!==e.length)throw new Qr(`The length of input n (${e.length}) does not match the number of dimensions in input x (${t.rank})`);return k(t,e)}function io(t,e=0,n=1,s,i){return x(t,e,n,s,i)}function ro(t,e,n,s){if(t.rank<2||e.rank<2)throw new ta(`dot requires both inputs to be rank >= 2 but got x shape = ${t.shape} and y shape = ${e.shape}`);if(e.rank>=3){if(t.shape.slice(-1)[0]!==e.shape.slice(-2)[0])throw new ta(`If rank y >= 3, then the second last dim of y must equal the last dim of x but got x shape = ${t.shape} and  y shape = `+e.shape)}if(2===t.rank&&2===e.rank){const i=!1,r=!1;return A.matMul({a:t,b:e,transposeA:i,transposeB:r,bias:s?lo(t.rank,s,"channelsLast"):null,activation:n})}{const i=t.shape.slice(),r=i.pop();t=t.reshape([-1,r]);const a=e.shape.slice(),o=a.pop(),l=a.pop(),u=[...a,o],h=Array.from({length:e.rank},(t,n)=>0===n?e.rank-2:n<=e.rank-2?n-1:n);e=e.transpose(h).reshape([l,-1]);const c=[...i,...u],p=!1,d=!1;return A.matMul({a:t,b:e,transposeA:p,transposeB:d,bias:s?lo(t.rank,s,"channelsLast"):null,activation:n}).reshape(c)}}function ao(t,e,n){return s(()=>(e=Array.isArray(e)?p(e,"int32"):e.toInt(),w(t,e,n)))}function oo(t){return a(t,t)}function lo(t,e,n){const s=e.shape;if(1!==e.rank&&e.rank!==t)throw new Qr("Unexpected bias dimensions: "+e.rank+"; expected it to be 1 or "+t);if(5===t){if("channelsFirst"===n)return 1===s.length?e.reshape([1,s[0],1,1,1]):e.reshape([1,s[3],s[0],s[1],s[2]]);if("channelsLast"===n)return 1===s.length?e.reshape([1,1,1,1,s[0]]):e.reshape([1].concat(s))}else if(4===t){if("channelsFirst"===n)return 1===s.length?e.reshape([1,s[0],1,1]):e.reshape([1,s[2],s[0],s[1]]);if("channelsLast"===n)return 1===s.length?e.reshape([1,1,1,s[0]]):e.reshape([1].concat(s))}else if(3===t){if("channelsFirst"===n)return 1===s.length?e.reshape([1,s[0],1]):e.reshape([1,s[1],s[0]]);if("channelsLast"===n)return 1===s.length?e.reshape([1,1,s[0]]):e.reshape([1].concat(s))}else if(t<3)return e;throw new Qr("Unsupported input rank by biasAdd: "+e.rank)}function uo(t,e,n){return s(()=>(null==n&&(n="channelsLast"),Ra(n),t.add(lo(t.rank,e,n))))}function ho(t,e,n,i){return s(()=>I(t,e,n,i))}function co(t,e,n=!1){return n?t():e()}const po=["fanIn","fanOut","fanAvg"],fo=["normal","uniform","truncatedNormal"];class go extends n.Serializable{fromConfigUsesCustomObjects(){return!1}getConfig(){return{}}}class mo extends go{apply(t,e){return E(t,e)}}mo.className="Zeros",n.registerClass(mo);class yo extends go{apply(t,e){return F(t,e)}}yo.className="Ones",n.registerClass(yo);class bo extends go{constructor(t){if(super(),"object"!=typeof t)throw new Qr("Expected argument of type ConstantConfig but got "+t);if(void 0===t.value)throw new Qr("config must have value set but got "+t);this.value=t.value}apply(t,e){return s(()=>a($(this.value),F(t,e)))}getConfig(){return{value:this.value}}}bo.className="Constant",n.registerClass(bo);class wo extends go{constructor(t){super(),this.DEFAULT_MINVAL=-.05,this.DEFAULT_MAXVAL=.05,this.minval=t.minval||this.DEFAULT_MINVAL,this.maxval=t.maxval||this.DEFAULT_MAXVAL,this.seed=t.seed}apply(t,e){return _(t,this.minval,this.maxval,e)}getConfig(){return{minval:this.minval,maxval:this.maxval,seed:this.seed}}}wo.className="RandomUniform",n.registerClass(wo);class ko extends go{constructor(t){super(),this.DEFAULT_MEAN=0,this.DEFAULT_STDDEV=.05,this.mean=t.mean||this.DEFAULT_MEAN,this.stddev=t.stddev||this.DEFAULT_STDDEV,this.seed=t.seed}apply(t,e){if("float32"!==(e=e||"float32")&&"int32"!==e)throw new ta(`randomNormal does not support dType ${e}.`);return io(t,this.mean,this.stddev,e,this.seed)}getConfig(){return{mean:this.mean,stddev:this.stddev,seed:this.seed}}}ko.className="RandomNormal",n.registerClass(ko);class xo extends go{constructor(t){super(),this.DEFAULT_MEAN=0,this.DEFAULT_STDDEV=.05,this.mean=t.mean||this.DEFAULT_MEAN,this.stddev=t.stddev||this.DEFAULT_STDDEV,this.seed=t.seed}apply(t,e){if("float32"!==(e=e||"float32")&&"int32"!==e)throw new ta(`truncatedNormal does not support dType ${e}.`);return L(t,this.mean,this.stddev,e,this.seed)}getConfig(){return{mean:this.mean,stddev:this.stddev,seed:this.seed}}}xo.className="TruncatedNormal",n.registerClass(xo);class vo extends go{constructor(t){super(),this.gain=null!=t.gain?t.gain:1}apply(t,e){return s(()=>{if(2!==t.length||t[0]!==t[1])throw new Qr("Identity matrix initializer can only be used for 2D square matrices.");return a(this.gain,R(t[0]))})}getConfig(){return{gain:this.gain}}}vo.className="Identity",n.registerClass(vo);class So extends go{constructor(t){if(super(),t.scale<0)throw new Qr("scale must be a positive float. Got: "+t.scale);var e;this.scale=null==t.scale?1:t.scale,this.mode=null==t.mode?"fanIn":t.mode,e=this.mode,ga(po,"FanMode",e),this.distribution=null==t.distribution?"normal":t.distribution,function(t){ga(fo,"Distribution",t)}(this.distribution),this.seed=t.seed}apply(t,e){const n=function(t,e="channelsLast"){let n,s;if(Ra(e),2===t.length)n=t[0],s=t[1];else if(-1!==[3,4,5].indexOf(t.length)){if("channelsFirst"===e){const e=Va(t,2);n=t[1]*e,s=t[0]*e}else if("channelsLast"===e){const e=Va(t,0,t.length-2);n=t[t.length-2]*e,s=t[t.length-1]*e}}else{const e=Va(t);n=Math.sqrt(e),s=Math.sqrt(e)}return[n,s]}(t),s=n[0],i=n[1];let r=this.scale;if("fanIn"===this.mode?r/=Math.max(1,s):"fanOut"===this.mode?r/=Math.max(1,i):r/=Math.max(1,(s+i)/2),"normal"===this.distribution){const n=Math.sqrt(r);if("float32"!==(e=e||"float32")&&"int32"!==e)throw new ta(`${this.getClassName()} does not support dType ${e}.`);return L(t,0,n,e,this.seed)}{const n=Math.sqrt(3*r);return _(t,-n,n,e)}}getConfig(){return{scale:this.scale,mode:this.mode,distribution:this.distribution,seed:this.seed}}}So.className="VarianceScaling",n.registerClass(So);class Io extends So{constructor(t){super({scale:1,mode:"fanAvg",distribution:"uniform",seed:null==t?null:t.seed})}getClassName(){return So.className}}Io.className="GlorotUniform",n.registerClass(Io);class No extends So{constructor(t){super({scale:1,mode:"fanAvg",distribution:"normal",seed:null==t?null:t.seed})}getClassName(){return So.className}}No.className="GlorotNormal",n.registerClass(No);class Ao extends So{constructor(t){super({scale:2,mode:"fanIn",distribution:"normal",seed:null==t?null:t.seed})}getClassName(){return So.className}}Ao.className="HeNormal",n.registerClass(Ao);class Co extends So{constructor(t){super({scale:2,mode:"fanIn",distribution:"uniform",seed:null==t?null:t.seed})}getClassName(){return So.className}}Co.className="HeUniform",n.registerClass(Co);class zo extends So{constructor(t){super({scale:1,mode:"fanIn",distribution:"normal",seed:null==t?null:t.seed})}getClassName(){return So.className}}zo.className="LeCunNormal",n.registerClass(zo);class Do extends So{constructor(t){super({scale:1,mode:"fanIn",distribution:"uniform",seed:null==t?null:t.seed})}getClassName(){return So.className}}Do.className="LeCunNormal",n.registerClass(Do);class To extends go{constructor(t){if(super(),this.DEFAULT_GAIN=1,this.gain=null==t.gain?this.DEFAULT_GAIN:t.gain,this.seed=t.seed,null!=this.seed)throw new ta("Random seed is not implemented for Orthogonal Initializer yet.")}apply(t,e){return s(()=>{if(t.length<2)throw new ta("Shape must be at least 2D.");t[0]*t[1]>2e3&&console.warn(`Orthogonal initializer is being called on a matrix with more than 2000 (${t[0]*t[1]}) elements: Slowness may result.`);const e=io(t[0]>t[1]?[t[1],t[0]]:t,0,1,"float32");let n=M.gramSchmidt(e);return t[0]>t[1]&&(n=n.transpose()),a(this.gain,n)})}getConfig(){return{gain:this.gain,seed:this.seed}}}To.className="Orthogonal",n.registerClass(To);const Eo={constant:"Constant",glorotNormal:"GlorotNormal",glorotUniform:"GlorotUniform",heNormal:"HeNormal",heUniform:"HeUniform",identity:"Identity",leCunNormal:"LeCunNormal",leCunUniform:"LeCunUniform",ones:"Ones",orthogonal:"Orthogonal",randomNormal:"RandomNormal",randomUniform:"RandomUniform",truncatedNormal:"TruncatedNormal",varianceScaling:"VarianceScaling",zeros:"Zeros"};function Fo(t,e={}){return ca(t,n.SerializationMap.getMap().classNameMap,e,"initializer")}function $o(t){return ha(t)}function _o(t){if("string"==typeof t){const e=t in Eo?Eo[t]:t;if("GlorotNormal"===e)return new No;if("GlorotUniform"===e)return new Io;if("HeNormal"===e)return new Ao;if("HeUniform"===e)return new Co;if("LeCunNormal"===e)return new zo;if("LeCunUniform"===e)return new Do;{const t={};return t.className=e,t.config={},Fo(t)}}return t instanceof go?t:Fo(t)}var Lo=Object.freeze({__proto__:null,zeros:function(){return new mo},ones:function(){return new yo},constant:function(t){return new bo(t)},randomUniform:function(t){return new wo(t)},randomNormal:function(t){return new ko(t)},truncatedNormal:function(t){return new xo(t)},identity:function(t){return new vo(t)},varianceScaling:function(t){return new So(t)},glorotUniform:function(t){return new Io(t)},glorotNormal:function(t){return new No(t)},heNormal:function(t){return new Ao(t)},heUniform:function(t){return new Co(t)},leCunNormal:function(t){return new zo(t)},leCunUniform:function(t){return new Do(t)},orthogonal:function(t){return new To(t)}});let Ro=0;function Mo(){return Ro++}const Oo={};function Bo(t=""){return t in Oo||(Oo[t]=0),Oo[t]+=1,t+Oo[t].toString()}function Po(t){return Array.isArray(t)&&Array.isArray(t[0])}function Wo(t){return 0===t.length?[]:Array.isArray(t[0])?t:[t]}function Uo(t){let e;if(Array.isArray(t)){if(1!==t.length)throw new Qr("Expected Tensor length to be 1; got "+t.length);e=t[0]}else e=t;return e}function Ko(t){if(Array.isArray(t)&&Array.isArray(t[0])){if(1===t.length)return(t=t)[0];throw new Qr("Expected exactly 1 Shape; got "+t.length)}return t}function jo(t){let e=0;for(const n of t)0===n.shape.length?e+=1:e+=n.shape.reduce((t,e)=>t*e);return e}class Vo{constructor(t,e="float32",n="Variable",s=!0,i=null){this.dtype=null==e?"float32":e,this.shape=t.shape,this.id=Mo(),n=null==n?"Variable":n,this.originalName=Wa(n),this.name=Ua(this.originalName),this.trainable_=s,this.constraint=i,this.val=O(t,this.trainable_,this.name,this.dtype)}read(){return this.assertNotDisposed(),this.val}write(t){return this.assertNotDisposed(),function(t,e){if(t.shape.toString()!==e.shape.toString())throw new Error("Shape mismatch: "+JSON.stringify(t.shape)+" vs. "+JSON.stringify(e.shape))}(this.val,t),this.val.id!==t.id&&(this.val.assign(t),null!=this.constraint&&this.val.assign(this.constraint.apply(this.val))),this}dispose(){this.assertNotDisposed(),this.val.dispose()}assertNotDisposed(){if(this.val.isDisposed)throw new Error(`LayersVariable ${this.name} is already disposed.`)}get trainable(){return this.trainable_}set trainable(t){this.trainable_=t,this.val.trainable=t}}function qo(t){return t.map(t=>t.read())}function Go(t){t.forEach(t=>{t[0].write(t[1])})}class Ho{constructor(t){this.dtype=t.dtype,this.shape=t.shape,null!=t.shape?this.ndim=t.shape.length:this.ndim=t.ndim,this.maxNDim=t.maxNDim,this.minNDim=t.minNDim,this.axes=t.axes||{}}}class Jo{constructor(t,e,n,s,i,r,a){this.dtype=t,this.shape=e,this.sourceLayer=n,this.inputs=s,this.callArgs=i,this.outputTensorIndex=a,this.id=Mo(),null!=r&&(this.originalName=Wa(r),this.name=Ua(this.originalName)),this.rank=e.length}}let Zo=0;class Xo{constructor(t,e){this.callArgs=e,this.id=Zo++,this.outboundLayer=t.outboundLayer,this.inboundLayers=t.inboundLayers,this.nodeIndices=t.nodeIndices,this.tensorIndices=t.tensorIndices,this.inputTensors=t.inputTensors,this.outputTensors=t.outputTensors,this.inputMasks=t.inputMasks,this.outputMasks=t.outputMasks,this.inputShapes=t.inputShapes,this.outputShapes=t.outputShapes;for(const e of t.inboundLayers)null!=e&&e.outboundNodes.push(this);t.outboundLayer.inboundNodes.push(this)}getConfig(){const t=[];for(const e of this.inboundLayers)null!=e?t.push(e.name):t.push(null);return{outboundLayer:this.outboundLayer?this.outboundLayer.name:null,inboundLayers:t,nodeIndices:this.nodeIndices,tensorIndices:this.tensorIndices}}}let Yo=0;class Qo extends n.Serializable{constructor(t={}){super(),this._callHook=null,this._addedWeightNames=[],this._stateful=!1,this.id=Yo++,this.activityRegularizer=null,this.inputSpec=null,this.supportsMasking=!1,this._trainableWeights=[],this._nonTrainableWeights=[],this._losses=[],this._updates=[],this._built=!1,this.inboundNodes=[],this.outboundNodes=[];let e=t.name;if(!e){const t=this.getClassName();e=oa(t)+"_"+Bo(t)}if(this.name=e,this.trainable_=null==t.trainable||t.trainable,null!=t.inputShape||null!=t.batchInputShape){let e;if(null!=t.batchInputShape)e=t.batchInputShape;else if(null!=t.inputShape){let n=null;null!=t.batchSize&&(n=t.batchSize),e=[n].concat(t.inputShape)}this.batchInputShape=e;let n=t.dtype;null==n&&(n=t.inputDType),null==n&&(n="float32"),this.dtype=n}null!=t.weights?this.initialWeights=t.weights:this.initialWeights=null,this._refCount=null,this.fastWeightInitDuringBuild=!1}static nodeKey(t,e){return t.name+"_ib-"+e.toString()}getNodeAtIndex(t,e){if(0===this.inboundNodes.length)throw new Yr(`The layer has never been called and thus has no defined ${e}.`);if(this.inboundNodes.length<=t)throw new Qr(`Asked to get ${e} at node ${t}, but the layer has only ${this.inboundNodes.length} inbound nodes.`);return this.inboundNodes[t]}getInputAt(t){return ra(this.getNodeAtIndex(t,"input").inputTensors)}getOutputAt(t){return ra(this.getNodeAtIndex(t,"output").outputTensors)}get input(){if(this.inboundNodes.length>1)throw new Xr("Layer "+this.name+' has multiple inbound nodes, hence the notion of "layer input" is ill-defined. Use `getInputAt(nodeIndex)` instead.');if(0===this.inboundNodes.length)throw new Xr("Layer "+this.name+" is not connected, no input to return.");return ra(this.getNodeAtIndex(0,"input").inputTensors)}get output(){if(0===this.inboundNodes.length)throw new Xr("Layer "+this.name+" has no inbound nodes.");if(this.inboundNodes.length>1)throw new Xr("Layer "+this.name+' has multiple inbound nodes, hence the notion of "layer output" is ill-defined. Use `getOutputAt(nodeIndex)` instead.');return ra(this.getNodeAtIndex(0,"output").outputTensors)}get losses(){return this._losses}calculateLosses(){return this.losses.map(t=>t())}get updates(){return this._updates}get built(){return this._built}set built(t){this._built=t}get trainable(){return this.trainable_}set trainable(t){this._trainableWeights.forEach(e=>e.trainable=t),this.trainable_=t}get trainableWeights(){return this.trainable_?this._trainableWeights.filter(t=>t.trainable):[]}set trainableWeights(t){this._trainableWeights=t}get nonTrainableWeights(){return this.trainable?this._trainableWeights.filter(t=>!t.trainable).concat(this._nonTrainableWeights):this._trainableWeights.concat(this._nonTrainableWeights)}set nonTrainableWeights(t){this._nonTrainableWeights=t}get weights(){return this.trainableWeights.concat(this.nonTrainableWeights)}get stateful(){return this._stateful}resetStates(){if(!this.stateful)throw new Error("Cannot call the resetStates() method of a non-stateful Layer object.")}assertInputCompatibility(t){if(t=aa(t),null==this.inputSpec||0===this.inputSpec.length)return;const e=aa(this.inputSpec);if(t.length!==e.length)throw new Qr(`Layer ${this.name} expects ${e.length} inputs, but it received ${t.length} input tensors. Input received: `+t);for(let n=0;n<t.length;n++){const s=t[n],i=e[n];if(null==i)continue;const r=s.rank;if(null!=i.ndim&&r!==i.ndim)throw new Qr(`Input ${n} is incompatible with layer ${this.name}: expected ndim=${i.ndim}, found ndim=${r}`);if(null!=i.maxNDim&&r>i.maxNDim)throw new Qr(`Input ${n} is incompatible with layer ${this.name}: expected max_ndim=${i.maxNDim}, found ndim=${r}`);if(null!=i.minNDim&&r<i.minNDim)throw new Qr(`Input ${n} is incompatible with layer ${this.name}: expected min_ndim=${i.minNDim}, found ndim=${r}.`);if(null!=i.dtype&&s.dtype!==i.dtype)throw new Qr(`Input ${n} is incompatible with layer ${this.name} : expected dtype=${i.dtype}, found dtype=${s.dtype}.`);if(i.axes){const t=s.shape;for(const e in i.axes){const s=Number(e),r=i.axes[e],a=s>=0?t[s]:t[t.length+s];if(null!=r&&-1===[r,null].indexOf(a))throw new Qr(`Input ${n} is incompatible with layer ${this.name}: expected axis ${s} of input shape to have value ${r} but got shape ${t}.`)}}if(null!=i.shape)for(let t=0;t<i.shape.length;++t){const e=i.shape[t],r=s.shape[t];if(null!=e&&null!=r&&e!==r)throw new Qr(`Input ${n} is incompatible with layer ${this.name}: expected shape=${i.shape}, found shape=${s.shape}.`)}}}call(t,e){return t}invokeCallHook(t,e){null!=this._callHook&&this._callHook(t,e)}setCallHook(t){this._callHook=t}clearCallHook(){this._callHook=null}apply(t,e){e=e||{},this.assertNotDisposed();const n=aa(t);let s=!0;for(const t of n)if(!(t instanceof Jo)){s=!1;break}let i=!0;for(const t of n)if(t instanceof Jo){i=!1;break}if(s===i)throw new Qr("Arguments to apply() must be all SymbolicTensors or all Tensors");return Pa(this.name,()=>{if(!this.built){this.assertInputCompatibility(t);const e=[];for(const n of aa(t))e.push(n.shape);this.build(ra(e)),this.built=!0,this.initialWeights&&this.setWeights(this.initialWeights),null===this._refCount&&i&&(this._refCount=1)}if(this.assertInputCompatibility(t),i){let s=this.call(t,e);const i=aa(s),r=[];for(let t of i)-1!==n.indexOf(t)&&(t=t.clone()),r.push(t);if(s=ra(r),null!=this.activityRegularizer)throw new ta("Layer invocation in the presence of activity regularizer(s) is not supported yet.");return s}{const n=function(t){t=aa(t);const e=[];for(const n of t)e.push(n.shape);return ra(e)}(t),s=this.computeOutputShape(n);let i;const r="float32";if(this.warnOnIncompatibleInputShape(Array.isArray(t)?n[0]:n),i=null!=s&&s.length>0&&Array.isArray(s[0])?s.map((n,s)=>new Jo(r,n,this,aa(t),e,this.name,s)):new Jo(r,s,this,aa(t),e,this.name),this.addInboundNode(t,i,null,null,n,s,e),this._refCount++,null!=this.activityRegularizer)throw new ta("Layer invocation in the presence of activity regularizer(s) is not supported yet.");return i}})}warnOnIncompatibleInputShape(t){if(null!=this.batchInputShape)if(t.length!==this.batchInputShape.length)console.warn("The rank of the input tensor provided (shape: "+JSON.stringify(t)+") does not match that of the "+`batchInputShape (${JSON.stringify(this.batchInputShape)}) of the layer `+this.name);else{let e=!1;this.batchInputShape.forEach((n,s)=>{null!=n&&null!=t[s]&&t[s]!==n&&(e=!0)}),e&&console.warn(`The shape of the input tensor (${JSON.stringify(t)}) does not match the expectation of layer ${this.name}: `+JSON.stringify(this.batchInputShape))}}get outputShape(){if(null==this.inboundNodes||0===this.inboundNodes.length)throw new Xr(`The layer ${this.name} has never been called and thus has no defined output shape.`);const t=[];for(const e of this.inboundNodes){const n=JSON.stringify(e.outputShapes);-1===t.indexOf(n)&&t.push(n)}if(1===t.length){const t=this.inboundNodes[0].outputShapes;return Array.isArray(t)&&Array.isArray(t[0])&&1===t.length?t[0]:t}throw new Xr(`The layer ${this.name} has multiple inbound nodes with different output shapes. Hence the notion of "output shape" is ill-defined for the layer.`)}countParams(){if(!this.built)throw new Yr(`You tried to call countParams() on ${this.name}, but the layer is not built yet. Build it first by calling build(batchInputShape).`);return jo(this.weights)}build(t){this.built=!0}getWeights(t=!1){return qo(t?this.trainableWeights:this.weights)}setWeights(t){s(()=>{const n=this.weights;if(n.length!==t.length)throw new Qr(`You called setWeights(weights) on layer "${this.name}" with a weight list of length ${t.length}, but the layer was expecting ${n.length} weights. Provided weights: ${t}...`);if(0===n.length)return;const s=[],i=qo(n);for(let r=0;r<i.length;++r){const a=i[r],o=n[r],l=t[r];if(!e.arraysEqual(a.shape,l.shape))throw new Qr(`Layer weight shape ${a.shape} not compatible with provided weight shape `+l.shape);s.push([o,l])}Go(s)})}addWeight(t,e,n,s,i,r,a){if(-1!==this._addedWeightNames.indexOf(t))throw new Qr(`Duplicate weight name ${t} for layer ${this.name}`);this._addedWeightNames.push(t),null==n&&(n="float32"),this.fastWeightInitDuringBuild&&(s=_o("zeros"));const o=s.apply(e,n),l=new Vo(o,n,t,r,a);return o.dispose(),null!=i&&this.addLoss(()=>i.apply(l.read())),null==r&&(r=!0),r?this._trainableWeights.push(l):this._nonTrainableWeights.push(l),l}setFastWeightInitDuringBuild(t){this.fastWeightInitDuringBuild=t}addLoss(t){null==t||Array.isArray(t)&&0===t.length||(t=aa(t),void 0!==this._losses&&null!==this._losses&&this.losses.push(...t))}computeOutputShape(t){return t}computeMask(t,e){if(!this.supportsMasking){if(null!=e){if(!Array.isArray(e))throw new TypeError(`Layer ${this.name} does not support masking, but was passed an inputMask.`);e.forEach(t=>{if(null!=t)throw new TypeError(`Layer ${this.name} does not support masking, but was passed an inputMask.`)})}return null}return e}addInboundNode(t,e,n,s,i,r,a=null){const o=aa(t);e=aa(e),n=aa(n),s=aa(s),i=Wo(i),r=Wo(r);const l=[],u=[],h=[];for(const t of o)l.push(t.sourceLayer),u.push(t.nodeIndex),h.push(t.tensorIndex);new Xo({outboundLayer:this,inboundLayers:l,nodeIndices:u,tensorIndices:h,inputTensors:o,outputTensors:e,inputMasks:n,outputMasks:s,inputShapes:i,outputShapes:r},a);for(let t=0;t<e.length;t++)e[t].sourceLayer=this,e[t].nodeIndex=this.inboundNodes.length-1,e[t].tensorIndex=t}getConfig(){const t={name:this.name,trainable:this.trainable};return null!=this.batchInputShape&&(t.batchInputShape=this.batchInputShape),null!=this.dtype&&(t.dtype=this.dtype),t}disposeWeights(){return this.weights.forEach(t=>t.dispose()),this.weights.length}assertNotDisposed(){if(0===this._refCount)throw new Error(`Layer '${this.name}' is already disposed.`)}dispose(){if(!this.built)throw new Error(`Cannot dispose Layer ${this.name} because it has not been built yet.`);if(null===this._refCount)throw new Error(`Cannot dispose Layer ${this.name} because it has not been used yet.`);this.assertNotDisposed();let t=0;return 0==--this._refCount&&(t=this.disposeWeights()),{refCountAfterDispose:this._refCount,numDisposedVariables:t}}}class tl extends Qo{constructor(t){if(super({dtype:t.dtype,name:null!=t.name?t.name:Bo("input").toString()}),null==t.batchSize&&(t.batchSize=null),null==t.sparse&&(t.sparse=!1),this.trainable=!1,this.built=!0,this.sparse=t.sparse,null!=t.inputShape&&null!=t.batchInputShape)throw new Qr("Only provide the inputShape OR batchInputShape argument to inputLayer, not both at the same time.");let e=t.batchInputShape;if(null==e){if(null==t.inputShape)throw new Qr("An InputLayer should be passed either a `batchInputShape` or an `inputShape`.");e=[t.batchSize].concat(t.inputShape)}else if(null!=t.batchSize)throw new Qr("Cannot specify batchSize if batchInputShape is specified when creating an InputLayer.");const n=t.dtype||"float32";this.batchInputShape=e,this.dtype=n,this.inputSpec=[{shape:e}];const s=new Jo(this.dtype,this.batchInputShape,this,[],{},this.name);s.nodeIndex=0,s.tensorIndex=0,new Xo({outboundLayer:this,inboundLayers:[],nodeIndices:[],tensorIndices:[],inputTensors:[s],outputTensors:[s],inputMasks:[null],outputMasks:[null],inputShapes:[e],outputShapes:[e]})}apply(t,e){throw new Qr("Cannot pass any input to an InputLayer's apply() method. InputLayer name: "+this.name)}dispose(){return{refCountAfterDispose:this._refCount,numDisposedVariables:0}}getConfig(){return{batchInputShape:this.batchInputShape,dtype:this.dtype,sparse:this.sparse,name:this.name}}}function el(t){if(null==t.batchShape&&null==t.shape)throw new Error("Please provide to Input either a `shape` or a `batchShape` argument. Note that `shape` does not include the batch dimension.");if(null!=t.batchShape&&null!=t.shape)throw new Qr("Please provide either a `shape` or `batchShape` argument to Input, but not both.");let e=t.batchShape;null!=t.shape&&null==e&&(e=[null].concat(t.shape));let n=t.dtype;null==n&&(n="float32");return new tl({batchInputShape:e,name:t.name,dtype:n,sparse:t.sparse}).inboundNodes[0].outputTensors[0]}async function nl(t){if(null==t)return;const e=[],n=[],s=[];for(const i in t){const r=t[i];if("number"!=typeof r){const t=r;e.push(t.data()),n.push(i),s.push(t)}}if(e.length>0){const i=await Promise.all(e);for(let e=0;e<i.length;++e)t[n[e]]=i[e][0];B(s)}}function sl(t){if(null!=t)for(const e in t){const n=t[e];"number"!=typeof n&&n.dispose()}}var il;tl.className="InputLayer",n.registerClass(tl),function(t){t[t.SILENT=0]="SILENT",t[t.VERBOSE=1]="VERBOSE"}(il||(il={}));class rl{constructor(){this.validationData=null}setParams(t){this.params=t}async onEpochBegin(t,e){}async onEpochEnd(t,e){}async onBatchBegin(t,e){}async onBatchEnd(t,e){}async onTrainBegin(t){}async onTrainEnd(t){}setModel(t){}}class al{constructor(t,e=10){null==t&&(t=[]),this.callbacks=t,this.queueLength=e}append(t){this.callbacks.push(t)}setParams(t){for(const e of this.callbacks)e.setParams(t)}setModel(t){for(const e of this.callbacks)e.setModel(t)}async onEpochBegin(t,e){null==e&&(e={});for(const n of this.callbacks)await n.onEpochBegin(t,e)}async onEpochEnd(t,e){null==e&&(e={});for(const n of this.callbacks)await n.onEpochEnd(t,e)}async onBatchBegin(t,e){null==e&&(e={});for(const n of this.callbacks)await n.onBatchBegin(t,e)}async onBatchEnd(t,e){null==e&&(e={});for(const n of this.callbacks)await n.onBatchEnd(t,e)}async onTrainBegin(t){null==t&&(t={});for(const e of this.callbacks)await e.onTrainBegin(t)}async onTrainEnd(t){null==t&&(t={});for(const e of this.callbacks)await e.onTrainEnd(t)}}class ol extends rl{constructor(){super()}async onEpochBegin(t){this.seen=0,this.totals={}}async onBatchEnd(t,e){null==e&&(e={});const n=null==e.size?0:e.size;this.seen+=n;for(const t in e){const i=e[t];if("number"==typeof i)this.totals.hasOwnProperty(t)||(this.totals[t]=0),this.totals[t]=this.totals[t]+i*n;else{let e;t in this.totals?e=this.totals[t]:this.totals[t]=0;const r=s(()=>u(this.totals[t],a(i,n)));this.totals[t]=r,null!=e&&e.dispose()}}}async onEpochEnd(t,e){if(null!=e)for(const t of this.params.metrics)null!=this.totals[t]&&("number"==typeof this.totals[t]?e[t]=this.totals[t]/this.seen:s(()=>{const n=a(l(1,this.seen),this.totals[t]);e[t]=n,this.totals[t].dispose(),W(e[t])}))}}class ll extends rl{async onTrainBegin(t){this.epoch=[],this.history={}}async onEpochEnd(t,e){null==e&&(e={}),this.epoch.push(t);for(const t in e)null==this.history[t]&&(this.history[t]=[]),this.history[t].push(e[t])}async syncData(){const t=[],e=[],n=[];for(const s in this.history){const i=this.history[s];for(let r=0;r<i.length;++r)if("number"!=typeof i[r]){const a=i[r];t.push(a.data()),e.push(s),n.push(r)}}const s=await Promise.all(t);for(let t=0;t<s.length;++t){this.history[e[t]][n[t]].dispose(),this.history[e[t]][n[t]]=s[t][0]}}}class ul extends rl{constructor(t,n){if(super(),this.currentEpoch=0,this.yieldEvery=n||"auto","auto"===this.yieldEvery&&(this.yieldEvery=125),"never"===this.yieldEvery&&null!=t.onYield)throw new Error("yieldEvery is `never` but you provided an `onYield` callback. Either change `yieldEvery` or remove the callback");e.isNumber(this.yieldEvery)&&(this.maybeWait=function(t,n){let s,i=e.now();return(...r)=>{const a=e.now();return a-i<n||(i=a,s=t(...r)),s}}(this.maybeWait.bind(this),this.yieldEvery)),this.trainBegin=t.onTrainBegin,this.trainEnd=t.onTrainEnd,this.epochBegin=t.onEpochBegin,this.epochEnd=t.onEpochEnd,this.batchBegin=t.onBatchBegin,this.batchEnd=t.onBatchEnd,this.yield=t.onYield}async maybeWait(t,e,n){const s=[];null!=this.yield&&(await nl(n),s.push(this.yield(t,e,n))),s.push(P()),await Promise.all(s)}async onEpochBegin(t,e){this.currentEpoch=t,null!=this.epochBegin&&(await nl(e),await this.epochBegin(t,e))}async onEpochEnd(t,e){const n=[];null!=this.epochEnd&&(await nl(e),n.push(this.epochEnd(t,e))),"epoch"===this.yieldEvery&&n.push(P()),await Promise.all(n)}async onBatchBegin(t,e){null!=this.batchBegin&&(await nl(e),await this.batchBegin(t,e))}async onBatchEnd(t,n){const s=[];null!=this.batchEnd&&(await nl(n),s.push(this.batchEnd(t,n))),"batch"===this.yieldEvery?s.push(P()):e.isNumber(this.yieldEvery)&&s.push(this.maybeWait(this.currentEpoch,t,n)),await Promise.all(s)}async onTrainBegin(t){null!=this.trainBegin&&(await nl(t),await this.trainBegin(t))}async onTrainEnd(t){null!=this.trainEnd&&(await nl(t),await this.trainEnd(t))}}function hl(t,e){if(null==t&&(t={}),t instanceof rl)return[t];if(Array.isArray(t)&&t[0]instanceof rl)return t;return aa(t).map(t=>new ul(t,e))}class cl{constructor(){}static registerCallbackConstructor(t,n){e.assert(t>=0&&Number.isInteger(t),()=>"Verbosity level is expected to be an integer >= 0, but got "+t),cl.checkForDuplicate(n),null==cl.constructors[t]&&(cl.constructors[t]=[]),cl.constructors[t].push(n)}static checkForDuplicate(t){for(const e in cl.constructors){cl.constructors[+e].forEach(e=>{if(e===t)throw new Qr("Duplicate callback constructor.")})}}static clear(){cl.constructors={}}static createCallbacks(t){const e=[];for(const n in cl.constructors){const s=+n;t>=s&&e.push(...cl.constructors[s])}return e.map(t=>new t)}}function pl(t,e,n,s,i,r,a,o,l){const u=new ll,h=[new ol,...cl.createCallbacks(e)];null!=t&&h.push(...t),h.push(u);const c=new al(h);return c.setParams({epochs:n,initialEpoch:s,samples:i,steps:r,batchSize:a,verbose:e,doValidation:o,metrics:l}),{callbackList:c,history:u}}function dl(t,e={},s=!1){return ca(t,n.SerializationMap.getMap().classNameMap,e,"layer",s)}function fl(t,e){return s(()=>{"float32"!==t.dtype&&(t=t.asType("float32"));const n=r(oo(t),e,!0),s=G(n.shape,Zr()),a=i(H(n,s));return l(t,a)})}function gl(t,e){return s(()=>j(oo(K(e,t)),-1))}function ml(t,e){return s(()=>j(S(K(e,t)),-1))}function yl(t,e){return s(()=>{const n=K(t,e),s=o(S(t),Zr(),Number.MAX_VALUE),i=S(l(n,s));return a(100,j(i,-1))})}function bl(t,e,n=!1){return s(()=>{if(n)e=V(e);else{const t=r(e,e.shape.length-1,!0);e=l(e,t)}return e=o(e,Zr(),1-Zr()),q(r(a(t.toFloat(),U(e)),e.shape.length-1))})}function wl(t,e,n=!1){return s(()=>{const s=J(function(t){const e=[Va(t.shape)];return t.reshape(e)}(t)).toInt(),i=(e=o(e,Zr(),1-Zr())).shape;return bl(Z(s,i[i.length-1]).reshape(i),e,n)})}function kl(t,n){return s(()=>{let i;return i=o(n,Zr(),1-Zr()),i=U(l(i,K(1,i))),j(function(t,n){if(!e.arraysEqual(t.shape,n.shape))throw new Qr(`logits and labels must have the same shape, but got shapes ${JSON.stringify(t.shape)} and ${JSON.stringify(n.shape)}`);return s(()=>{const e=n.relu(),s=n.abs().neg();return e.sub(n.mul(t)).add(s.exp().log1p())})}(t,i),-1)})}function xl(t,e){return s(()=>{const n=fl(t,-1),s=fl(e,-1),i=a(n,s);return q(r(i,-1))})}cl.constructors={};const vl={meanSquaredError:gl,meanAbsoluteError:ml,meanAbsolutePercentageError:yl,meanSquaredLogarithmicError:function(t,e){return s(()=>{const n=o(e,Zr(),Number.MAX_VALUE),s=U(u(1,n)),i=o(t,Zr(),Number.MAX_VALUE),r=U(u(1,i));return j(oo(K(s,r)),-1)})},squaredHinge:function(t,e){return s(()=>{const n=H(0,K(1,a(t,e)));return j(oo(n),-1)})},hinge:function(t,e){return s(()=>{const n=H(0,K(1,a(t,e)));return j(n,-1)})},categoricalHinge:function(t,e){return s(()=>{const n=r(a(t,e),-1),s=c(a(K(1,t),e),-1);return H(0,u(1,K(s,n)))})},logcosh:function(t,e){return s(()=>{const n=Math.log(2),s=K(e,t),i=K(u(s,X(a(-2,s))),n);return j(i,-1)})},categoricalCrossentropy:bl,sparseCategoricalCrossentropy:wl,binaryCrossentropy:kl,kullbackLeiblerDivergence:function(t,e){return s(()=>{const n=o(t,Zr(),1),s=o(e,Zr(),1);return r(a(t,U(l(n,s))),-1)})},poisson:function(t,e){return s(()=>{const n=U(u(Zr(),e));return j(K(e,a(t,n)),-1)})},cosineProximity:xl};function Sl(t){if("string"==typeof t){if(t in vl)return vl[t];let e="Unknown loss "+t;throw t.toLowerCase().includes("softmaxcrossentropy")&&(e=`Unknown loss ${t}. Use "categoricalCrossentropy" as the string name for tf.losses.softmaxCrossEntropy`),new Qr(e)}return t}function Il(t,e){return s(()=>{const n=a(.5,Y(e)),s=Za(Q(e,n),t.dtype);return j(tt(t,s),-1)})}function Nl(t,e){return s(()=>Za(tt(et(t,-1),et(e,-1)),"float32"))}function Al(t,e){return s(()=>nt(t.equal(1),e.equal(1)).sum().cast("float32"))}function Cl(t,e){return s(()=>{const n=Al(t,e),i=function(t,e){return s(()=>nt(t.equal(0),e.equal(1)).sum().cast("float32"))}(t,e),r=n.add(i);return st(Q(r,0),n.div(r),0).cast("float32")})}function zl(t,e){return s(()=>{const n=Al(t,e),i=function(t,e){return s(()=>nt(t.equal(1),e.equal(0)).sum().cast("float32"))}(t,e),r=n.add(i);return st(Q(r,0),n.div(r),0).cast("float32")})}function Dl(t,e){return kl(t,e)}function Tl(t,e){return t.rank===e.rank&&(t=t.squeeze([t.rank-1])),(e=e.argMax(-1)).dtype!==t.dtype&&(e=e.asType(t.dtype)),tt(t,e).asType("float32")}const El=bl,Fl=wl,$l={binaryAccuracy:Il,categoricalAccuracy:Nl,precision:Cl,categoricalCrossentropy:El,sparseCategoricalCrossentropy:Fl,mse:gl,MSE:gl,mae:ml,MAE:ml,mape:yl,MAPE:yl,cosine:xl};function _l(t){if("string"==typeof t&&t in $l)return $l[t];if("string"!=typeof t&&null!=t)return t;throw new Qr("Unknown metric "+t)}function Ll(t){if(sa(null!==t,"Unknown LossOrMetricFn "+t),"string"==typeof t)return t;{let e;for(const n of Object.keys(vl))if(vl[n]===t){e=n;break}if(void 0!==e)return e;for(const n of Object.keys($l))if($l[n]===t){e=n;break}return void 0!==e?e:t.name}}function Rl(t,e,n=!1){if(null==t||"object"!=typeof t||Object.getPrototypeOf(t)!==Object.prototype||!function t(e){if(null===e)return!0;if("object"==typeof e){if(Object.getPrototypeOf(e)===Object.prototype){const n=Object.keys(e);for(const s of n){if("string"!=typeof s)return!1;if(!t(e[s]))return!1}return!0}if(Array.isArray(e)){for(const n of e)if(!t(n))return!1;return!0}return!1}{const t=typeof e;return"string"===t||"number"===t||"boolean"===t}}(t))throw new Error("User-defined metadata is expected to be a JSON object, but is not.");if(n){const n=JSON.stringify(t);n.length>1048576&&console.warn(`User-defined metadata of model "${e}" is too large in size (length=${n.length} when serialized). It is not recommended to store such large objects in user-defined metadata. Please make sure its serialized length is <= 1048576.`)}}function Ml(t,e,n,s=console.log){const i=function(t){let e=!0;const n=[],s=[];for(const e in t.nodesByDepth)n.push(t.nodesByDepth[e]);for(const t of n){if(t.length>1||1===t.length&&t[0].inboundLayers.length>1){e=!1;break}s.push(...t)}if(e)for(const n of t.layers){let t=!1;for(const i of n.inboundNodes)if(-1!==s.indexOf(i)){if(t){e=!1;break}t=!0}if(!e)break}return e}(t),r=["Layer (type)","Output shape","Param #"];let a;if(i?(e=e||65,n=n||[.45,.85,1]):(e=e||98,n=n||[.33,.55,.67,1]),n[n.length-1]<=1&&(n=n.map(t=>Math.floor(e*t))),!i){r.push("Receives inputs"),a=[];for(const e in t.nodesByDepth)a.push(...t.nodesByDepth[e])}s("_".repeat(e)),Ol(r,n,s),s("=".repeat(e));const o=t.layers;for(let t=0;t<o.length;++t)i?Bl(o[t],n,s):Pl(o[t],n,a,s),s((t===o.length-1?"=":"_").repeat(e));t.checkTrainableWeightsConsistency();const l=function(t){let e;e=null!=t.collectedTrainableWeights?jo(t.collectedTrainableWeights):jo(t.trainableWeights);return e}(t),u=jo(t.nonTrainableWeights);s("Total params: "+(l+u)),s("Trainable params: "+l),s("Non-trainable params: "+u),s("_".repeat(e))}function Ol(t,e,n=console.log){let s="";for(let n=0;n<t.length;++n)n>0&&(s=s.slice(0,s.length-1)+" "),s+=t[n],s=s.slice(0,e[n]),s+=" ".repeat(e[n]-s.length);n(s)}function Bl(t,e,n){let s;try{s=JSON.stringify(t.outputShape)}catch(t){s="multiple"}Ol([`${t.name} (${t.getClassName()})`,s,t.countParams().toString()],e,n)}function Pl(t,e,n,s){let i;try{i=JSON.stringify(t.outputShape)}catch(t){i="multiple"}const r=[];for(const e of t.inboundNodes)if(!(null!=n&&n.length>0&&-1===n.indexOf(e)))for(let t=0;t<e.inboundLayers.length;++t){const n=e.inboundLayers[t].name,s=e.nodeIndices[t],i=e.tensorIndices[t];r.push(`${n}[${s}][${i}]`)}const a=t.name,o=t.getClassName(),l=0===r.length?"":r[0];Ol([`${a} (${o})`,i,t.countParams().toString(),l],e,s);for(let t=1;t<r.length;++t)Ol(["","","",r[t]],e,s)}function Wl(t,e,n){return("inboundNodes"===t||"outputLayers"===t||"inputLayers"===t)&&0===e&&"string"==typeof n}function Ul(t,e){if(null===t)return null;if("string"==typeof t)return la(t);if("number"==typeof t||"boolean"==typeof t)return t;if(t instanceof Array){const n=[],s=t.length;for(let i=0;i<s;++i){const s=t[i];Wl(e,i,s)?n.push(s):n.push(Ul(s,e))}return n}{const e={};for(const n of Object.keys(t)){const s=t[n];if("name"===n&&"string"==typeof s)e[n]=s;else{const t=la(n);e[t]=Ul(s,t)}}return e}}const Kl="3.0.0";class jl{constructor(t){if(this.id2Value={},this.id2Mask={},this.name2Id={},t instanceof jl)for(const e in t.id2Value)this.id2Value[e]=t.id2Value[e],e in t.id2Mask&&(this.id2Mask[e]=t.id2Mask[e]);else{if(null==t)return;for(const e of t)this.add(e.key,e.value)}}add(t,e,n){if(null!=this.id2Value[t.id])throw new Qr(`Duplicate key: name=${t.name}, id=${t.id}`);return this.id2Value[t.id]=function(t,e){if(null==t.dtype||t.dtype===e.dtype)return e;try{return at(e,t.dtype)}catch(n){throw new Qr(`The dtype of the feed (${e.dtype}) can not be cast to the dtype of the key '${t.name}' (${t.dtype}).`)}}(t,e),this.name2Id[t.name]=t.id,null!=n&&(this.id2Mask[t.id]=n),this}addFeed(t){this.add(t.key,t.value)}hasKey(t){return null!=this.id2Value[t.id]}names(){return Object.keys(this.name2Id)}getValue(t){if(t instanceof Jo){if(null==this.id2Value[t.id])throw new Qr("Nonexistent key: "+t.name);return this.id2Value[t.id]}{const e=this.name2Id[t];if(null==e)throw new Qr("Feed dict has no SymbolicTensor name: "+t);return this.id2Value[e]}}getMask(t){if(t instanceof Jo){if(null==this.id2Value[t.id])throw new Qr("Nonexistent key: "+t.name);return this.id2Mask[t.id]}{const e=this.name2Id[t];if(null==e)throw new Qr("Feed dict has no SymbolicTensor name: "+t);return this.id2Mask[e]}}disposeMasks(){null!=this.id2Mask&&B(this.id2Mask)}}const Vl={},ql={};function Gl(t,n,s,i){const r=null!=s&&s.training,a=Array.isArray(t),o=a?t:[t],l=o.map(t=>t.name),u=[],h=n.names();for(const t of l)-1!==h.indexOf(t)?u.push(n.getValue(t)):u.push(null);null!=i&&(i.maxNumTensors=-1/0,i.minNumTensors=1/0);const c=l.join(",")+"|"+n.names().join(",");let p,d;if(null==Vl[c]){const t=function(t,n){e.assert(null!=t&&t.length>0,()=>"Expected at least one fetch, got none");let s=[],i={};if(1===t.length){const e=Jl(t[0],n);s=e.sorted,i=e.recipientMap}else{const e=new Set;for(const r of t){const{sorted:t,recipientMap:a}=Jl(r,n);for(const n of t)e.has(n.name)||(s.push(n),e.add(n.name));for(const t in a)null==i[t]&&(i[t]=new Set),a[t].forEach(e=>i[t].add(e))}}return{sorted:s,recipientCounts:Hl(i)}}(o,n);p=t.sorted,d=t.recipientCounts,Vl[c]=p,ql[c]=d}p=Vl[c],d={},r||Object.assign(d,ql[c]);const f=new jl(n);for(let t=0;t<p.length;++t){if(null!=i){const t=rt().numTensors;t>i.maxNumTensors&&(i.maxNumTensors=t),t<i.minNumTensors&&(i.minNumTensors=t)}const e=p[t],a=e.sourceLayer;if(a instanceof tl)continue;const o=[],h=[],c=[];let g=!1;for(const t of e.inputs){const e=f.getValue(t),s=f.getMask(t);o.push(e),h.push(s),null!=s&&(g=!0),r||(d[t.name]--,0!==d[t.name]||n.hasKey(t)||-1!==l.indexOf(t.name)||e.isDisposed||!0===t.sourceLayer.stateful||c.push(e))}g&&((s=s||{}).mask=h[0]);const m=aa(a.apply(o,s));let y=null;a.supportsMasking&&(y=a.computeMask(o,h));const b=Zl(e),w=Array.isArray(b)?b:[b];for(let t=0;t<w.length;++t){f.hasKey(w[t])||f.add(w[t],m[t],Array.isArray(y)?y[0]:y);const e=l.indexOf(w[t].name);-1!==e&&(u[e]=m[t])}r||B(c)}return f.disposeMasks(),a?u:u[0]}function Hl(t){const e={};for(const n in t)e[n]=t[n].size;return e}function Jl(t,e){const n=new Set,s=[],i={};for(const t of e.names())n.add(t);const r=[],a=[];for(r.push(t);r.length>0;){const t=r[r.length-1];if(n.has(t.name)){r.pop();continue}const e=a[a.length-1]===r.length-1;if(0===t.inputs.length||e)r.pop(),s.push(t),n.add(t.name),e&&a.pop();else{a.push(r.length-1);for(const e of t.inputs)null==i[e.name]&&(i[e.name]=new Set),i[e.name].add(t.name),n.has(e.name)||r.push(e)}}return{sorted:s,recipientMap:i}}function Zl(t){let e;if(1===t.sourceLayer.inboundNodes.length)e=t.sourceLayer.output;else{let n=null;for(let e=0;e<t.sourceLayer.inboundNodes.length;++e)for(const s of t.sourceLayer.inboundNodes[e].outputTensors)if(s.id===t.id){n=e;break}e=t.sourceLayer.getOutputAt(n)}return e}class Xl extends Qo{constructor(t){if(super({}),this.containerNodes=new Set,this.name=t.name,null==this.name){const t=this.getClassName().toLowerCase();this.name=Bo(t)}if(this.supportsMasking=!1,this.trainable_=!0,Array.isArray(t.inputs)?this.inputs=t.inputs.slice():this.inputs=[t.inputs],Array.isArray(t.outputs)?this.outputs=t.outputs.slice():this.outputs=[t.outputs],da(this.inputs).length!==this.inputs.length)throw new Qr("The list of inputs passed to the model is redundant. All inputs should only appear once. Found: "+this.inputs.map(t=>t.name));da(this.outputs).length!==this.outputs.length&&console.warn("The list of outputs passed to the model is redundant. All outputs should only appear once. Found: "+this.outputs.map(t=>t.name)),this.inputLayers=[],this.inputLayersNodeIndices=[],this.inputLayersTensorIndices=[],this.outputLayers=[],this.outputLayersNodeIndices=[],this.outputLayersTensorIndices=[],this.layers=[],this.internalContainerRefs=[];for(const t of this.outputs){const e=t.sourceLayer,n=t.nodeIndex,s=t.tensorIndex;this.outputLayers.push(e),this.outputLayersNodeIndices.push(n),this.outputLayersTensorIndices.push(s)}for(const t of this.inputs){const e=t.sourceLayer,n=t.nodeIndex,s=t.tensorIndex;sa(0===n,"input layer has >1 nodes"),sa(0===s,"input layer has >1 tensors"),this.inputLayers.push(e),this.inputLayersNodeIndices.push(n),this.inputLayersTensorIndices.push(s)}this.inputNames=[],this.outputNames=[],this.feedInputShapes=[],this.feedInputNames=[],this.feedOutputNames=[];for(let e=0;e<this.inputLayers.length;e++){const n=this.inputLayers[e];if(!(n instanceof tl))throw new TypeError(`Input layers to a LayersModel must be InputLayer objects. Received inputs: ${t.inputs}. Input ${e} (0-based) originates from layer type ${n.getClassName()}.`);this.inputNames.push(n.name),this.feedInputShapes.push(n.batchInputShape),this.feedInputNames.push(n.name)}for(const t of this.outputLayers)this.outputNames.push(t.name);this.internalInputShapes=this.inputs.map(t=>t.shape),this.internalOutputShapes=this.outputs.map(t=>t.shape);const e={},n={},s={},i={},r={},a=[],o=(t,e,n,s,i,l)=>{null!=s&&null!=i&&null!=l||(s=t.sourceLayer,i=t.nodeIndex,l=t.tensorIndex);const u=s.inboundNodes[i];if(-1!==n.indexOf(u))throw new Yr(`The tensor ${t.name} at layer "${s.name}" is part of a cycle.`);if(-1!==e.indexOf(u))return;this.containerNodes.add(Xl.nodeKey(s,i)),s.id in r||(r[s.id]=Object.keys(r).length),-1===n.indexOf(u)&&n.push(u);const h=u.inboundLayers.length;for(let t=0;t<h;t++){const s=u.inputTensors[t],i=u.inboundLayers[t],r=u.nodeIndices[t],a=u.tensorIndices[t];o(s,e,n,i,r,a)}for(e.push(u);n.indexOf(u)>=0;)n.splice(n.indexOf(u),1);a.push(u)},l=[],u=[];for(const t of this.outputs)o(t,l,u);const h=a.slice().reverse();for(const t of h){n[t.id]=t,t.id in e||(e[t.id]=0);let r=e[t.id];const a=null==s[t.outboundLayer.id]?0:s[t.outboundLayer.id];r=Math.max(r,a),s[t.outboundLayer.id]=r,i[t.outboundLayer.id]=t.outboundLayer,e[t.id]=r;for(let s=0;s<t.inboundLayers.length;s++){const i=t.inboundLayers[s],a=t.nodeIndices[s],o=i.inboundNodes[a],l=null==e[o.id]?0:e[o.id];e[o.id]=Math.max(r+1,l),n[o.id]=o}}const c={};for(const t in e){const s=e[t];s in c||(c[s]=[]),c[s].push(n[t])}const p={};for(const t in s){const e=s[t];e in p||(p[e]=[]),p[e].push(i[t])}let d=Object.keys(p).map(t=>parseInt(t,10)).sort(pa);this.layers=[];for(const t of d){const e=p[t];e.sort((t,e)=>{const n=r[t.id],s=r[e.id];return n<s?-1:n>s?1:0});for(const t of e)t instanceof Xl&&this.internalContainerRefs.push(t),this.layers.push(t)}this.layersByDepth=p,d=Object.keys(c).map(t=>parseInt(t,10)).sort(pa);const f=this.inputs.slice(),g=[];for(const t of d)for(const e of c[t]){const t=e.outboundLayer;if(null!=t){for(const n of e.inputTensors)if(-1===f.indexOf(n))throw new Yr("Graph disconnected: cannot obtain value for tensor "+n+` at layer "${t.name}". The following previous layers were accessed without issue: `+g);for(const t of e.outputTensors)f.push(t);g.push(t.name)}}this.nodesByDepth=c;const m=this.layers.map(t=>t.name);for(const t of m){const e=m.filter(e=>e===t).length;if(1!==e)throw new Yr(`The name "${t}" is used ${e} times in the model. All layer names should be unique. Layer names: `+JSON.stringify(m))}this.outboundNodes=[],this.inboundNodes=[],new Xo({outboundLayer:this,inboundLayers:[],nodeIndices:[],tensorIndices:[],inputTensors:this.inputs,outputTensors:this.outputs,inputMasks:this.inputs.map(t=>null),outputMasks:this.outputs.map(t=>null),inputShapes:this.inputs.map(t=>t.shape),outputShapes:this.outputs.map(t=>t.shape)}),this.built=!0,this._refCount=1}assertNotDisposed(){if(0===this._refCount)throw new Error(`Container '${this.name}' is already disposed.`)}dispose(){this.assertNotDisposed();const t={refCountAfterDispose:null,numDisposedVariables:0};if(0==--this._refCount){for(const e of this.layers)t.numDisposedVariables+=e.dispose().numDisposedVariables;for(const e of this.internalContainerRefs)t.numDisposedVariables+=e.dispose().numDisposedVariables}return t.refCountAfterDispose=this._refCount,t}get trainable(){return this.trainable_}set trainable(t){this.layers.forEach(e=>{e._trainableWeights.forEach(e=>e.trainable=t)}),this.trainable_=t}get trainableWeights(){if(this._trainableWeights.length>0)throw new Qr("Container instance unexpectedly contains _trainableWeights.The trainable weights of a Container are a union of the trainable weights of its consituent Layers. Its own _trainableWeights must remain an empty Array.");if(!this.trainable)return[];let t=[];for(const e of this.layers)t=t.concat(e.trainableWeights);return t}get nonTrainableWeights(){const t=[];for(const e of this.layers)t.push(...e.nonTrainableWeights);if(!this.trainable){const e=[];for(const t of this.layers)e.push(...t.trainableWeights);return e.concat(t)}return t}get weights(){return this.trainableWeights.concat(this.nonTrainableWeights)}loadWeights(t,e=!0){const n={};let s=0;for(const t of this.layers)for(const e of t.weights){if(null!=n[e.originalName])throw new Qr("Duplicate weight name: "+e.originalName);n[e.originalName]=e,s++}const i=[];for(const s in t){let r=s;if(null==n[s]){const t=s.split("/");r=t.slice(0,-2).concat([t[t.length-1]]).join("/")}if(null!=n[r])i.push([n[r],t[s]]);else if(e)throw new Qr("Provided weight data has no target variable: "+s);delete n[r]}if(e){const t=[];for(const e in n)t.push(e);if(t.length>0)throw new Qr(`${t.length} of ${s} weights are not set: `+t)}Go(i)}updatedConfig(){const t=this.getConfig(),e={};return e.className=this.getClassName(),e.config=t,e.kerasVersion="tfjs-layers 3.0.0",e.backend="TensorFlow.js",e}toJSON(t,e=!0){const n=function t(e,n){if(null==e)return null;if("string"==typeof e)return oa(e);if("number"==typeof e||"boolean"==typeof e)return e;if(e instanceof Array){const s=[],i=e.length;for(let r=0;r<i;++r){const i=e[r];Wl(n,r,i)?s.push(i):s.push(t(i,n))}return s}{const n={};for(const s of Object.keys(e)){const i=e[s],r=oa(s);n[r]="name"!==s&&"className"!==s||"string"!=typeof i?t(i,s):i}return n}}(this.updatedConfig());return e?JSON.stringify(n):n}call(t,e){return s(()=>{t=aa(t);const n=new jl;for(let e=0;e<this.inputs.length;++e)n.add(this.inputs[e],t[e]);return Gl(this.outputs,n,e)})}computeMask(t,e){return s(()=>{let n;return t=aa(t),n=null==e?na(null,t.length):aa(e),this.runInternalGraph(t,n)[1]})}computeOutputShape(t){const e=Wo(t);if(e.length!==this.inputLayers.length)throw new Qr(`Invalid inputShape argument ${t}: model has ${this.inputLayers.length} tensor inputs.`);const n={};for(let t=0;t<e.length;t++){const s=this.inputLayers[t],i=e[t];n[s.name+"_0_0"]=i}const s=Object.keys(this.nodesByDepth).map(t=>parseInt(t,10)).sort(pa);if(s.length>1)for(const t of s){const e=this.nodesByDepth[t];for(const t of e){const e=t.outboundLayer;if(-1!==this.inputLayers.map(t=>t.id).indexOf(e.id))continue;const s=[];for(let e=0;e<t.inboundLayers.length;e++){const i=t.inboundLayers[e],r=t.nodeIndices[e],a=t.tensorIndices[e],o=n[`${i.name}_${r}_${a}`];s.push(o)}const i=Wo(e.computeOutputShape(ra(s))),r=e.inboundNodes.indexOf(t);for(let t=0;t<i.length;t++){n[`${e.name}_${r}_${t}`]=i[t]}}}const i=[],r=[];for(let t=0;t<this.outputLayers.length;t++){const e=this.outputLayers[t],n=this.outputLayersNodeIndices[t],s=this.outputLayersTensorIndices[t],i=`${e.name}_${n}_${s}`;r.push(i)}for(let t=0;t<r.length;t++){const e=r[t];sa(e in n),i.push(n[e])}return ra(i)}runInternalGraph(t,e){null==e&&(e=na(null,t.length));const n={};for(let s=0;s<this.inputs.length;++s){const i=this.inputs[s],r=t[s],a=e[s];n[i.id]=[r,a]}const s=Object.keys(this.nodesByDepth).map(t=>parseInt(t,10)).sort(pa);for(const t of s){const e=this.nodesByDepth[t];for(const t of e){const e=t.outboundLayer,s=t.inputTensors,i=t.outputTensors,r=new Array;for(const t of s)t.id in n&&r.push(n[t.id]);if(r.length===s.length){let s,a,o,l,u={};if(null!=t.callArgs&&(u=t.callArgs),1===r.length){const[t,n]=r[0];null==u.mask&&(u.mask=n),o=aa(e.call(t,u)),l=aa(e.computeMask(t,n)),s=[t],a=[n]}else s=r.map(t=>t[0]),a=r.map(t=>t[1]),null==u.mask&&(u.mask=a),o=aa(e.call(s,u)),l=aa(e.computeMask(s,a));if(e.activityRegularizer)throw new ta("LayersModel invocation with concrete Tensor value(s) in the presence of activity regularizer(s) is not supported yet.");for(let t=0;t<i.length;++t){const e=i[t],s=o[t],r=l[t];n[e.id]=[s,r]}}}}const i=[],r=[],a=[];for(const t of this.outputs){sa(t.id in n,`Could not compute output ${t.name} : ${t.id}`);const[e,s]=n[t.id];a.push(e.shape),i.push(e),r.push(s)}return[i,r,a]}buildNodeConversionMap(t){const e={};let n;for(const t of this.layers){n=t instanceof Xl?1:0;for(let s=0;s<t.inboundNodes.length;s++){const i=Xl.nodeKey(t,s);this.containerNodes.has(i)&&(e[i]=n,n+=1)}}return e}getLayer(t,e){if(null!=e){if(this.layers.length<=e)throw new Qr(`Was asked to retrieve layer at index ${e}, but model only has ${this.layers.length} layer(s).`);return this.layers[e]}if(null==t)throw new Qr("Provide either a layer name or layer index");for(const e of this.layers)if(e.name===t)return e;throw new Qr("No such layer: "+t)}calculateLosses(){return s(()=>{const t=[];for(const e of this.layers)for(let n=0;n<e.inboundNodes.length;++n){const s=Xl.nodeKey(e,n);this.containerNodes.has(s)&&t.push(...e.calculateLosses())}return t})}getConfig(){const t={name:this.name},e=this.buildNodeConversionMap(this.layers),n=[];for(const t of this.layers){const s=t.getClassName(),i=t.getConfig(),r=[];for(let n=0;n<t.inboundNodes.length;n++){const s=t.inboundNodes[n],i=Xl.nodeKey(t,n);let a={};if(this.containerNodes.has(i)){if(s.callArgs)try{JSON.stringify(s.callArgs),a=s.callArgs}catch(e){console.warn(`Layer ${t.name} was passed non-serializable keyword arguments: `+s.callArgs+". They will not be included in the serialized model (and thus will be missing at deserialization time)."),a={}}if(s.inboundLayers.length>0){const t=[];for(let n=0;n<s.inboundLayers.length;n++){const i=s.inboundLayers[n],r=s.nodeIndices[n],o=s.tensorIndices[n];let l=e[Xl.nodeKey(i,r)];null==l&&(l=0),t.push([i.name,l,o,a])}r.push(t)}}}const a={};a.name=t.name,a.className=s,a.config=i,a.inboundNodes=r,n.push(a)}t.layers=n;const s=[];for(let t=0;t<this.inputLayers.length;t++){const n=this.inputLayers[t],i=this.inputLayersNodeIndices[t],r=Xl.nodeKey(n,i);if(!this.containerNodes.has(r))continue;let a=e[r];null==a&&(a=0);const o=this.inputLayersTensorIndices[t];s.push([n.name,a,o])}t.inputLayers=s;const i=[];for(let t=0;t<this.outputLayers.length;t++){const n=this.outputLayers[t],s=this.outputLayersNodeIndices[t],r=Xl.nodeKey(n,s);if(!this.containerNodes.has(r))continue;let a=e[r];null==a&&(a=0);const o=this.outputLayersTensorIndices[t];i.push([n.name,a,o])}return t.outputLayers=i,t}static fromConfig(t,e,n={},s=!1){const i={},r={};function a(t,e){t.name in r?r[t.name].push(e):r[t.name]=[e]}function o(t,e){const n=[];let s;for(const r of e){const o=r[0],l=r[1],u=r[2];if(s=null==r[3]?{}:r[3],!(o in i))return void a(t,e);const h=i[o];if(h.inboundNodes.length<=l)return void a(t,e);const c=h.inboundNodes[l];n.push(c.outputTensors[u])}n.length>0&&t.apply(ra(n),s)}function l(t){const n=t.name,r=dl(t,null!=e.customObjects?e.customObjects:{});r.setFastWeightInitDuringBuild(s),i[n]=r;t.inboundNodes.forEach(t=>{if(!(t instanceof Array))throw new Qr("Corrupted configuration, expected array for nodeData: "+t);a(r,t)})}const u=e.name,h=e.layers;for(const t of h)l(t);for(;!fa(r);)for(const t of h){const e=i[t.name];if(e.name in r){const t=r[e.name];delete r[e.name];for(const n of t)o(e,n)}}const c=[],p=[],d=e.inputLayers;for(const t of d){const e=t[0],n=t[1],s=t[2];sa(e in i);const r=i[e].inboundNodes[n].outputTensors;c.push(r[s])}const f=e.outputLayers;for(const t of f){const e=t[0],n=t[1],s=t[2];sa(e in i);const r=i[e].inboundNodes[n].outputTensors;p.push(r[s])}return new t({inputs:c,outputs:p,name:u})}get stateful(){if(this._stateful)throw new Qr("Container instance unexpectedly has _stateful = true. The statefulness of a Container is determined by the Layers it contains. Its _stateful property must remain the default false.");for(const t of this.layers)if(t.stateful)return!0;return!1}resetStates(){s(()=>{this.layers.forEach(t=>{t.stateful&&t.resetStates()})})}}function Yl(t,e){return function(t,e,n){const s=e.length;if(null==t||Array.isArray(t)&&0===t.length)return e.map(t=>null);if(1===s)return Array.isArray(t)&&1===t.length?t:"object"==typeof t&&e[0]in t?[t[e[0]]]:[t];if(Array.isArray(t)){if(t.length!==s)throw new Error(`Provided ${n} is an array of ${t.length} element(s), but the model has ${s} outputs. Make sure a set of weights is provided for each model output.`);return t}if("object"==typeof t&&Object.keys(t).length>0&&"object"==typeof t[Object.keys(t)[0]]){const n=[];return e.forEach(e=>{e in t?n.push(t[e]):n.push(null)}),n}throw new Error(`The model has multiple (${s}) outputs, so ${n} must be either an array with ${s} elements or an object with ${e} keys. Provided ${n} not understood: ${JSON.stringify(t)}`)}(t,e,"classWeight")}async function Ql(t,e,n,i){if(null!=e||null!=i)throw new Error("Support sampleWeight is not implemented yet");if(null!=n){const e=s(()=>{if(1===t.shape.length)return t.clone();if(2===t.shape.length){if(t.shape[1]>1){const e=1;return t.argMax(e)}if(1===t.shape[1])return t.reshape([t.shape[0]]);throw new Error(`Encountered unexpected last-dimension size (${t.shape[1]}) during handling of class weights. The size is expected to be >= 1.`)}throw new Error(`Unexpected rank of target (y) tensor (${t.rank}) during handling of class weights. The rank is expected to be 1 or 2.`)}),i=Array.from(await e.data());B(e);const r=[];return i.forEach(t=>{if(null==n[t])throw new Error(`classWeight must contain all classes in the training data. The class ${t} exists in the data but not in classWeight`);r.push(n[t])}),p(r,"float32")}return null}function tu(t,e){return a(t,e)}function eu(t,n){let s,i;const r=n;s=r.xs,i=r.ys,e.assert(null!=s&&null!=i,()=>"A Dataset iterator for fitDataset() is expected to generate objects of the form `{xs: xVal, ys: yVal}`, where the two values may be `tf.Tensor`, an array of Tensors, or a map of string to Tensor.  The provided Dataset instead generates "+n);const a=nu("input",t.inputNames,s),o=nu("output",t.outputNames,i),l=a[0].shape[0];e.assert(a.length===t.inputs.length,()=>`LayersModel has ${t.inputs.length} inputs, but the dataset provides ${a.length} inputs.  (Expected input keys: `+JSON.stringify(t.inputNames)+")"),e.assert(o.length===t.outputs.length,()=>`LayersModel has ${t.outputs.length} outputs, but the dataset provides ${o.length} outputs.  (Expected output keys: `+JSON.stringify(t.outputNames)+")");for(let n=0;n<a.length;n++)e.assert(a[n].shape[0]===l,()=>`Batch size mismatch: input ${t.inputNames[n]} has ${a[n].shape[0]}; expected  ${l} based on input ${t.inputNames[0]}.`);for(let n=0;n<o.length;n++)e.assert(o[n].shape[0]===l,()=>`Batch size mismatch: output ${t.outputNames[n]} has ${o[n].shape[0]}; expected  ${l} based on input ${t.inputNames[0]}.`);return{xs:a,ys:o}}function nu(t,n,s){if(s instanceof ot)return[s];if(Array.isArray(s))return e.assert(s.length===n.length,()=>`Received an array of ${s.length} Tensors, but expected ${n.length} to match the ${t} keys ${n}.`),s;{const e=[];for(const i of n){if(null==s[i])throw new Qr(`The feature data generated by the dataset lacks the required ${t} key '${i}'.`);e.push(s[i])}return e}}async function su(t,n,s){const i=null!=s.batchesPerEpoch;if(e.assert(null!=t.optimizer,()=>"You must compile a model before training/testing. Use LayersModel.compile(modelCompileConfig)."),e.assert(null!=s,()=>"For fitDataset(), the 2nd argument (config) is required, but it is not provided in this call."),e.assert(null!=s.epochs&&s.epochs>0&&Number.isInteger(s.epochs),()=>"For fitDataset(), config.epochs is expected to be a positive integer, but got "+s.epochs),e.assert(!i||s.batchesPerEpoch>0&&Number.isInteger(s.batchesPerEpoch),()=>"For fitDataset(), config.batchesPerEpoch is expected to be a positive integer if specified, but got "+s.batchesPerEpoch),e.assert(null==s.validationSplit,()=>"`validationSplit` is not supported by `fitDataset()`. Use validationData instead."),t.isTraining)throw new Error("Cannot start training because another fit() call is ongoing.");t.isTraining=!0;try{const r=null!=s.validationData;let a,o;if(r)if(iu(s.validationData))e.assert(null==s.validationBatches||s.validationBatches>0&&Number.isInteger(s.validationBatches),()=>"For fitDataset() with dataset-based validation, config.validationBatches is expected not to be provided, or to be a positive integer, but got "+s.validationBatches);else{const t=function(t){if(3===t.length)throw new ta("Validation with sample weights is not implemented yet.");return{xs:t[0],ys:t[1]}}(s.validationData);a=t.xs,o=t.ys}const l=t.makeTrainFunction(),u=t.getDedupedMetricsNames();let h;h=r?u.slice().concat(u.map(t=>"val_"+t)):u.slice();const c=hl(s.callbacks,s.yieldEvery),p=null==s.verbose?1:s.verbose,{callbackList:d,history:f}=pl(c,p,s.epochs,null,null,function(t,e){let n=null;null!=e.batchesPerEpoch?n=e.batchesPerEpoch:Number.isFinite(t.size)&&(n=t.size);return n}(n,s),null,r,h);d.setModel(t),t.history=f,await d.onTrainBegin(),t.stopTraining_=!1;let g=null==s.initialEpoch?0:s.initialEpoch,m=await n.iterator();for(;g<s.epochs;){const e={};await d.onEpochBegin(g);let h=0,c=0;for(i||(m=await n.iterator());!i||h<s.batchesPerEpoch;){const n=await m.next();if(i&&n.done){console.warn("You provided `batchesPerEpoch` as "+s.batchesPerEpoch+", but your dataset iterator ran out of data after "+h+" batches; interrupting training. Make sure that your dataset can generate at least `batchesPerEpoch * epochs` batches (in this case, "+s.batchesPerEpoch*s.epochs+" batches). You may need to use the repeat() function when building your dataset.");break}if(null!=n.value){const{xs:e,ys:i}=eu(t,n.value),r={};r.batch=c,r.size=e[0].shape[0],await d.onBatchBegin(c,r);const a=[];if(null!=s.classWeight){const e=Yl(s.classWeight,t.outputNames);for(let t=0;t<e.length;++t)a.push(await Ql(i[t],null,e[t]))}const o=e.concat(i).concat(a),p=l(o);B(o);for(let t=0;t<u.length;++t){const e=u[t],n=p[t];r[e]=n,W(n)}await d.onBatchEnd(c,r),sl(r),c++,h++}if(i?h>=s.batchesPerEpoch:n.done){if(r){let n;n=iu(s.validationData)?aa(await t.evaluateDataset(s.validationData,{batches:s.validationBatches})):aa(t.evaluate(a,o,{batchSize:null==s.validationBatchSize?32:s.validationBatchSize,verbose:0}));for(let s=0;s<t.metricsNames.length;++s)e["val_"+t.metricsNames[s]]=n[s]}break}if(t.stopTraining_)break}if(await d.onEpochEnd(g,e),g++,t.stopTraining_)break}return await d.onTrainEnd(),await t.history.syncData(),t.history}finally{t.isTraining=!1}}function iu(t){return"function"==typeof t.iterator}function ru(t){e.assert(t>0&&Number.isInteger(t),()=>"batchSize is required to be a positive integer, but got "+t)}function au(t,e,n){return null==t?[null]:Array.isArray(t)?t.map(t=>Ya(t,e,n-e)):Ya(t,e,n-e)}function ou(t,e){return s(()=>null==t?null:Array.isArray(t)?t.map(t=>ou(t,e)):ao(t,"int32"===e.dtype?e:e.toInt()))}function lu(t,e){const n=[];let s=0,i=null;for(;s<t;)i=s+e,i>=t&&(i=t),n.push([s,i]),s=i;return n}async function uu(t,n,i,r={}){if(t.isTraining)throw new Error("Cannot start training because another fit() call is ongoing.");let a,o,l,u,h,c,d;t.isTraining=!0;try{const f=null==r.batchSize?32:r.batchSize;ru(f);const g=!1,m=await t.standardizeUserData(n,i,r.sampleWeight,r.classWeight,g,f);a=m[0],o=m[1],d=m[2];let y,b=!1;if(null!=r.validationData&&r.validationData.length>0){if(b=!0,2!==r.validationData.length)throw 3===r.validationData.length?new ta("validationData including sample weights is not supported yet."):new Qr("When passing validation data, it must contain 2 (valX, valY) or 3 (valX, valY, valSampleWeight) items; "+r.validationData+" is invalid.");l=r.validationData[0],u=r.validationData[1];const e=!0,n=await t.standardizeUserData(l,u,null,null,e,f);h=n[0],c=n[1],y=h.concat(c)}else if(null!=r.validationSplit&&r.validationSplit>0&&r.validationSplit<1){b=!0;const t=Math.floor(a[0].shape[0]*(1-r.validationSplit)),e=a[0].shape[0];h=au(a,t,e),a=au(a,0,t),c=au(o,t,e),o=au(o,0,t),y=h.concat(c)}else null!=r.validationSteps&&(b=!0);const w=a.concat(o).concat(d);t.checkTrainableWeightsConsistency();const k=t.makeTrainFunction(),x=t.getDedupedMetricsNames();let v,S;b?(t.makeTestFunction(),v=t.testFunction,S=x.slice().concat(x.map(t=>"val_"+t))):(v=null,y=[],S=x.slice());const I=hl(r.callbacks,r.yieldEvery);return await async function(t,n,i,r,a,o,l,u,h,c,d,f,g,m,y){null==a&&(a=32),null==o&&(o=1),null==d&&(d=!0),null==g&&(g=0);let b=!1;if(null!=h&&null!=c&&(b=!0),null!=y&&(b=!0,null==m))throw new Qr("Can only use `validationSteps` when doing step-wise training, i.e., `stepsPerEpoch` must be set.");const w=t.checkNumSamples(i,a,m,"steps_per_epoch");let k;null!=w&&(k=Ja(0,w)),null==l&&(l=1);const{callbackList:x,history:v}=pl(u,l,o,g,w,m,a,b,f);x.setModel(t),t.history=v,await x.onTrainBegin(),t.stopTraining_=!1;for(let l=g;l<o;++l){await x.onEpochBegin(l);const o={};if(null!=m)throw new ta("stepsPerEpoch mode is not implemented yet.");{if("batch"===d)throw new ta("batch shuffling is not implemneted yet");d&&e.shuffle(k);const l=p(k),u=lu(w,a);for(let e=0;e<u.length;++e){const p={};if(await x.onBatchBegin(e,p),s(()=>{const s=u[e][0],d=u[e][1],f=Ya(l,s,d-s);p.batch=e,p.size=d-s;const g=ou(i,f),m=n(g);for(let t=0;t<r.length;++t){const e=r[t],n=m[t];p[e]=n,W(n)}if(e===u.length-1&&b){const e=t.testLoop(h,c,a);for(let t=0;t<r.length;++t){const n=r[t],s=e[t];W(s),o["val_"+n]=s}}}),await x.onBatchEnd(e,p),sl(p),t.stopTraining_)break}l.dispose()}if(await x.onEpochEnd(l,o),t.stopTraining_)break}return await x.onTrainEnd(),await t.history.syncData(),t.history}(t,k,w,x,f,r.epochs,r.verbose,I,v,y,r.shuffle,S,r.initialEpoch,null,null)}finally{t.isTraining=!1,cu(a,n),cu(o,i),cu(h,l),cu(c,u),null!=d&&B(d)}}function hu(t){const e=[];t instanceof ot&&(t=[t]);for(let n=0;n<t.length;++n){const s=t[n];if(1===s.rank)e.push(Xa(s,1));else{if(0===s.rank)throw new Error("Expected tensor to be at least 1D, but received a 0D tensor (scalar).");e.push(s)}}return e}function cu(t,e){if(null==t)return;const n=[];if(e instanceof ot)n.push(e.id);else if(Array.isArray(e))e.forEach(t=>n.push(t.id));else if(null!=e)for(const t in e){const s=e[t];n.push(s.id)}const s=[];if(t instanceof ot)-1===n.indexOf(t.id)&&s.push(t);else if(Array.isArray(t))t.forEach(t=>{-1===n.indexOf(t.id)&&s.push(t)});else if(null!=t)for(const e in t){const i=t[e];-1===n.indexOf(i.id)&&s.push(i)}s.forEach(t=>{t.isDisposed||t.dispose()})}function pu(t){return Array.isArray(t)}function du(t){return!function(t){return t instanceof ot}(t)&&!pu(t)}function fu(t,e,n,s=!0,i=""){if(null==e||0===e.length){if(null!=t){let e=!1;if(pu(t)&&t.length>0)e=!0;else if(du(t)){for(const n in t)if(t.hasOwnProperty(n)){e=!0;break}}else e=!0;if(e)throw new Qr(`Error when checking model ${i} expected no data, but got `+t)}return[]}if(null==t)return e.map(t=>null);let r;if(du(t)){t=t,r=[];for(const n of e){if(null==t[n])throw new Qr(`No data provided for "${n}". Need data for each key in: `+e);r.push(t[n])}}else if(pu(t)){if((t=t).length!==e.length)throw new Qr(`Error when checking model ${i}: the Array of Tensors that you are passing to your model is not the size the model expected. Expected to see ${e.length} Tensor(s), but instead got the following list of Tensor(s): `+t);r=t}else{if(t=t,e.length>1)throw new Qr(`The model ${i} expects ${e.length} Tensor(s), but only received one Tensor. Found: Tensor with shape `+t.shape);r=[t]}if(r=hu(r),null!=n)for(let t=0;t<e.length;++t){if(null==n[t])continue;const a=r[t];if(a.shape.length!==n[t].length)throw new Qr(`Error when checking ${i}: expected ${e[t]} to have ${n[t].length} dimension(s). but got array with shape `+a.shape);for(let r=0;r<n[t].length;++r){if(0===r&&!s)continue;const o=a.shape[r],l=n[t][r];if(null!=l&&l>=0&&o!==l)throw new Qr(`Error when checking ${i}: expected ${e[t]} to have shape [${n[t]}], but got array with shape [${a.shape}].`)}}return r}function gu(t,e,n,s=!0,i=""){let r;if(Array.isArray(t)){if(t.length!==e.length)throw new Qr(`Error when checking model ${i}: the Array of Tensors that you are passing to your model is not the size the the model expected. Expected to see ${e.length} Tensor(s), but instead got ${t.length} Tensors(s).`);r=t}else{if(e.length>1)throw new Qr(`The model expects ${e.length} ${i} Tensors, but only received one Tensor. Found: array with shape `+JSON.stringify(t.shape)+".");r=[t]}if(null!=n)for(let t=0;t<e.length;++t){if(null==n[t])continue;const a=r[t];if(a.shape.length!==n[t].length)throw new Qr(`Error when checking ${i}: expected ${e[t]} to have ${n[t].length} dimension(s), but got array with shape `+JSON.stringify(a.shape));for(let r=0;r<n[t].length;++r){if(0===r&&!s)continue;const o=a.shape[r],l=n[t][r];if(null!=l&&l!==o)throw new Qr(`Error when checking ${i}: expected ${e[t]} to have shape ${JSON.stringify(n[t])} but got array with shape ${JSON.stringify(a.shape)}.`)}}}class mu extends Xl{constructor(t){super(t),this.isTraining=!1}summary(t,e,n=console.log){if(!this.built)throw new Qr("This model has never been called, thus its weights have not been created yet. So no summary can be displayed. Build the model first (e.g., by calling it on some test data).");Ml(this,t,e,n)}compile(t){if(null==t.loss&&(t.loss=[]),this.loss=t.loss,"string"==typeof t.optimizer)this.optimizer_=function(t){const e={Adagrad:()=>it.adagrad(.01),Adadelta:()=>it.adadelta(1,.95,Zr()),Adam:()=>it.adam(.001,.9,.999,Zr()),Adamax:()=>it.adamax(.002,.9,.999,Zr(),0),RMSProp:()=>it.rmsprop(.001,.9,0,Zr()),SGD:()=>it.sgd(.01)};if(e.adagrad=e.Adagrad,e.adadelta=e.Adadelta,e.adam=e.Adam,e.adamax=e.Adamax,e.rmsprop=e.RMSProp,e.sgd=e.SGD,t in e)return e[t]();throw new Qr("Unknown Optimizer "+t)}(t.optimizer),this.isOptimizerOwned=!0;else{if(!(t.optimizer instanceof lt))throw new Qr("User-defined optimizer must be an instance of tf.Optimizer.");this.optimizer_=t.optimizer,this.isOptimizerOwned=!1}let e=[];if(Array.isArray(t.loss)||"string"==typeof t.loss||"function"==typeof t.loss)if(Array.isArray(t.loss)){if(t.loss.length!==this.outputs.length)throw new Qr(`When passing an Array as loss, it should have one entry per model output. The model has ${this.outputs.length} output(s), but you passed loss=${t.loss}.`);const n=t.loss;e=n.map(t=>Sl(t))}else{const n=Sl(t.loss);this.outputs.forEach(t=>{e.push(n)})}else{t.loss=t.loss;for(const e in t.loss)if(-1===this.outputNames.indexOf(e))throw new Qr(`Unknown entry in loss dictionary: "${e}". Only expected the following keys: `+this.outputNames);for(const n of this.outputNames)null==t.loss[n]&&console.warn(`Output "${n}" is missing from loss dictionary. We assume this was done on purpose, and we will not be expecting data to be passed to ${n} during training`),e.push(Sl(t.loss[n]))}this.lossFunctions=e,this.feedOutputNames=[],this.feedOutputShapes=[],this.feedLossFns=[];for(let t=0;t<this.outputs.length;++t){const e=this.internalOutputShapes[t],n=this.outputNames[t];this.feedOutputNames.push(n),this.feedOutputShapes.push(e),this.feedLossFns.push(this.lossFunctions[t])}const n=[];this.metrics=t.metrics,this.metricsNames=["loss"],this.metricsTensors=[],Pa("loss",()=>{for(let t=0;t<this.outputs.length;++t){if(-1!==n.indexOf(t))continue;const e=this.lossFunctions[t];this.outputs.length>1&&(this.metricsTensors.push([e,t]),this.metricsNames.push(this.outputNames[t]+"_loss"))}});const s=function(t,e){if(null==t||Array.isArray(t)&&0===t.length)return e.map(t=>[]);let n;if("string"==typeof t||"function"==typeof t)n=[t];else{if(!Array.isArray(t)&&"object"!=typeof t)throw new TypeError("Type of metrics argument not understood. Expected an string,function, Array, or Object, found: "+t);n=t}if(Array.isArray(n))return e.map(t=>n);{const t=[];for(const s of e){let e=n.hasOwnProperty(s)?n[s]:[];Array.isArray(e)||(e=[e]),t.push(e)}return t}}(t.metrics,this.outputNames),i=(t,e,n)=>{this.outputNames.length>1&&(e=this.outputNames[t]+"_"+e),this.metricsNames.push(e),this.metricsTensors.push([n,t])};Pa("metric",()=>{for(let t=0;t<this.outputs.length;++t){if(-1!==n.indexOf(t))continue;(e=>{let n,s,r;for(const a of e){if("string"==typeof a&&-1!==["accuracy","acc","crossentropy","ce"].indexOf(a)){const e=this.internalOutputShapes[t];let i;1===e[e.length-1]||this.lossFunctions[t]===kl?-1!==["accuracy","acc"].indexOf(a)?s=Il:-1!==["crossentropy","ce"].indexOf(a)&&(s=Dl):this.lossFunctions[t]===wl?-1!==["accuracy","acc"].indexOf(a)?s=Tl:-1!==["crossentropy","ce"].indexOf(a)&&(s=Fl):-1!==["accuracy","acc"].indexOf(a)?s=Nl:-1!==["crossentropy","ce"].indexOf(a)&&(s=El),-1!==["accuracy","acc"].indexOf(a)?i="acc":-1!==["crossentropy","ce"].indexOf(a)&&(i="ce"),r=s,n=""+i}else{const t=_l(a);r=t,n=""+Ll(a)}let e;Pa(n,()=>{e=r}),i(t,n,e)}})(s[t])}}),this.collectedTrainableWeights=this.trainableWeights}checkTrainableWeightsConsistency(){null!=this.collectedTrainableWeights&&this.trainableWeights.length!==this.collectedTrainableWeights.length&&console.warn("Discrepancy between trainableweights and collected trainable weights. Did you set `model.trainable` without calling `model.compile()` afterwards?")}evaluate(t,e,n={}){const s=null==n.batchSize?32:n.batchSize;ru(s);const i=this.standardizeUserDataXY(t,e,!0,s);try{const r=i[0].concat(i[1]);this.makeTestFunction();const a=this.testFunction;return ra(this.testLoop(a,r,s,n.verbose,n.steps))}finally{cu(i[0],t),cu(i[1],e)}}async evaluateDataset(t,n){return this.makeTestFunction(),async function(t,n,i){const r=null!=(i=i||{}).batches,o=t.testFunction;let h=[];if(i.verbose>0)throw new ta("Verbose mode is not implemented yet.");e.assert(!r||i.batches>0&&Number.isInteger(i.batches),()=>"Test loop expects `batches` to be a positive integer, but received "+JSON.stringify(i.batches));const c="function"==typeof n.next?n:await n.iterator();let p=0,d=0;for(;!r||d<i.batches;){const e=await c.next();if(h=s(()=>{if(e.value){const{xs:n,ys:i}=eu(t,e.value),r=n.concat(i),l=s(()=>o(r));if(B(r),0===d)for(let t=0;t<l.length;++t)h.push($(0));const c=r[0].shape[0];for(let t=0;t<l.length;++t){const e=l[t],n=h[t];h[t]=s(()=>u(h[t],a(c,e))),d>0&&B(n)}B(l),p+=c,++d}return h}),e.done){r&&console.warn(`Your dataset iterator ran out of data during evaluateDataset(). Interrupting evalution. Make sure that your dataset can generate at least \`batches\` batches (in this case, ${i.batches} batches). You may need to use the repeat() function when building your dataset.`);break}}for(let t=0;t<h.length;++t){const e=h[t];h[t]=l(h[t],p),B(e)}return ra(h)}(this,t,n)}checkNumSamples(t,e,n,s="steps"){let i;if(null!=n){if(i=null,null!=e)throw new Qr(`If ${s} is set, batchSize must be null or undefined.Got batchSize = `+e)}else{if(null==t)throw new Qr("Either the input data should have a defined shape, or "+s+" shoud be specified.");i=Array.isArray(t)?t[0].shape[0]:t.shape[0]}return i}execute(t,e){if(Array.isArray(e)&&0===e.length)throw new Qr("`outputs` is an empty Array, which is not allowed.");const n=Array.isArray(e),s=n?e:[e],i=this.retrieveSymbolicTensors(s),r=new jl;if(t instanceof ot&&(t=[t]),Array.isArray(t)){if(t.length!==this.inputs.length)throw new Qr(`The number of inputs provided (${t.length}) does not match the number of inputs of this model (${this.inputs.length}).`);for(let e=0;e<this.inputs.length;++e)r.add(this.inputs[e],t[e])}else for(const e of this.inputs){const n=t[e.name];if(null==n)throw new Qr("No value is provided for the model's input "+e.name);r.add(e,n)}const a=Gl(i,r);return n?a:a[0]}retrieveSymbolicTensors(t){const e=na(null,t.length);let n=t.length;for(const s of this.layers){const i=Array.isArray(s.output)?s.output:[s.output],r=i.map(t=>t.name);for(let s=0;s<t.length;++s){const a=r.indexOf(t[s]);if(-1!==a&&(e[s]=i[a],n--),0===n)break}if(0===n)break}if(n>0){const n=[];throw e.forEach((e,s)=>{null==e&&n.push(t[s])}),new Qr("Cannot find SymbolicTensors for output name(s): "+JSON.stringify(n))}return e}predictLoop(t,e=32,n=!1){return s(()=>{const i=this.checkNumSamples(t);if(n)throw new ta("Verbose predictLoop() is not implemented yet.");const r=lu(i,e),a=this.outputs.map(t=>[]);for(let e=0;e<r.length;++e){s(()=>{const n=r[e][0],s=r[e][1],i=au(t,n,s),a=[];if(Array.isArray(i))for(let t=0;t<i.length;++t)a.push({key:this.inputs[t],value:i[t]});else a.push({key:this.inputs[0],value:i});const o=new jl(a);return Gl(this.outputs,o)}).forEach((t,e)=>a[e].push(t))}return ra(a.map(t=>N(t,0)))})}predict(t,e={}){const n=hu(t);gu(n,this.inputNames,this.feedInputShapes,!1);try{const s=null==e.batchSize?32:e.batchSize;return ru(s),this.predictLoop(n,s)}finally{cu(n,t)}}predictOnBatch(t){gu(t,this.inputNames,this.feedInputShapes,!0);const e=(Array.isArray(t)?t[0]:t).shape[0];return this.predictLoop(t,e)}standardizeUserDataXY(t,n,s=!0,i){if(null==this.optimizer_)throw new Yr("You must compile a model before training/testing. Use LayersModel.compile(modelCompileArgs).");const r=[];for(let t=0;t<this.feedOutputShapes.length;++t){const e=this.feedOutputShapes[t];this.feedLossFns[t]===wl?r.push(e.slice(0,e.length-1).concat([1])):r.push(e)}if(function(t,n,s){const i=da(t.map(t=>t.shape[0]));i.sort();const r=da(n.map(t=>t.shape[0]));if(r.sort(),i.length>1)throw new Qr("All input Tensors (x) should have the same number of samples. Got array shapes: "+JSON.stringify(t.map(t=>t.shape)));if(r.length>1)throw new Qr("All target Tensors (y) should have the same number of samples. Got array shapes: "+JSON.stringify(n.map(t=>t.shape)));if(i.length>0&&r.length>0&&!e.arraysEqual(i,r))throw new Qr(`Input Tensors should have the same number of samples as target Tensors. Found ${i[0]} input sample(s) and ${r[0]} target sample(s).`)}(t=fu(t,this.feedInputNames,this.feedInputShapes,!1,"input"),n=fu(n,this.feedOutputNames,r,!1,"target")),function(t,e,n){const s=[gl,kl,bl];for(let i=0;i<t.length;++i){const r=t[i],a=e[i],o=n[i];if(null!=a){if(a===bl&&1===r.shape[r.shape.length-1])throw new Qr(`You are passing a target array of shape ${r.shape} while using a loss 'categorical_crossentropy'. 'categorical_crossentropy'expects targets to be binary matrices (1s and 0s) of shape [samples, classes].`);if(-1!==s.indexOf(a)){const t=r.shape.slice(1),e=o.slice(1);for(let n=0;n<t.length;++n){const s=t[n],i=e[n];if(null!=i&&s!==i)throw new Qr(`A target Tensor with shape ${r.shape} was passed for an output of shape ${o}, while using a loss function that expects targets to have the same shape as the output.`)}}}}}(n,this.feedLossFns,this.feedOutputShapes),this.stateful&&null!=i&&i>0&&t[0].shape[0]%i!=0)throw new Qr(`In a stateful network, you should only pass inputs with a number of samples that is divisible by the batch size ${i}. Found: ${t[0].shape[0]} sample(s).`);return[t,n]}async standardizeUserData(t,e,n,s,i=!0,r){const[a,o]=this.standardizeUserDataXY(t,e,i,r);if(null!=n)throw new Error("sample weight is not supported yet.");let l=null;if(null!=s){const t=Yl(s,this.outputNames);l=[];for(let e=0;e<t.length;++e)l.push(await Ql(o[e],null,t[e]))}return[a,o,l]}testLoop(t,e,n,i=0,r){return s(()=>{const s=this.checkNumSamples(e,n,r,"steps"),o=[];if(i>0)throw new ta("Verbose mode is not implemented yet.");if(null!=r)throw new ta("steps mode in testLoop() is not implemented yet");{const i=lu(s,n),r=p(Ja(0,s));for(let n=0;n<i.length;++n){const s=i[n][0],l=i[n][1],h=Ya(r,s,l-s),c=ou(e,h),p=t(c);if(0===n)for(let t=0;t<p.length;++t)o.push($(0));for(let t=0;t<p.length;++t){const e=p[t];o[t]=u(o[t],a(l-s,e))}}for(let t=0;t<o.length;++t)o[t]=l(o[t],s)}return o})}getDedupedMetricsNames(){const t=this.metricsNames,e=[];for(let n=0;n<t.length;++n){const s=t[n];let i=s;if(ia(t,s)>1){i+="_"+ia(t.slice(0,n),s)}e.push(i)}return e}makeTrainFunction(){return t=>{const e=[],n=t.slice(0,this.inputs.length),s=t.slice(this.inputs.length,this.inputs.length+this.outputs.length),i=t.slice(this.inputs.length+this.outputs.length,this.inputs.length+2*this.outputs.length),r=[],a=this.collectedTrainableWeights.map(t=>t.read());return[this.optimizer_.minimize(()=>{const t=[];for(let e=0;e<this.inputs.length;++e)t.push({key:this.inputs[e],value:n[e]});const a=new jl(t),o=Gl(this.outputs,a,{training:!0});let l;for(let t=0;t<this.lossFunctions.length;++t){let n=(0,this.lossFunctions[t])(s[t],o[t]);null!=i[t]&&(n=tu(n,i[t]));const r=j(n);e.push(r),l=0===t?n:u(l,n)}for(let t=0;t<this.metricsTensors.length;++t){let n;if(this.outputs.length>1&&t<this.outputs.length)n=e[t];else{const e=this.metricsTensors[t][0],i=this.metricsTensors[t][1];n=j(e(s[i],o[i]))}W(n),r.push(n)}return l=j(l),this.calculateLosses().forEach(t=>{l=u(l,t)}),l},!0,a)].concat(r)}}makeTestFunction(){this.testFunction=t=>s(()=>{const e=[];let n;const s=t.slice(0,this.inputs.length),i=t.slice(this.inputs.length,this.inputs.length+this.outputs.length),r=[];for(let t=0;t<this.inputs.length;++t)r.push({key:this.inputs[t],value:s[t]});const a=new jl(r),o=Gl(this.outputs,a);for(let t=0;t<this.lossFunctions.length;++t){const s=this.lossFunctions[t],r=j(s(i[t],o[t]));n=0===t?r:u(n,r),e.push(n)}for(let t=0;t<this.metricsTensors.length;++t){const n=this.metricsTensors[t][0],s=this.metricsTensors[t][1],r=j(n(i[s],o[s]));e.push(r)}return e})}async fit(t,e,n={}){return uu(this,t,e,n)}async fitDataset(t,e){return su(this,t,e)}async trainOnBatch(t,e){const n=await this.standardizeUserData(t,e),s=n[0],i=n[1],r=this.makeTrainFunction()(s.concat(i)),a=[];for(const t of r){const e=await t.data();a.push(e[0])}return B(r),ra(a)}getNamedWeights(t){const e=[],n=null!=t&&t.trainableOnly,s=n?this.trainableWeights:this.weights,i=this.getWeights(n);for(let t=0;t<s.length;++t)n&&!s[t].trainable||e.push({name:s[t].originalName,tensor:i[t]});return e}set stopTraining(t){this.stopTraining_=t}get stopTraining(){return this.stopTraining_}get optimizer(){return this.optimizer_}set optimizer(t){this.optimizer_!==t&&(this.optimizer_=t,this.isOptimizerOwned=!1)}dispose(){const t=super.dispose();if(0===t.refCountAfterDispose&&null!=this.optimizer&&this.isOptimizerOwned){const e=rt().numTensors;this.optimizer_.dispose(),t.numDisposedVariables+=e-rt().numTensors}return t}getLossIdentifiers(){let t;if("string"==typeof this.loss)t=oa(this.loss);else if(Array.isArray(this.loss)){for(const t of this.loss)if("string"!=typeof t)throw new Error("Serialization of non-string loss is not supported.");t=this.loss.map(t=>oa(t))}else{const e=Object.keys(this.loss);t={};const n=this.loss;for(const s of e){if("string"!=typeof n[s])throw new Error("Serialization of non-string loss is not supported.");t[s]=oa(n[s])}}return t}getMetricIdentifiers(){if("string"==typeof this.metrics||"function"==typeof this.metrics)return[oa(Ll(this.metrics))];if(Array.isArray(this.metrics))return this.metrics.map(t=>oa(Ll(t)));{const t={};for(const e in this.metrics)t[e]=oa(Ll(this.metrics[e]));return t}}getTrainingConfig(){return{loss:this.getLossIdentifiers(),metrics:this.getMetricIdentifiers(),optimizer_config:{class_name:this.optimizer.getClassName(),config:this.optimizer.getConfig()}}}loadTrainingConfig(t){if(null!=t.weighted_metrics)throw new Error("Loading weight_metrics is not supported yet.");if(null!=t.loss_weights)throw new Error("Loading loss_weights is not supported yet.");if(null!=t.sample_weight_mode)throw new Error("Loading sample_weight_mode is not supported yet.");const e=dl(Ul(t.optimizer_config));let n,s;if("string"==typeof t.loss)n=la(t.loss);else if(Array.isArray(t.loss))n=t.loss.map(t=>la(t));else if(null!=t.loss){n={};for(const e in t.loss)n[e]=la(t.loss[e])}if(Array.isArray(t.metrics))s=t.metrics.map(t=>la(t));else if(null!=t.metrics){s={};for(const e in t.metrics)s[e]=la(t.metrics[e])}this.compile({loss:n,metrics:s,optimizer:e})}async save(t,e){if("string"==typeof t){const e=ut.getSaveHandlers(t);if(0===e.length)throw new Qr(`Cannot find any save handlers for URL '${t}'`);if(e.length>1)throw new Qr(`Found more than one (${e.length}) save handlers for URL '${t}'`);t=e[0]}if(null==t.save)throw new Qr("LayersModel.save() cannot proceed because the IOHandler provided does not have the `save` attribute defined.");const n=await ut.encodeWeights(this.getNamedWeights(e)),s={modelTopology:this.toJSON(null,!1),format:"layers-model",generatedBy:"TensorFlow.js tfjs-layers v3.0.0",convertedBy:null};if(null!=e&&e.includeOptimizer&&null!=this.optimizer){s.trainingConfig=this.getTrainingConfig();const t="optimizer",{data:e,specs:i}=await ut.encodeWeights(await this.optimizer.getWeights(),t);n.specs.push(...i),n.data=ut.concatenateArrayBuffers([n.data,e])}if(null!=this.userDefinedMetadata){const t=!0;Rl(this.userDefinedMetadata,this.name,t),s.userDefinedMetadata=this.userDefinedMetadata}return s.weightData=n.data,s.weightSpecs=n.specs,t.save(s)}setUserDefinedMetadata(t){Rl(t,this.name),this.userDefinedMetadata=t}getUserDefinedMetadata(){return this.userDefinedMetadata}}mu.className="Model",n.registerClass(mu);class yu extends mu{}async function bu(t,e){if(null==e&&(e={}),"string"==typeof t){const n=ut.getLoadHandlers(t,e);if(0===n.length)n.push(ut.browserHTTPRequest(t,e));else if(n.length>1)throw new Qr(`Found more than one (${n.length}) load handlers for URL '${t}'`);t=n[0]}return async function(t,e,n){null==n&&(n={});if(null==t.load)throw new Qr("Cannot proceed with model loading because the IOHandler provided does not have the `load` method implemented.");const s=await t.load();let i=s.modelTopology;null!=i.model_config&&(i=i.model_config);const r=null==n.strict||n.strict,a=null!=s.weightData&&null!=s.weightSpecs&&r,o=dl(Ul(i),e,a),l=s.trainingConfig;null!=l&&o.loadTrainingConfig(l);null!=s.userDefinedMetadata&&o.setUserDefinedMetadata(s.userDefinedMetadata);if(null!=s.weightData){if(null==s.weightSpecs)throw new Qr("LayersModel artifacts contains weight data, but not weight specs. Therefore loading of weights cannot proceed.");const{modelWeights:t,optimizerWeights:e}=function(t,e){const n=ut.decodeWeights(t,e),s={},i=[];return e.forEach(t=>{"optimizer"===t.group?i.push({name:t.name,tensor:n[t.name]}):s[t.name]=n[t.name]}),{modelWeights:s,optimizerWeights:i}}(s.weightData,s.weightSpecs);o.loadWeights(t,r),null!=o.optimizer&&e.length>0&&await o.optimizer.setWeights(e),B(t),B(e.map(t=>t.tensor))}return o}(t,void 0,e)}yu.className="Functional",n.registerClass(yu);class wu extends mu{constructor(t){if(super({inputs:[],outputs:[]}),t=t||{},this.trainable=!0,this.built=!1,this.name=null!=t.name?t.name:Bo("sequential_"),null!=t.layers)for(const e of t.layers)this.add(e)}checkShape(t){if(t.inboundNodes[0].outputTensors[0].shape.some(t=>t<0))throw new Qr("Negative dimension size caused by adding layer "+t.name+" with input shape ["+t.inboundNodes[0].inputTensors[0].shape+"]")}add(t){const e=t instanceof wu||t instanceof mu;let n;if(e){if(n=t,1!==n.outputs.length)throw new Qr("All layers in a Sequential model should have a single output tensor. For multi-output layers, use the functional API.");if(1!==n.inputs.length)throw new Qr("All layers in a Sequential model should have a single input tensor. For multi-input layers, use the functional API.")}if(0===this.outputs.length){if(0===t.inboundNodes.length){if(null==t.batchInputShape)throw new Qr("The first layer in a Sequential model must get an `inputShape` or `batchInputShape` argument.");const e=el({batchShape:t.batchInputShape,dtype:t.dtype,name:t.name+"_input"});t.apply(e)}if(e)this.outputs=n.outputs,this.inputs=n.inputs;else{if(1!==t.inboundNodes.length)throw new Qr(`A layer added to a Sequential model must not already be connected somewhere else. LayersModel received layer ${t.name} which has ${t.inboundNodes.length} pre-existing inbound connections.`);if(1!==t.inboundNodes[0].outputTensors.length)throw new Qr("All layers in a Sequential model should have a single output tensor. For multi-output layers, use the functional API.");this.checkShape(t),this.outputs=[t.inboundNodes[0].outputTensors[0]],this.inputs=function t(e,n,s){if((null==n||null!=s&&s>0)&&(n=e.sourceLayer,s=e.nodeIndex),0===n.inboundNodes.length)return[e];{const e=n.inboundNodes[s];if(0===e.inboundLayers.length)return e.inputTensors;{const n=[];for(let s=0;s<e.inboundLayers.length;s++){const i=t(e.inputTensors[s],e.inboundLayers[s],e.nodeIndices[s]);for(const t of i)-1===n.indexOf(t)&&n.push(t)}return n}}}(this.outputs[0])}this.inboundNodes=[],new Xo({outboundLayer:this,inboundLayers:[],nodeIndices:[],tensorIndices:[],inputTensors:this.inputs,outputTensors:this.outputs,inputMasks:na(null,this.inputs.length),outputMasks:[null],inputShapes:this.inputs.map(t=>t.shape),outputShapes:this.outputs[0].shape})}else{const e=t.apply(this.outputs[0]);if(Array.isArray(e))throw new TypeError("All layers in a Sequential model should have a single output tensor. For multi-output layers, use the functional API.");this.checkShape(t),this.outputs=[e],this.inboundNodes[0].outputTensors=this.outputs,this.inboundNodes[0].outputShapes=[this.outputs[0].shape]}this.layers.push(t),this.built=!1}pop(){if(0===this.layers.length)throw new TypeError("There are no layers in the model.");if(this.layers.pop(),0===this.layers.length)this.outputs=[],this.inboundNodes=[],this.outboundNodes=[];else{const t=this.layers.length-1;this.layers[t].outboundNodes=[],this.outputs=[this.layers[t].output],this.inboundNodes[0].outputTensors=this.outputs,this.inboundNodes[0].outputShapes=[this.outputs[0].shape]}}call(t,e){return null==this.model&&this.build(),this.model.call(t,e)}build(t){if(Ko(t),0===this.inputs.length||0===this.outputs.length)throw new TypeError("Sequential model cannot be built: model is empty. Add some layers first.");this.model=new mu({inputs:this.inputs,outputs:this.outputs[0],name:this.name+"_model"}),this.model.trainable=this.trainable,this.supportsMasking=this.model.supportsMasking,this.inputLayers=this.model.inputLayers,this.inputLayersNodeIndices=this.model.inputLayersNodeIndices,this.inputLayersTensorIndices=this.model.inputLayersTensorIndices,this.outputLayers=this.model.outputLayers,this.outputLayersNodeIndices=this.model.outputLayersNodeIndices,this.outputLayersTensorIndices=this.model.outputLayersTensorIndices,this.nodesByDepth=this.model.nodesByDepth,this.containerNodes=this.model.containerNodes,this.outputNames=this.model.outputNames,this.inputNames=this.model.inputNames,this.built=!0}countParams(){return this.built||this.build(),super.countParams()}summary(t,e,n=console.log){this.built||this.build(),super.summary(t,e,n)}setWeights(t){null==this.model&&this.build(),this.model.setWeights(t)}evaluate(t,e,n={}){if(!this.built)throw new Yr("The model needs to be compiled before being used.");return this.model.evaluate(t,e,n)}async evaluateDataset(t,e){if(!this.built)throw new Yr("The model needs to be compiled before being used.");return this.model.evaluateDataset(t,e)}predict(t,e={}){return null==this.model&&this.build(),this.model.predict(t,e)}predictOnBatch(t){return null==this.model&&this.build(),this.model.predictOnBatch(t)}compile(t){this.build(),this.model.compile(t),this.optimizer_=this.model.optimizer,this.isOptimizerOwned=this.model.isOptimizerOwned,this.loss=this.model.loss,this.metrics=this.model.metrics,this.metricsTensors=this.model.metricsTensors,this.metricsNames=this.model.metricsNames}get optimizer(){return null==this.model?void 0:this.model.optimizer}set optimizer(t){this.model.optimizer=t}async fit(t,e,n={}){if(!this.built)throw new Yr("The model needs to be compiled before being used.");return this.model.fit(t,e,n)}async fitDataset(t,e){if(!this.built)throw new Yr("The model needs to be compiled before being used.");return this.model.fitDataset(t,e)}async trainOnBatch(t,e){return this.model.trainOnBatch(t,e)}static fromConfig(t,n,s={},i=!1){let r,a={};if(n instanceof Array){if(null==n[0].className||"Merge"===n[0].className)throw new Qr("Legacy serialization format not supported yet.");r=n}else e.assert(null!=n.layers,()=>"When the config data for a Sequential model is not an Array, it must be an Object that contains the 'layers' field."),r=n.layers,delete n.layers,a=n;const o=new t(a);if(!(o instanceof wu))throw new ta("Sequential.fromConfig called on non-Sequential input: "+o);for(const t of r){const e=dl(t,void 0,i);i&&e.setFastWeightInitDuringBuild(!0),o.add(e)}return o}set stopTraining(t){if(null==this.model)throw new Qr("Cannot set the stopTraining property of a sequential model before it is compiled.");this.model.stopTraining=t}get stopTraining(){if(null==this.model)throw new Qr("Cannot get the stopTraining property of a sequential model before it is compiled.");return this.model.stopTraining}getConfig(){const t=[];for(const e of this.layers){const n={};n.className=e.getClassName(),n.config=e.getConfig(),t.push(n)}return{name:this.name,layers:t}}}function ku(t){return new mu(t)}function xu(t){return new wu(t)}function vu(t,e){return null==e&&(e={}),bu(t,e)}function Su(t){return el(t)}function Iu(t,e){cl.registerCallbackConstructor(t,e)}wu.className="Sequential",n.registerClass(wu);class Nu extends n.Serializable{getConfig(){return{}}}class Au extends Nu{apply(t,e=1){return function(t,e=1){if(1!==e)throw new ta(`Support for alpha values other than 1 (${e}) is not implemented yet.`);return v(t)}(t,e)}}Au.className="elu",n.registerClass(Au);class Cu extends Nu{apply(t){return ht(t)}}Cu.className="selu",n.registerClass(Cu);class zu extends Nu{apply(t){return h(t)}}zu.className="relu",n.registerClass(zu);class Du extends Nu{apply(t){return s(()=>ct(6,h(t)))}}Du.className="relu6",n.registerClass(Du);class Tu extends Nu{apply(t){return t}}Tu.className="linear",n.registerClass(Tu);class Eu extends Nu{apply(t){return pt(t)}}Eu.className="sigmoid",n.registerClass(Eu);class Fu extends Nu{apply(t){return function(t){return s(()=>{const e=u(.5,a(.2,t));return o(e,0,1)})}(t)}}Fu.className="hardSigmoid",n.registerClass(Fu);class $u extends Nu{apply(t){return X(t)}}$u.className="softplus",n.registerClass($u);class _u extends Nu{apply(t){return function(t){return s(()=>l(t,S(t).add(1)))}(t)}}_u.className="softsign",n.registerClass(_u);class Lu extends Nu{apply(t){return dt(t)}}Lu.className="tanh",n.registerClass(Lu);class Ru extends Nu{apply(t,e=-1){return V(t,e)}}Ru.className="softmax",n.registerClass(Ru);class Mu extends Nu{apply(t,e=-1){return ft(t,e)}}Mu.className="logSoftmax",n.registerClass(Mu);class Ou extends Nu{apply(t,e=1){return s(()=>pt(t.mul(e)).mul(t))}}function Bu(t){return t.getClassName()}function Pu(t,e={}){return ca(t,n.SerializationMap.getMap().classNameMap,e,"activation")}function Wu(t){if(null==t){const t={className:"linear",config:{}};return Pu(t)}if("string"==typeof t){const e={};return e.className=t,e.config={},Pu(e)}return t instanceof Nu?t:Pu(t)}function Uu(t){if(null!=t&&"object"!=typeof t)throw new Error("Argument to L1L2 regularizer's constructor is expected to be an object, but received: "+t)}Ou.className="swish",n.registerClass(Ou);class Ku extends n.Serializable{}class ju extends Ku{constructor(t){super(),Uu(t),this.l1=null==t||null==t.l1?.01:t.l1,this.l2=null==t||null==t.l2?.01:t.l2,this.hasL1=0!==this.l1,this.hasL2=0!==this.l2}apply(t){return s(()=>{let e=E([1]);return this.hasL1&&(e=u(e,r(a(this.l1,S(t))))),this.hasL2&&(e=u(e,r(a(this.l2,oo(t))))),e.asScalar()})}getConfig(){return{l1:this.l1,l2:this.l2}}static fromConfig(t,e){return new t({l1:e.l1,l2:e.l2})}}ju.className="L1L2",n.registerClass(ju);const Vu={l1l2:"L1L2"};function qu(t){return ha(t)}function Gu(t,e={}){return ca(t,n.SerializationMap.getMap().classNameMap,e,"regularizer")}function Hu(t){if(null==t)return null;if("string"==typeof t){return Gu({className:t in Vu?Vu[t]:t,config:{}})}return t instanceof Ku?t:Gu(t)}class Ju extends Qo{constructor(t){super(null==t?{}:t),this.supportsMasking=!0,null!=t&&(this.maxValue=t.maxValue)}call(t,e){t=Uo(t);let n=h(t);return null!=this.maxValue&&(n=o(n,0,this.maxValue)),n}computeOutputShape(t){return t}getConfig(){const t={maxValue:this.maxValue},e=super.getConfig();return Object.assign(t,e),t}}Ju.className="ReLU",n.registerClass(Ju);class Zu extends Qo{constructor(t){super(null==t?{}:t),this.DEFAULT_ALPHA=.3,null==t&&(t={}),this.alpha=null==t.alpha?this.DEFAULT_ALPHA:t.alpha}call(t,e){const n=Uo(t);return gt(n,this.alpha)}computeOutputShape(t){return t}getConfig(){const t={alpha:this.alpha},e=super.getConfig();return Object.assign(t,e),t}}Zu.className="LeakyReLU",n.registerClass(Zu);class Xu extends Qo{constructor(t){if(super(null==t?{}:t),this.DEFAULT_ALPHA_INITIALIZER="zeros",null==t&&(t={}),this.supportsMasking=!0,this.alphaInitializer=_o(t.alphaInitializer||this.DEFAULT_ALPHA_INITIALIZER),this.alphaRegularizer=Hu(t.alphaRegularizer),this.alphaConstraint=za(t.alphaConstraint),null==t.sharedAxes)this.sharedAxes=null;else if(Array.isArray(t.sharedAxes))this.sharedAxes=t.sharedAxes;else{if("number"!=typeof t.sharedAxes)throw new Qr("Expected sharedAxes to be a number or an array of numbers, but got "+t.sharedAxes);this.sharedAxes=[t.sharedAxes]}}build(t){const e=(t=Ko(t)).slice(1);if(null!=this.sharedAxes)for(const t of this.sharedAxes)e[t-1]=1;this.alpha=this.addWeight("alpha",e,"float32",this.alphaInitializer,this.alphaRegularizer,!0,this.alphaConstraint);const n={};if(null!=this.sharedAxes)for(let e=1;e<t.length;++e)n[e]=t[e];this.inputSpec=[new Ho({ndim:t.length,axes:n})],this.built=!0}call(t,e){return t=Uo(t),mt(t,this.alpha.read())}getConfig(){const t={alphaInitializer:$o(this.alphaInitializer),alphaRegularizer:qu(this.alphaRegularizer),alphaConstraint:Aa(this.alphaConstraint),sharedAxes:this.sharedAxes},e=super.getConfig();return Object.assign(t,e),t}}Xu.className="PReLU",n.registerClass(Xu);class Yu extends Qo{constructor(t){if(super(null==t?{}:t),this.DEFAULT_ALPHA=1,null==t&&(t={}),null!=t.alpha&&t.alpha!==this.DEFAULT_ALPHA)throw new ta(`Non-default alpha value (${t.alpha}) is not supported by the ELU layer yet.`);this.alpha=null==t.alpha?this.DEFAULT_ALPHA:t.alpha}call(t,e){const n=Uo(t);return v(n)}computeOutputShape(t){return t}getConfig(){const t={alpha:this.alpha},e=super.getConfig();return Object.assign(t,e),t}}Yu.className="ELU",n.registerClass(Yu);class Qu extends Qo{constructor(t){super(null==t?{}:t),this.DEFAULT_THETA=1,null==t&&(t={}),this.theta=null==t.theta?this.DEFAULT_THETA:t.theta}call(t,e){const n=Uo(t);return n.mul(Za(n.greater(this.theta),"float32"))}computeOutputShape(t){return t}getConfig(){const t={theta:this.theta},e=super.getConfig();return Object.assign(t,e),t}}Qu.className="ThresholdedReLU",n.registerClass(Qu);class th extends Qo{constructor(t){super(null==t?{}:t),this.DEFAULT_AXIS=1,null==t&&(t={}),this.softmax=(new Ru).apply,this.axis=null==t.axis?this.DEFAULT_AXIS:t.axis}call(t,e){const n=Uo(t);return this.softmax(n,this.axis)}computeOutputShape(t){return t}getConfig(){const t={axis:this.axis},e=super.getConfig();return Object.assign(t,e),t}}function eh(t,e,n){if("number"==typeof t)return na(t,e);if(t.length!==e)throw new Qr(`The ${n} argument must be an integer or tuple of ${e} integers. Received: ${t.length} elements.`);for(let i=0;i<e;++i){const r=t[i];if((s=r)!==parseInt(s.toString(),10))throw new Qr(`The ${n} argument must be an integer or tuple of ${e} integers. Received: ${JSON.stringify(t)} including a non-integer number `+r)}return t;var s}function nh(t,e,n,s,i=1){if(null==t)return t;let r;return r="same"===n?t:t-(e+(e-1)*(i-1))+1,Math.floor((r+s-1)/s)}function sh(t,e,n,s){if(null==t)return null;if("valid"===s)t=t*e+Ha([n-e,0]);else{if("same"!==s)throw new Qr(`Unsupport padding mode: ${s}.`);t*=e}return t}function ih(t,e){return s(()=>(Ra(e),"channelsFirst"===e?yt(t,[0,2,3,1]):t))}function rh(t,e){return s(()=>(Ra(e),"channelsFirst"===e?yt(t,[0,2,3,4,1]):t))}function ah(t,e,n,i=[1,1],r="valid",a,o,l=null){return s(()=>{if(null==a&&(a="channelsLast"),Ra(a),3!==t.rank&&4!==t.rank)throw new Qr(`conv2dWithBiasActivation expects input to be of rank 3 or 4, but received ${t.rank}.`);if(3!==e.rank&&4!==e.rank)throw new Qr(`conv2dWithBiasActivation expects kernel to be of rank 3 or 4, but received ${t.rank}.`);let s=ih(t,a);if("causal"===r)throw new ta("The support for CAUSAL padding mode in conv1dWithBias is not implemented yet.");return s=A.conv2d({x:s,filter:e,strides:i,pad:"same"===r?"same":"valid",dilations:o,dataFormat:"NHWC",bias:n,activation:l}),"channelsFirst"===a&&(s=yt(s,[0,3,1,2])),s})}th.className="Softmax",n.registerClass(th);class oh extends Qo{constructor(t,e){if(super(e),this.bias=null,this.DEFAULT_KERNEL_INITIALIZER="glorotNormal",this.DEFAULT_BIAS_INITIALIZER="zeros",oh.verifyArgs(e),this.rank=t,ya(this.rank,"rank"),1!==this.rank&&2!==this.rank&&3!==this.rank)throw new ta(`Convolution layer for rank other than 1, 2, or 3 (${this.rank}) is not implemented yet.`);if(this.kernelSize=eh(e.kernelSize,t,"kernelSize"),this.strides=eh(null==e.strides?1:e.strides,t,"strides"),this.padding=null==e.padding?"valid":e.padding,Ma(this.padding),this.dataFormat=null==e.dataFormat?"channelsLast":e.dataFormat,Ra(this.dataFormat),this.activation=Wu(e.activation),this.useBias=null==e.useBias||e.useBias,this.biasInitializer=_o(e.biasInitializer||this.DEFAULT_BIAS_INITIALIZER),this.biasConstraint=za(e.biasConstraint),this.biasRegularizer=Hu(e.biasRegularizer),this.activityRegularizer=Hu(e.activityRegularizer),this.dilationRate=eh(null==e.dilationRate?1:e.dilationRate,t,"dilationRate"),1===this.rank&&Array.isArray(this.dilationRate)&&1!==this.dilationRate.length)throw new Qr("dilationRate must be a number or an array of a single number for 1D convolution, but received "+JSON.stringify(this.dilationRate));if(2===this.rank){if("number"==typeof this.dilationRate)this.dilationRate=[this.dilationRate,this.dilationRate];else if(2!==this.dilationRate.length)throw new Qr("dilationRate must be a number or array of two numbers for 2D convolution, but received "+JSON.stringify(this.dilationRate))}else if(3===this.rank)if("number"==typeof this.dilationRate)this.dilationRate=[this.dilationRate,this.dilationRate,this.dilationRate];else if(3!==this.dilationRate.length)throw new Qr("dilationRate must be a number or array of three numbers for 3D convolution, but received "+JSON.stringify(this.dilationRate))}static verifyArgs(t){if(sa("kernelSize"in t,"required key 'kernelSize' not in config"),"number"!=typeof t.kernelSize&&!ma(t.kernelSize,"number",1,3))throw new Qr(`BaseConv expects config.kernelSize to be number or number[] with length 1, 2, or 3, but received ${JSON.stringify(t.kernelSize)}.`)}getConfig(){const t={kernelSize:this.kernelSize,strides:this.strides,padding:this.padding,dataFormat:this.dataFormat,dilationRate:this.dilationRate,activation:Bu(this.activation),useBias:this.useBias,biasInitializer:$o(this.biasInitializer),biasRegularizer:qu(this.biasRegularizer),activityRegularizer:qu(this.activityRegularizer),biasConstraint:Aa(this.biasConstraint)},e=super.getConfig();return Object.assign(t,e),t}}class lh extends oh{constructor(t,e){super(t,e),this.kernel=null,lh.verifyArgs(e),this.filters=e.filters,ya(this.filters,"filters"),this.kernelInitializer=_o(e.kernelInitializer||this.DEFAULT_KERNEL_INITIALIZER),this.kernelConstraint=za(e.kernelConstraint),this.kernelRegularizer=Hu(e.kernelRegularizer)}build(t){t=Ko(t);const e="channelsFirst"===this.dataFormat?1:t.length-1;if(null==t[e])throw new Qr("The channel dimension of the input should be defined. Found "+t[e]);const n=t[e],s=this.kernelSize.concat([n,this.filters]);this.kernel=this.addWeight("kernel",s,null,this.kernelInitializer,this.kernelRegularizer,!0,this.kernelConstraint),this.useBias&&(this.bias=this.addWeight("bias",[this.filters],null,this.biasInitializer,this.biasRegularizer,!0,this.biasConstraint)),this.inputSpec=[{ndim:this.rank+2,axes:{[e]:n}}],this.built=!0}call(t,e){return s(()=>{let e;t=Uo(t);const n=null==this.bias?null:this.bias.read(),i=ba(this.activation.getClassName());if(null!=i&&2===this.rank)e=ah(t,this.kernel.read(),n,this.strides,this.padding,this.dataFormat,this.dilationRate,i);else{if(1===this.rank)e=function(t,e,n,i=1,r="valid",a,o=1){return s(()=>{if(null==a&&(a="channelsLast"),Ra(a),3!==t.shape.length)throw new Qr("The input of a conv1dWithBias operation should be 3, but is "+t.shape.length+" instead.");if(3!==e.shape.length)throw new Qr("The kernel for a conv1dWithBias operation should be 3, but is "+e.shape.length+" instead");if(null!=n&&1!==n.shape.length)throw new Qr("The bias for a conv1dWithBias operation should be 1, but is "+e.shape.length+" instead");if("channelsFirst"===a&&(t=yt(t,[0,2,1])),"causal"===r)throw new ta("The support for CAUSAL padding mode in conv1dWithBias is not implemented yet.");let s=wt(t,e,i,"same"===r?"same":"valid","NWC",o);return null!=n&&(s=uo(s,n)),s})}(t,this.kernel.read(),n,this.strides[0],this.padding,this.dataFormat,this.dilationRate[0]);else if(2===this.rank)e=ah(t,this.kernel.read(),n,this.strides,this.padding,this.dataFormat,this.dilationRate);else{if(3!==this.rank)throw new ta("convolutions greater than 3D are not implemented yet.");e=function(t,e,n,i=[1,1,1],r="valid",a,o){return s(()=>{if(null==a&&(a="channelsLast"),Ra(a),4!==t.rank&&5!==t.rank)throw new Qr("conv3dWithBias expects input to be of rank 4 or 5, but received "+t.rank+".");if(4!==e.rank&&5!==e.rank)throw new Qr("conv3dWithBias expects kernel to be of rank 4 or 5, but received "+t.rank+".");let s=rh(t,a);if("causal"===r)throw new ta("The support for CAUSAL padding mode in conv3dWithBias is not implemented yet.");return s=kt(s,e,i,"same"===r?"same":"valid","NDHWC",o),null!=n&&(s=uo(s,n)),"channelsFirst"===a&&(s=yt(s,[0,4,1,2,3])),s})}(t,this.kernel.read(),n,this.strides,this.padding,this.dataFormat,this.dilationRate)}null!=this.activation&&(e=this.activation.apply(e))}return e})}computeOutputShape(t){t=Ko(t);const e=[],n="channelsLast"===this.dataFormat?t.slice(1,t.length-1):t.slice(2);for(let t=0;t<n.length;++t){const s=nh(n[t],this.kernelSize[t],this.padding,this.strides[t],"number"==typeof this.dilationRate?this.dilationRate:this.dilationRate[t]);e.push(s)}let s=[t[0]];return"channelsLast"===this.dataFormat?(s=s.concat(e),s.push(this.filters)):(s.push(this.filters),s=s.concat(e)),s}getConfig(){const t={filters:this.filters,kernelInitializer:$o(this.kernelInitializer),kernelRegularizer:qu(this.kernelRegularizer),kernelConstraint:Aa(this.kernelConstraint)},e=super.getConfig();return Object.assign(t,e),t}static verifyArgs(t){if(!("filters"in t)||"number"!=typeof t.filters||t.filters<1)throw new Qr("Convolution layer expected config.filters to be a 'number' > 0 but got "+JSON.stringify(t.filters))}}class uh extends lh{constructor(t){super(2,t),uh.verifyArgs(t)}getConfig(){const t=super.getConfig();return delete t.rank,t}static verifyArgs(t){if("number"!=typeof t.kernelSize&&!ma(t.kernelSize,"number",1,2))throw new Qr(`Conv2D expects config.kernelSize to be number or number[] with length 1 or 2, but received ${JSON.stringify(t.kernelSize)}.`)}}uh.className="Conv2D",n.registerClass(uh);class hh extends lh{constructor(t){super(3,t),hh.verifyArgs(t)}getConfig(){const t=super.getConfig();return delete t.rank,t}static verifyArgs(t){if("number"!=typeof t.kernelSize&&(!Array.isArray(t.kernelSize)||1!==t.kernelSize.length&&3!==t.kernelSize.length))throw new Qr(`Conv3D expects config.kernelSize to be number or [number, number, number], but received ${JSON.stringify(t.kernelSize)}.`)}}hh.className="Conv3D",n.registerClass(hh);class ch extends uh{constructor(t){if(super(t),this.inputSpec=[new Ho({ndim:4})],"same"!==this.padding&&"valid"!==this.padding)throw new Qr("Conv2DTranspose currently supports only padding modes 'same' and 'valid', but received padding mode "+this.padding)}build(t){if(4!==(t=Ko(t)).length)throw new Qr("Input should have rank 4; Received input shape: "+JSON.stringify(t));const e="channelsFirst"===this.dataFormat?1:t.length-1;if(null==t[e])throw new Qr("The channel dimension of the inputs should be defined. Found `None`.");const n=t[e],s=this.kernelSize.concat([this.filters,n]);this.kernel=this.addWeight("kernel",s,"float32",this.kernelInitializer,this.kernelRegularizer,!0,this.kernelConstraint),this.useBias&&(this.bias=this.addWeight("bias",[this.filters],"float32",this.biasInitializer,this.biasRegularizer,!0,this.biasConstraint)),this.inputSpec=[new Ho({ndim:4,axes:{[e]:n}})],this.built=!0}call(t,e){return s(()=>{let e=Uo(t);if(4!==e.shape.length)throw new Qr("Conv2DTranspose.call() expects input tensor to be rank-4, but received a tensor of rank-"+e.shape.length);const n=e.shape,s=n[0];let i,r;"channelsFirst"===this.dataFormat?(i=2,r=3):(i=1,r=2);const a=n[i],o=n[r],l=this.kernelSize[0],u=this.kernelSize[1],h=this.strides[0],c=this.strides[1],p=[s,sh(a,h,l,this.padding),sh(o,c,u,this.padding),this.filters];"channelsLast"!==this.dataFormat&&(e=yt(e,[0,2,3,1]));let d=bt(e,this.kernel.read(),p,this.strides,this.padding);return"channelsLast"!==this.dataFormat&&(d=yt(d,[0,3,1,2])),null!=this.bias&&(d=uo(d,this.bias.read(),this.dataFormat)),null!=this.activation&&(d=this.activation.apply(d)),d})}computeOutputShape(t){const e=(t=Ko(t)).slice();let n,s,i;"channelsFirst"===this.dataFormat?(n=1,s=2,i=3):(n=3,s=1,i=2);const r=this.kernelSize[0],a=this.kernelSize[1],o=this.strides[0],l=this.strides[1];return e[n]=this.filters,e[s]=sh(e[s],o,r,this.padding),e[i]=sh(e[i],l,a,this.padding),e}getConfig(){const t=super.getConfig();return delete t.dilationRate,t}}ch.className="Conv2DTranspose",n.registerClass(ch);class ph extends lh{constructor(t,e){if(super(t,e),this.DEFAULT_DEPTHWISE_INITIALIZER="glorotUniform",this.DEFAULT_POINTWISE_INITIALIZER="glorotUniform",this.depthwiseKernel=null,this.pointwiseKernel=null,null==e.filters)throw new Qr("The `filters` configuration field is required by SeparableConv, but is unspecified.");if(null!=e.kernelInitializer||null!=e.kernelRegularizer||null!=e.kernelConstraint)throw new Qr("Fields kernelInitializer, kernelRegularizer and kernelConstraint are invalid for SeparableConv2D. Use depthwiseInitializer, depthwiseRegularizer, depthwiseConstraint, pointwiseInitializer, pointwiseRegularizer and pointwiseConstraint instead.");if(null!=e.padding&&"same"!==e.padding&&"valid"!==e.padding)throw new Qr(`SeparableConv${this.rank}D supports only padding modes: 'same' and 'valid', but received `+JSON.stringify(e.padding));this.depthMultiplier=null==e.depthMultiplier?1:e.depthMultiplier,this.depthwiseInitializer=_o(e.depthwiseInitializer||this.DEFAULT_DEPTHWISE_INITIALIZER),this.depthwiseRegularizer=Hu(e.depthwiseRegularizer),this.depthwiseConstraint=za(e.depthwiseConstraint),this.pointwiseInitializer=_o(e.depthwiseInitializer||this.DEFAULT_POINTWISE_INITIALIZER),this.pointwiseRegularizer=Hu(e.pointwiseRegularizer),this.pointwiseConstraint=za(e.pointwiseConstraint)}build(t){if((t=Ko(t)).length<this.rank+2)throw new Qr(`Inputs to SeparableConv${this.rank}D should have rank `+(this.rank+2)+", but received input shape: "+JSON.stringify(t));const e="channelsFirst"===this.dataFormat?1:t.length-1;if(null==t[e]||t[e]<0)throw new Qr("The channel dimension of the inputs should be defined, but found "+JSON.stringify(t[e]));const n=t[e],s=this.kernelSize.concat([n,this.depthMultiplier]),i=[];for(let t=0;t<this.rank;++t)i.push(1);i.push(n*this.depthMultiplier,this.filters);this.depthwiseKernel=this.addWeight("depthwise_kernel",s,"float32",this.depthwiseInitializer,this.depthwiseRegularizer,!0,this.depthwiseConstraint),this.pointwiseKernel=this.addWeight("pointwise_kernel",i,"float32",this.pointwiseInitializer,this.pointwiseRegularizer,!0,this.pointwiseConstraint),this.useBias?this.bias=this.addWeight("bias",[this.filters],"float32",this.biasInitializer,this.biasRegularizer,!0,this.biasConstraint):this.bias=null,this.inputSpec=[new Ho({ndim:this.rank+2,axes:{[e]:n}})],this.built=!0}call(t,e){return s(()=>{let e;if(t=Uo(t),1===this.rank)throw new ta("1D separable convolution is not implemented yet.");return 2===this.rank&&("channelsFirst"===this.dataFormat&&(t=yt(t,[0,2,3,1])),e=xt(t,this.depthwiseKernel.read(),this.pointwiseKernel.read(),this.strides,this.padding,this.dilationRate,"NHWC")),this.useBias&&(e=uo(e,this.bias.read(),this.dataFormat)),null!=this.activation&&(e=this.activation.apply(e)),"channelsFirst"===this.dataFormat&&(e=yt(e,[0,3,1,2])),e})}getConfig(){const t=super.getConfig();return delete t.rank,delete t.kernelInitializer,delete t.kernelRegularizer,delete t.kernelConstraint,t.depthwiseInitializer=$o(this.depthwiseInitializer),t.pointwiseInitializer=$o(this.pointwiseInitializer),t.depthwiseRegularizer=qu(this.depthwiseRegularizer),t.pointwiseRegularizer=qu(this.pointwiseRegularizer),t.depthwiseConstraint=Aa(this.depthwiseConstraint),t.pointwiseConstraint=Aa(this.pointwiseConstraint),t}}ph.className="SeparableConv";class dh extends ph{constructor(t){super(2,t)}}dh.className="SeparableConv2D",n.registerClass(dh);class fh extends lh{constructor(t){super(1,t),fh.verifyArgs(t),this.inputSpec=[{ndim:3}]}getConfig(){const t=super.getConfig();return delete t.rank,delete t.dataFormat,t}static verifyArgs(t){if("number"!=typeof t.kernelSize&&!ma(t.kernelSize,"number",1,1))throw new Qr(`Conv1D expects config.kernelSize to be number or number[] with length 1, but received ${JSON.stringify(t.kernelSize)}.`)}}fh.className="Conv1D",n.registerClass(fh);class gh extends Qo{constructor(t){super(t),"number"==typeof t.cropping?this.cropping=[[t.cropping,t.cropping],[t.cropping,t.cropping]]:"number"==typeof t.cropping[0]?this.cropping=[[t.cropping[0],t.cropping[0]],[t.cropping[1],t.cropping[1]]]:this.cropping=t.cropping,this.dataFormat=void 0===t.dataFormat?"channelsLast":t.dataFormat,this.inputSpec=[{ndim:4}]}computeOutputShape(t){return"channelsFirst"===this.dataFormat?[t[0],t[1],t[2]-this.cropping[0][0]-this.cropping[0][1],t[3]-this.cropping[1][0]-this.cropping[1][1]]:[t[0],t[1]-this.cropping[0][0]-this.cropping[0][1],t[2]-this.cropping[1][0]-this.cropping[1][1],t[3]]}call(t,e){return s(()=>{if(t=Uo(t),"channelsLast"===this.dataFormat){const e=to(t,this.cropping[0][0],t.shape[1]-this.cropping[0][0]-this.cropping[0][1],2);return to(e,this.cropping[1][0],t.shape[2]-this.cropping[1][1]-this.cropping[1][0],3)}{const e=to(t,this.cropping[0][0],t.shape[2]-this.cropping[0][0]-this.cropping[0][1],3);return to(e,this.cropping[1][0],t.shape[3]-this.cropping[1][1]-this.cropping[1][0],4)}})}getConfig(){const t={cropping:this.cropping,dataFormat:this.dataFormat},e=super.getConfig();return Object.assign(t,e),t}}gh.className="Cropping2D",n.registerClass(gh);class mh extends Qo{constructor(t){var e;super(t),this.DEFAULT_SIZE=[2,2],this.inputSpec=[{ndim:4}],this.size=null==t.size?this.DEFAULT_SIZE:t.size,this.dataFormat=null==t.dataFormat?"channelsLast":t.dataFormat,Ra(this.dataFormat),this.interpolation=null==t.interpolation?"nearest":t.interpolation,e=this.interpolation,ga(Ea,"InterpolationFormat",e)}computeOutputShape(t){if("channelsFirst"===this.dataFormat){const e=null==t[2]?null:this.size[0]*t[2],n=null==t[3]?null:this.size[1]*t[3];return[t[0],t[1],e,n]}{const e=null==t[1]?null:this.size[0]*t[1],n=null==t[2]?null:this.size[1]*t[2];return[t[0],e,n,t[3]]}}call(t,e){return s(()=>{let e=Uo(t);const n=e.shape;if("channelsFirst"===this.dataFormat){e=yt(e,[0,2,3,1]);const t=this.size[0]*n[2],s=this.size[1]*n[3],i="nearest"===this.interpolation?e.resizeNearestNeighbor([t,s]):e.resizeBilinear([t,s]);return yt(i,[0,3,1,2])}{const t=this.size[0]*n[1],s=this.size[1]*n[2];return"nearest"===this.interpolation?e.resizeNearestNeighbor([t,s]):e.resizeBilinear([t,s])}})}getConfig(){const t={size:this.size,dataFormat:this.dataFormat},e=super.getConfig();return Object.assign(t,e),t}}mh.className="UpSampling2D",n.registerClass(mh);class yh extends oh{constructor(t){super(2,t),this.depthwiseKernel=null,this.depthMultiplier=null==t.depthMultiplier?1:t.depthMultiplier,this.depthwiseInitializer=_o(t.depthwiseInitializer||this.DEFAULT_KERNEL_INITIALIZER),this.depthwiseConstraint=za(t.depthwiseConstraint),this.depthwiseRegularizer=Hu(t.depthwiseRegularizer)}build(t){if((t=Ko(t)).length<4)throw new Qr(`Inputs to DepthwiseConv2D should have rank 4. Received input shape: ${JSON.stringify(t)}.`);const e="channelsFirst"===this.dataFormat?1:3;if(null==t[e]||t[e]<0)throw new Qr(`The channel dimension of the inputs to DepthwiseConv2D should be defined, but is not (${t[e]}).`);const n=t[e],s=[this.kernelSize[0],this.kernelSize[1],n,this.depthMultiplier];this.depthwiseKernel=this.addWeight("depthwise_kernel",s,null,this.depthwiseInitializer,this.depthwiseRegularizer,!0,this.depthwiseConstraint),this.useBias?this.bias=this.addWeight("bias",[n*this.depthMultiplier],null,this.biasInitializer,this.biasRegularizer,!0,this.biasConstraint):this.bias=null,this.built=!0}call(t,e){return s(()=>{let e=function(t,e,n=[1,1],i="valid",r,a){return s(()=>{null==r&&(r="channelsLast"),Ra(r);let s=ih(t,r);if(4!==t.rank)throw new Qr("Input for depthwiseConv2d is required to be 4-D, but is instead "+t.rank+"-D");if(4!==e.rank)throw new Qr("depthwiseKernel is required to be 4-D, but is instead "+e.rank+"-D");return s=vt(s,e,n,"same"===i?"same":"valid","NHWC",a),"channelsFirst"===r&&(s=yt(s,[0,3,1,2])),s})}(t=Uo(t),this.depthwiseKernel.read(),this.strides,this.padding,this.dataFormat,null);return this.useBias&&(e=uo(e,this.bias.read(),this.dataFormat)),null!=this.activation&&(e=this.activation.apply(e)),e})}computeOutputShape(t){t=Ko(t);const e="channelsFirst"===this.dataFormat?t[2]:t[1],n="channelsFirst"===this.dataFormat?t[3]:t[2],s="channelsFirst"===this.dataFormat?t[1]*this.depthMultiplier:t[3]*this.depthMultiplier,i=nh(e,this.kernelSize[0],this.padding,this.strides[0]),r=nh(n,this.kernelSize[1],this.padding,this.strides[1]);return"channelsFirst"===this.dataFormat?[t[0],s,i,r]:[t[0],i,r,s]}getConfig(){const t=super.getConfig();return t.depthMultiplier=this.depthMultiplier,t.depthwiseInitializer=$o(this.depthwiseInitializer),t.depthwiseRegularizer=qu(this.depthwiseRegularizer),t.depthwiseConstraint=Aa(this.depthwiseRegularizer),t}}function bh(t,e,n,s){if(Array.isArray(t)){if(null!=e||null!=n)throw new Qr("When inputs is an array, neither initialState or constants should be provided");null!=s&&(n=t.slice(t.length-s,t.length),t=t.slice(0,t.length-s)),t.length>1&&(e=t.slice(1,t.length)),t=t[0]}function i(t){return null==t||Array.isArray(t)?t:[t]}return{inputs:t,initialState:e=i(e),constants:n=i(n)}}function wh(t,e,n,i=!1,r,a,o=!1,l=!1){return s(()=>{const u=e.shape.length;if(u<3)throw new Qr(`Input should be at least 3D, but is ${u}D.`);const h=[1,0].concat(Ja(2,u));if(e=yt(e,h),null!=a)throw new ta("The rnn() functoin of the deeplearn.js backend does not support constants yet.");o&&console.warn("Backend rnn(): the unroll = true option is not applicable to the imperative deeplearn.js backend."),null!=r&&((r=r.asType("bool").asType("float32")).rank===u-1&&(r=St(r,-1)),r=yt(r,h)),i&&(e=It(e,0),null!=r&&(r=It(r,0)));const c=[];let p,d=n;const f=e.shape[0],g=Nt(e);let m,y;null!=r&&(m=Nt(r));for(let e=0;e<f;++e){const n=g[e],i=s(()=>t(n,d));if(null==r)p=i[0],d=i[1];else{const t=s(()=>{const t=m[e],n=Y(t).sub(t);return{output:i[0].mul(t).add(d[0].mul(n)),newStates:d.map((e,s)=>i[1][s].mul(t).add(e.mul(n)))}});p=t.output,d=t.newStates}l&&c.push(p)}if(l){y=At(c,1)}return[p,y,d]})}yh.className="DepthwiseConv2D",n.registerClass(yh);class kh extends Qo{constructor(t){let e;if(super(t),null==t.cell)throw new Qr("cell property is missing for the constructor of RNN.");if(e=Array.isArray(t.cell)?new zh({cells:t.cell}):t.cell,null==e.stateSize)throw new Qr("The RNN cell should have an attribute `stateSize` (tuple of integers, one integer per RNN state).");this.cell=e,this.returnSequences=null!=t.returnSequences&&t.returnSequences,this.returnState=null!=t.returnState&&t.returnState,this.goBackwards=null!=t.goBackwards&&t.goBackwards,this._stateful=null!=t.stateful&&t.stateful,this.unroll=null!=t.unroll&&t.unroll,this.supportsMasking=!0,this.inputSpec=[new Ho({ndim:3})],this.stateSpec=null,this.states_=null,this.numConstants=null,this.keptStates=[]}getStates(){if(null==this.states_){return Ja(0,Array.isArray(this.cell.stateSize)?this.cell.stateSize.length:1).map(t=>null)}return this.states_}setStates(t){this.states_=t}computeOutputShape(t){Po(t)&&(t=t[0]),t=t;let e=this.cell.stateSize;Array.isArray(e)||(e=[e]);const n=e[0];let s;if(s=this.returnSequences?[t[0],t[1],n]:[t[0],n],this.returnState){const n=[];for(const s of e)n.push([t[0],s]);return[s].concat(n)}return s}computeMask(t,e){return s(()=>{Array.isArray(e)&&(e=e[0]);const t=this.returnSequences?e:null;if(this.returnState){const e=this.states.map(t=>null);return[t].concat(e)}return t})}get states(){if(null==this.states_){const t=Array.isArray(this.cell.stateSize)?this.cell.stateSize.length:1,e=[];for(let n=0;n<t;++n)e.push(null);return e}return this.states_}set states(t){this.states_=t}build(t){if(null!=this.numConstants)throw new ta("Constants support is not implemented in RNN yet.");Po(t)&&(t=t[0]),t=t;const n=this.stateful?t[0]:null,s=t.slice(2);this.inputSpec[0]=new Ho({shape:[n,null,...s]});const i=[t[0]].concat(t.slice(2));let r;if(this.cell.build(i),r=Array.isArray(this.cell.stateSize)?this.cell.stateSize:[this.cell.stateSize],null!=this.stateSpec){if(!e.arraysEqual(this.stateSpec.map(t=>t.shape[t.shape.length-1]),r))throw new Qr(`An initialState was passed that is not compatible with cell.stateSize. Received stateSpec=${this.stateSpec}; However cell.stateSize is `+this.cell.stateSize)}else this.stateSpec=r.map(t=>new Ho({shape:[null,t]}));this.stateful&&this.resetStates()}resetStates(t,n=!1){s(()=>{if(!this.stateful)throw new Xr("Cannot call resetStates() on an RNN Layer that is not stateful.");const s=this.inputSpec[0].shape[0];if(null==s)throw new Qr("If an RNN is stateful, it needs to know its batch size. Specify the batch size of your input tensors: \n- If using a Sequential model, specify the batch size by passing a `batchInputShape` option to your first layer.\n- If using the functional API, specify the batch size by passing a `batchShape` option to your Input layer.");if(null==this.states_)Array.isArray(this.cell.stateSize)?this.states_=this.cell.stateSize.map(t=>E([s,t])):this.states_=[E([s,this.cell.stateSize])];else if(null==t)B(this.states_),null!=this.keptStates&&(B(this.keptStates),this.keptStates=[]),Array.isArray(this.cell.stateSize)?this.states_=this.cell.stateSize.map(t=>E([s,t])):this.states_[0]=E([s,this.cell.stateSize]);else{if(Array.isArray(t)||(t=[t]),t.length!==this.states_.length)throw new Qr(`Layer ${this.name} expects ${this.states_.length} state(s), but it received ${t.length} state value(s). Input received: `+t);!0===n?this.keptStates.push(this.states_.slice()):B(this.states_);for(let n=0;n<this.states_.length;++n){const i=t[n],r=Array.isArray(this.cell.stateSize)?this.cell.stateSize[n]:this.cell.stateSize,a=[s,r];if(!e.arraysEqual(i.shape,a))throw new Qr(`State ${n} is incompatible with layer ${this.name}: expected shape=${a}, received shape=${i.shape}`);this.states_[n]=i}}this.states_=this.states_.map(t=>W(t.clone()))})}apply(t,e){let n=null==e?null:e.initialState,s=null==e?null:e.constants;null==e&&(e={});const i=bh(t,n,s,this.numConstants);t=i.inputs,n=i.initialState,s=i.constants;let r=[],a=[];if(null!=n){e.initialState=n,r=r.concat(n),this.stateSpec=[];for(const t of n)this.stateSpec.push(new Ho({shape:t.shape}));a=a.concat(this.stateSpec)}null!=s&&(e.constants=s,r=r.concat(s),this.numConstants=s.length);if(r[0]instanceof Jo){const n=[t].concat(r),s=this.inputSpec.concat(a),i=this.inputSpec;this.inputSpec=s;const o=super.apply(n,e);return this.inputSpec=i,o}return super.apply(t,e)}call(t,e){return s(()=>{const n=null==e?null:e.mask,s=null==e?null:e.training;let i=null==e?null:e.initialState;t=Uo(t),null==i&&(i=this.stateful?this.states_:this.getInitialState(t));const r=Array.isArray(this.cell.stateSize)?this.cell.stateSize.length:1;if(i.length!==r)throw new Qr(`RNN Layer has ${r} state(s) but was passed `+i.length+" initial state(s).");this.unroll&&console.warn("Ignoring unroll = true for RNN layer, due to imperative backend.");const a={training:s},o=wh((t,e)=>{const n=this.cell.call([t].concat(e),a);return[n[0],n.slice(1)]},t,i,this.goBackwards,n,null,this.unroll,this.returnSequences),l=o[0],u=o[1],h=o[2];this.stateful&&this.resetStates(h,s);const c=this.returnSequences?u:l;return this.returnState?[c].concat(h):c})}getInitialState(t){return s(()=>{let e=E(t.shape);return e=r(e,[1,2]),e=Xa(e),Array.isArray(this.cell.stateSize)?this.cell.stateSize.map(t=>t>1?so(e,[1,t]):e):this.cell.stateSize>1?[so(e,[1,this.cell.stateSize])]:[e]})}get trainableWeights(){return this.trainable?this.cell.trainableWeights:[]}get nonTrainableWeights(){return this.trainable?this.cell.nonTrainableWeights:this.cell.weights}setFastWeightInitDuringBuild(t){super.setFastWeightInitDuringBuild(t),null!=this.cell&&this.cell.setFastWeightInitDuringBuild(t)}getConfig(){const t=super.getConfig(),e={returnSequences:this.returnSequences,returnState:this.returnState,goBackwards:this.goBackwards,stateful:this.stateful,unroll:this.unroll};null!=this.numConstants&&(e.numConstants=this.numConstants);const n=this.cell.getConfig();return this.getClassName()===kh.className&&(e.cell={className:this.cell.getClassName(),config:n}),Object.assign({},n,t,e)}static fromConfig(t,e,n={}){const s=dl(e.cell,n);return new t(Object.assign(e,{cell:s}))}}kh.className="RNN",n.registerClass(kh);class xh extends Qo{}class vh extends xh{constructor(t){super(t),this.DEFAULT_ACTIVATION="tanh",this.DEFAULT_KERNEL_INITIALIZER="glorotNormal",this.DEFAULT_RECURRENT_INITIALIZER="orthogonal",this.DEFAULT_BIAS_INITIALIZER="zeros",this.units=t.units,ya(this.units,"units"),this.activation=Wu(null==t.activation?this.DEFAULT_ACTIVATION:t.activation),this.useBias=null==t.useBias||t.useBias,this.kernelInitializer=_o(t.kernelInitializer||this.DEFAULT_KERNEL_INITIALIZER),this.recurrentInitializer=_o(t.recurrentInitializer||this.DEFAULT_RECURRENT_INITIALIZER),this.biasInitializer=_o(t.biasInitializer||this.DEFAULT_BIAS_INITIALIZER),this.kernelRegularizer=Hu(t.kernelRegularizer),this.recurrentRegularizer=Hu(t.recurrentRegularizer),this.biasRegularizer=Hu(t.biasRegularizer),this.kernelConstraint=za(t.kernelConstraint),this.recurrentConstraint=za(t.recurrentConstraint),this.biasConstraint=za(t.biasConstraint),this.dropout=Ga([1,Ha([0,null==t.dropout?0:t.dropout])]),this.recurrentDropout=Ga([1,Ha([0,null==t.recurrentDropout?0:t.recurrentDropout])]),this.stateSize=this.units,this.dropoutMask=null,this.recurrentDropoutMask=null}build(t){t=Ko(t),this.kernel=this.addWeight("kernel",[t[t.length-1],this.units],null,this.kernelInitializer,this.kernelRegularizer,!0,this.kernelConstraint),this.recurrentKernel=this.addWeight("recurrent_kernel",[this.units,this.units],null,this.recurrentInitializer,this.recurrentRegularizer,!0,this.recurrentConstraint),this.useBias?this.bias=this.addWeight("bias",[this.units],null,this.biasInitializer,this.biasRegularizer,!0,this.biasConstraint):this.bias=null,this.built=!0}call(t,e){return s(()=>{if(2!==(t=t).length)throw new Qr(`SimpleRNNCell expects 2 input Tensors, got ${t.length}.`);let n=t[1];t=t[0];const s=null!=e.training&&e.training;let i;0<this.dropout&&this.dropout<1&&null==this.dropoutMask&&(this.dropoutMask=Dh({ones:()=>Y(t),rate:this.dropout,training:s})),0<this.recurrentDropout&&this.recurrentDropout<1&&null==this.recurrentDropoutMask&&(this.recurrentDropoutMask=Dh({ones:()=>Y(n),rate:this.recurrentDropout,training:s}));const r=this.dropoutMask,o=this.recurrentDropoutMask;i=ro(null!=r?a(t,r):t,this.kernel.read()),null!=this.bias&&(i=uo(i,this.bias.read())),null!=o&&(n=a(n,o));let l=u(i,ro(n,this.recurrentKernel.read()));return null!=this.activation&&(l=this.activation.apply(l)),[l,l]})}getConfig(){const t=super.getConfig(),e={units:this.units,activation:Bu(this.activation),useBias:this.useBias,kernelInitializer:$o(this.kernelInitializer),recurrentInitializer:$o(this.recurrentInitializer),biasInitializer:$o(this.biasInitializer),kernelRegularizer:qu(this.kernelRegularizer),recurrentRegularizer:qu(this.recurrentRegularizer),biasRegularizer:qu(this.biasRegularizer),activityRegularizer:qu(this.activityRegularizer),kernelConstraint:Aa(this.kernelConstraint),recurrentConstraint:Aa(this.recurrentConstraint),biasConstraint:Aa(this.biasConstraint),dropout:this.dropout,recurrentDropout:this.recurrentDropout};return Object.assign({},t,e)}}vh.className="SimpleRNNCell",n.registerClass(vh);class Sh extends kh{constructor(t){t.cell=new vh(t),super(t)}call(t,e){return s(()=>{null!=this.cell.dropoutMask&&(B(this.cell.dropoutMask),this.cell.dropoutMask=null),null!=this.cell.recurrentDropoutMask&&(B(this.cell.recurrentDropoutMask),this.cell.recurrentDropoutMask=null);const n=null==e?null:e.mask,s=null==e?null:e.training,i=null==e?null:e.initialState;return super.call(t,{mask:n,training:s,initialState:i})})}static fromConfig(t,e){return new t(e)}}Sh.className="SimpleRNN",n.registerClass(Sh);class Ih extends xh{constructor(t){if(super(t),this.DEFAULT_ACTIVATION="tanh",this.DEFAULT_RECURRENT_ACTIVATION="hardSigmoid",this.DEFAULT_KERNEL_INITIALIZER="glorotNormal",this.DEFAULT_RECURRENT_INITIALIZER="orthogonal",this.DEFAULT_BIAS_INITIALIZER="zeros",t.resetAfter)throw new Qr("GRUCell does not support reset_after parameter set to true.");this.units=t.units,ya(this.units,"units"),this.activation=Wu(void 0===t.activation?this.DEFAULT_ACTIVATION:t.activation),this.recurrentActivation=Wu(void 0===t.recurrentActivation?this.DEFAULT_RECURRENT_ACTIVATION:t.recurrentActivation),this.useBias=null==t.useBias||t.useBias,this.kernelInitializer=_o(t.kernelInitializer||this.DEFAULT_KERNEL_INITIALIZER),this.recurrentInitializer=_o(t.recurrentInitializer||this.DEFAULT_RECURRENT_INITIALIZER),this.biasInitializer=_o(t.biasInitializer||this.DEFAULT_BIAS_INITIALIZER),this.kernelRegularizer=Hu(t.kernelRegularizer),this.recurrentRegularizer=Hu(t.recurrentRegularizer),this.biasRegularizer=Hu(t.biasRegularizer),this.kernelConstraint=za(t.kernelConstraint),this.recurrentConstraint=za(t.recurrentConstraint),this.biasConstraint=za(t.biasConstraint),this.dropout=Ga([1,Ha([0,null==t.dropout?0:t.dropout])]),this.recurrentDropout=Ga([1,Ha([0,null==t.recurrentDropout?0:t.recurrentDropout])]),this.implementation=t.implementation,this.stateSize=this.units,this.dropoutMask=null,this.recurrentDropoutMask=null}build(t){const e=(t=Ko(t))[t.length-1];this.kernel=this.addWeight("kernel",[e,3*this.units],null,this.kernelInitializer,this.kernelRegularizer,!0,this.kernelConstraint),this.recurrentKernel=this.addWeight("recurrent_kernel",[this.units,3*this.units],null,this.recurrentInitializer,this.recurrentRegularizer,!0,this.recurrentConstraint),this.useBias?this.bias=this.addWeight("bias",[3*this.units],null,this.biasInitializer,this.biasRegularizer,!0,this.biasConstraint):this.bias=null,this.built=!0}call(t,e){return s(()=>{if(2!==(t=t).length)throw new Qr("GRUCell expects 2 input Tensors (inputs, h, c), got "+t.length+".");const n=null!=e.training&&e.training;let s=t[1];t=t[0],0<this.dropout&&this.dropout<1&&null==this.dropoutMask&&(this.dropoutMask=Dh({ones:()=>Y(t),rate:this.dropout,training:n,count:3})),0<this.recurrentDropout&&this.recurrentDropout<1&&null==this.recurrentDropoutMask&&(this.recurrentDropoutMask=Dh({ones:()=>Y(s),rate:this.recurrentDropout,training:n,count:3}));const i=this.dropoutMask,r=this.recurrentDropoutMask;let o,l,h;0<this.dropout&&this.dropout<1&&(t=a(t,i[0]));let c=ro(t,this.kernel.read());this.useBias&&(c=uo(c,this.bias.read())),0<this.recurrentDropout&&this.recurrentDropout<1&&(s=a(s,r[0]));const p=this.recurrentKernel.read(),[d,f]=Ct(p,[2*this.units,this.units],p.rank-1),g=ro(s,d),[m,y,b]=Ct(c,3,c.rank-1),[w,k]=Ct(g,2,g.rank-1);o=this.recurrentActivation.apply(u(m,w)),l=this.recurrentActivation.apply(u(y,k));const x=ro(a(l,s),f);h=this.activation.apply(u(b,x));const v=u(a(o,s),a(u(1,q(o)),h));return[v,v]})}getConfig(){const t=super.getConfig(),e={units:this.units,activation:Bu(this.activation),recurrentActivation:Bu(this.recurrentActivation),useBias:this.useBias,kernelInitializer:$o(this.kernelInitializer),recurrentInitializer:$o(this.recurrentInitializer),biasInitializer:$o(this.biasInitializer),kernelRegularizer:qu(this.kernelRegularizer),recurrentRegularizer:qu(this.recurrentRegularizer),biasRegularizer:qu(this.biasRegularizer),activityRegularizer:qu(this.activityRegularizer),kernelConstraint:Aa(this.kernelConstraint),recurrentConstraint:Aa(this.recurrentConstraint),biasConstraint:Aa(this.biasConstraint),dropout:this.dropout,recurrentDropout:this.recurrentDropout,implementation:this.implementation,resetAfter:!1};return Object.assign({},t,e)}}Ih.className="GRUCell",n.registerClass(Ih);class Nh extends kh{constructor(t){0===t.implementation&&console.warn("`implementation=0` has been deprecated, and now defaults to `implementation=1`. Please update your layer call."),t.cell=new Ih(t),super(t)}call(t,e){return s(()=>{null!=this.cell.dropoutMask&&(B(this.cell.dropoutMask),this.cell.dropoutMask=null),null!=this.cell.recurrentDropoutMask&&(B(this.cell.recurrentDropoutMask),this.cell.recurrentDropoutMask=null);const n=null==e?null:e.mask,s=null==e?null:e.training,i=null==e?null:e.initialState;return super.call(t,{mask:n,training:s,initialState:i})})}static fromConfig(t,e){return 0===e.implmentation&&(e.implementation=1),new t(e)}}Nh.className="GRU",n.registerClass(Nh);class Ah extends xh{constructor(t){super(t),this.DEFAULT_ACTIVATION="tanh",this.DEFAULT_RECURRENT_ACTIVATION="hardSigmoid",this.DEFAULT_KERNEL_INITIALIZER="glorotNormal",this.DEFAULT_RECURRENT_INITIALIZER="orthogonal",this.DEFAULT_BIAS_INITIALIZER="zeros",this.units=t.units,ya(this.units,"units"),this.activation=Wu(void 0===t.activation?this.DEFAULT_ACTIVATION:t.activation),this.recurrentActivation=Wu(void 0===t.recurrentActivation?this.DEFAULT_RECURRENT_ACTIVATION:t.recurrentActivation),this.useBias=null==t.useBias||t.useBias,this.kernelInitializer=_o(t.kernelInitializer||this.DEFAULT_KERNEL_INITIALIZER),this.recurrentInitializer=_o(t.recurrentInitializer||this.DEFAULT_RECURRENT_INITIALIZER),this.biasInitializer=_o(t.biasInitializer||this.DEFAULT_BIAS_INITIALIZER),this.unitForgetBias=t.unitForgetBias,this.kernelRegularizer=Hu(t.kernelRegularizer),this.recurrentRegularizer=Hu(t.recurrentRegularizer),this.biasRegularizer=Hu(t.biasRegularizer),this.kernelConstraint=za(t.kernelConstraint),this.recurrentConstraint=za(t.recurrentConstraint),this.biasConstraint=za(t.biasConstraint),this.dropout=Ga([1,Ha([0,null==t.dropout?0:t.dropout])]),this.recurrentDropout=Ga([1,Ha([0,null==t.recurrentDropout?0:t.recurrentDropout])]),this.implementation=t.implementation,this.stateSize=[this.units,this.units],this.dropoutMask=null,this.recurrentDropoutMask=null}build(t){var e;const n=(t=Ko(t))[t.length-1];let s;if(this.kernel=this.addWeight("kernel",[n,4*this.units],null,this.kernelInitializer,this.kernelRegularizer,!0,this.kernelConstraint),this.recurrentKernel=this.addWeight("recurrent_kernel",[this.units,4*this.units],null,this.recurrentInitializer,this.recurrentRegularizer,!0,this.recurrentConstraint),this.useBias){if(this.unitForgetBias){const t=this.biasInitializer,n=this.units;s=new((e=class extends go{apply(e,s){const i=t.apply([n]),r=(new yo).apply([n]),a=t.apply([2*n]);return no(no(i,r),a)}}).className="CustomInit",e)}else s=this.biasInitializer;this.bias=this.addWeight("bias",[4*this.units],null,s,this.biasRegularizer,!0,this.biasConstraint)}else this.bias=null;this.built=!0}call(t,e){return s(()=>{const n=null!=e.training&&e.training;if(3!==(t=t).length)throw new Qr("LSTMCell expects 3 input Tensors (inputs, h, c), got "+t.length+".");let s=t[1];const i=t[2];t=t[0],0<this.dropout&&this.dropout<1&&null==this.dropoutMask&&(this.dropoutMask=Dh({ones:()=>Y(t),rate:this.dropout,training:n,count:4})),0<this.recurrentDropout&&this.recurrentDropout<1&&null==this.recurrentDropoutMask&&(this.recurrentDropoutMask=Dh({ones:()=>Y(s),rate:this.recurrentDropout,training:n,count:4}));const r=this.dropoutMask,o=this.recurrentDropoutMask;let l,h,c,p;0<this.dropout&&this.dropout<1&&(t=a(t,r[0]));let d=ro(t,this.kernel.read());0<this.recurrentDropout&&this.recurrentDropout<1&&(s=a(s,o[0])),d=u(d,ro(s,this.recurrentKernel.read())),this.useBias&&(d=uo(d,this.bias.read()));const[f,g,m,y]=Ct(d,4,d.rank-1);l=this.recurrentActivation.apply(f),h=this.recurrentActivation.apply(g),c=u(a(h,i),a(l,this.activation.apply(m))),p=this.recurrentActivation.apply(y);const b=a(p,this.activation.apply(c));return[b,b,c]})}getConfig(){const t=super.getConfig(),e={units:this.units,activation:Bu(this.activation),recurrentActivation:Bu(this.recurrentActivation),useBias:this.useBias,kernelInitializer:$o(this.kernelInitializer),recurrentInitializer:$o(this.recurrentInitializer),biasInitializer:$o(this.biasInitializer),unitForgetBias:this.unitForgetBias,kernelRegularizer:qu(this.kernelRegularizer),recurrentRegularizer:qu(this.recurrentRegularizer),biasRegularizer:qu(this.biasRegularizer),activityRegularizer:qu(this.activityRegularizer),kernelConstraint:Aa(this.kernelConstraint),recurrentConstraint:Aa(this.recurrentConstraint),biasConstraint:Aa(this.biasConstraint),dropout:this.dropout,recurrentDropout:this.recurrentDropout,implementation:this.implementation};return Object.assign({},t,e)}}Ah.className="LSTMCell",n.registerClass(Ah);class Ch extends kh{constructor(t){0===t.implementation&&console.warn("`implementation=0` has been deprecated, and now defaults to `implementation=1`. Please update your layer call."),t.cell=new Ah(t),super(t)}call(t,e){return s(()=>{null!=this.cell.dropoutMask&&(B(this.cell.dropoutMask),this.cell.dropoutMask=null),null!=this.cell.recurrentDropoutMask&&(B(this.cell.recurrentDropoutMask),this.cell.recurrentDropoutMask=null);const n=null==e?null:e.mask,s=null==e?null:e.training,i=null==e?null:e.initialState;return super.call(t,{mask:n,training:s,initialState:i})})}static fromConfig(t,e){return 0===e.implmentation&&(e.implementation=1),new t(e)}}Ch.className="LSTM",n.registerClass(Ch);class zh extends xh{constructor(t){super(t),this.cells=t.cells}get stateSize(){const t=[];for(const e of this.cells.slice().reverse())Array.isArray(e.stateSize)?t.push(...e.stateSize):t.push(e.stateSize);return t}call(t,e){return s(()=>{let n=(t=t).slice(1);const s=[];for(const t of this.cells.slice().reverse())Array.isArray(t.stateSize)?s.push(n.splice(0,t.stateSize.length)):s.push(n.splice(0,1));s.reverse();const i=[];let r;for(let a=0;a<this.cells.length;++a){const o=this.cells[a];n=s[a],r=0===a?[t[0]].concat(n):[r[0]].concat(n),r=o.call(r,e),i.push(r.slice(1))}n=[];for(const t of i.slice().reverse())n.push(...t);return[r[0]].concat(n)})}build(t){let e;Po(t)&&(t=t[0]),t=t,this.cells.forEach((n,s)=>{Pa("RNNCell_"+s,()=>{n.build(t),e=Array.isArray(n.stateSize)?n.stateSize[0]:n.stateSize,t=[t[0],e]})}),this.built=!0}getConfig(){const t=super.getConfig(),e={cells:this.cells.map(t=>({className:t.getClassName(),config:t.getConfig()}))};return Object.assign({},t,e)}static fromConfig(t,e,n={}){const s=[];for(const t of e.cells)s.push(dl(t,n));return new t({cells:s})}get trainableWeights(){if(!this.trainable)return[];const t=[];for(const e of this.cells)t.push(...e.trainableWeights);return t}get nonTrainableWeights(){const t=[];for(const e of this.cells)t.push(...e.nonTrainableWeights);if(!this.trainable){const e=[];for(const t of this.cells)e.push(...t.trainableWeights);return e.concat(t)}return t}getWeights(){const t=[];for(const e of this.cells)t.push(...e.weights);return qo(t)}setWeights(t){const e=[];for(const n of this.cells){const s=n.weights.length,i=t.splice(s);for(let t=0;t<n.weights.length;++t)e.push([n.weights[t],i[t]])}Go(e)}}function Dh(t){const{ones:e,rate:n,training:s=!1,count:i=1}=t,r=()=>ho(e(),n),a=()=>co(r,e,s);if(!i||i<=1)return W(a().clone());return Array(i).fill(void 0).map(a).map(t=>W(t.clone()))}zh.className="StackedRNNCells",n.registerClass(zh);class Th extends kh{constructor(t){if(t.unroll)throw new ta("Unrolling is not possible with convolutional RNNs.");if(Array.isArray(t.cell))throw new ta("It is not possible at the moment to stack convolutional cells.");super(t),this.inputSpec=[new Ho({ndim:5})]}call(t,e){return s(()=>{if(null!=this.cell.dropoutMask&&(B(this.cell.dropoutMask),this.cell.dropoutMask=null),null!=this.cell.recurrentDropoutMask&&(B(this.cell.recurrentDropoutMask),this.cell.recurrentDropoutMask=null),e&&e.constants)throw new Qr("ConvRNN2D cell does not support constants");const n=null==e?null:e.mask,s=null==e?null:e.training,i=null==e?null:e.initialState;return super.call(t,{mask:n,training:s,initialState:i})})}computeOutputShape(t){let e=this.computeSingleOutputShape(t);return this.returnSequences||(e=[e[0],...e.slice(2)]),this.returnState&&(e=[e,...Array(2).fill([t[0],...e.slice(-3)])]),e}getInitialState(t){return s(()=>{const{stateSize:e}=this.cell,n=t.shape,s=this.computeSingleOutputShape(n),i=[s[0],...s.slice(2)],r=E(i);return Array.isArray(e)?Array(e.length).fill(r):[r]})}resetStates(t,n=!1){s(()=>{if(!this.stateful)throw new Xr("Cannot call resetStates() on an RNN Layer that is not stateful.");const s=this.inputSpec[0].shape,i=this.computeSingleOutputShape(s),r=[i[0],...i.slice(2)];if(null==s[0])throw new Qr("If an RNN is stateful, it needs to know its batch size. Specify the batch size of your input tensors: \n- If using a Sequential model, specify the batch size by passing a `batchInputShape` option to your first layer.\n- If using the functional API, specify the batch size by passing a `batchShape` option to your Input layer.");if(null==this.getStates())Array.isArray(this.cell.stateSize)?this.states_=this.cell.stateSize.map(()=>E(r)):this.states_=[E(r)];else if(null==t)B(this.states_),null!=this.keptStates&&(B(this.keptStates),this.keptStates=[]),Array.isArray(this.cell.stateSize)?this.states_=this.cell.stateSize.map(()=>E(r)):this.states_[0]=E(r);else{if(Array.isArray(t)||(t=[t]),t.length!==this.states_.length)throw new Qr(`Layer ${this.name} expects ${this.states_.length} state(s), but it received ${t.length} state value(s). Input received: `+t);n?this.keptStates.push(this.states_.slice()):B(this.states_);for(let n=0;n<this.states_.length;++n){const s=t[n],i=r;if(!e.arraysEqual(s.shape,i))throw new Qr(`State ${n} is incompatible with layer ${this.name}: expected shape=${i}, received shape=${s.shape}`);this.states_[n]=s}}this.states_=this.states_.map(t=>W(t.clone()))})}computeSingleOutputShape(t){const{dataFormat:e,filters:n,kernelSize:s,padding:i,strides:r,dilationRate:a}=this.cell,o="channelsFirst"===e,l=t[o?3:2],u=t[o?4:3],h=nh(l,s[0],i,r[0],a[0]),c=nh(u,s[1],i,r[1],a[1]);return[...t.slice(0,2),...o?[n,h,c]:[h,c,n]]}}Th.className="ConvRNN2D";class Eh extends Ah{constructor(t){const{filters:e,kernelSize:n,strides:s,padding:i,dataFormat:r,dilationRate:a}=t;super(Object.assign({},t,{units:e})),this.filters=e,ya(this.filters,"filters"),this.kernelSize=eh(n,2,"kernelSize"),this.kernelSize.forEach(t=>ya(t,"kernelSize")),this.strides=eh(s||1,2,"strides"),this.strides.forEach(t=>ya(t,"strides")),this.padding=i||"valid",Ma(this.padding),this.dataFormat=r||"channelsLast",Ra(this.dataFormat),this.dilationRate=eh(a||1,2,"dilationRate"),this.dilationRate.forEach(t=>ya(t,"dilationRate"))}build(t){var e;t=Ko(t);const n="channelsFirst"===this.dataFormat?1:t.length-1;if(null==t[n])throw new Qr("The channel dimension of the input should be defined. Found "+t[n]);const s=t[n],i=this.kernelSize.concat([s,4*this.filters]);this.kernel=this.addWeight("kernel",i,null,this.kernelInitializer,this.kernelRegularizer,!0,this.kernelConstraint);const r=this.kernelSize.concat([this.filters,4*this.filters]);if(this.recurrentKernel=this.addWeight("recurrent_kernel",r,null,this.recurrentInitializer,this.recurrentRegularizer,!0,this.recurrentConstraint),this.useBias){let t;if(this.unitForgetBias){const n=this.biasInitializer,s=this.filters;t=new((e=class extends go{apply(t,e){return eo([n.apply([s]),F([s]),n.apply([2*s])])}}).className="CustomInit",e)}else t=this.biasInitializer;this.bias=this.addWeight("bias",[4*this.filters],null,t,this.biasRegularizer,!0,this.biasConstraint)}this.built=!0}call(t,e){return s(()=>{if(3!==t.length)throw new Qr("ConvLSTM2DCell expects 3 input Tensors (inputs, h, c), got "+t.length+".");const n=e.training||!1,s=t[0],i=t[1],r=t[2];0<this.dropout&&this.dropout<1&&null==this.dropoutMask&&(this.dropoutMask=Dh({ones:()=>Y(s),rate:this.dropout,training:n,count:4}));const o=this.dropoutMask,l=(t,e,n)=>e&&e[n]?a(e[n],t):t;let h=l(s,o,0),c=l(s,o,1),p=l(s,o,2),d=l(s,o,3);0<this.recurrentDropout&&this.recurrentDropout<1&&null==this.recurrentDropoutMask&&(this.recurrentDropoutMask=Dh({ones:()=>Y(i),rate:this.recurrentDropout,training:n,count:4}));const f=this.recurrentDropoutMask;let g=l(i,f,0),m=l(i,f,1),y=l(i,f,2),b=l(i,f,3);const[w,k,x,v]=Ct(this.kernel.read(),4,3),[S,I,N,A]=this.useBias?Ct(this.bias.read(),4):[null,null,null,null];h=this.inputConv(h,w,S,this.padding),c=this.inputConv(c,k,I,this.padding),p=this.inputConv(p,x,N,this.padding),d=this.inputConv(d,v,A,this.padding);const[C,z,D,T]=Ct(this.recurrentKernel.read(),4,3);g=this.recurrentConv(g,C),m=this.recurrentConv(m,z),y=this.recurrentConv(y,D),b=this.recurrentConv(b,T);const E=this.recurrentActivation.apply(u(h,g)),F=this.recurrentActivation.apply(u(c,m)),$=u(a(F,r),a(E,this.activation.apply(u(p,y)))),_=a(this.recurrentActivation.apply(u(d,b)),this.activation.apply($));return[_,_,$]})}getConfig(){const t=function(t,e){var n={};for(var s in t)Object.prototype.hasOwnProperty.call(t,s)&&e.indexOf(s)<0&&(n[s]=t[s]);if(null!=t&&"function"==typeof Object.getOwnPropertySymbols){var i=0;for(s=Object.getOwnPropertySymbols(t);i<s.length;i++)e.indexOf(s[i])<0&&Object.prototype.propertyIsEnumerable.call(t,s[i])&&(n[s[i]]=t[s[i]])}return n}(super.getConfig(),["units"]),e={filters:this.filters,kernelSize:this.kernelSize,padding:this.padding,dataFormat:this.dataFormat,dilationRate:this.dilationRate,strides:this.strides};return Object.assign({},t,e)}inputConv(t,e,n,s){const i=zt(t,e,this.strides,s||"valid","channelsFirst"===this.dataFormat?"NCHW":"NHWC",this.dilationRate);return n?uo(i,n,this.dataFormat):i}recurrentConv(t,e){return zt(t,e,1,"same","channelsFirst"===this.dataFormat?"NCHW":"NHWC")}}Eh.className="ConvLSTM2DCell",n.registerClass(Eh);class Fh extends Th{constructor(t){const e=new Eh(t);super(Object.assign({},t,{cell:e}))}static fromConfig(t,e){return new t(e)}}Fh.className="ConvLSTM2D",n.registerClass(Fh);class $h extends Qo{constructor(t){super(t),this.rate=Math.max(Math.min(t.rate,1),0),this.noiseShape=t.noiseShape,this.seed=t.seed,this.supportsMasking=!0}getNoiseShape(t){if(null==this.noiseShape)return this.noiseShape;const e=t.shape,n=[];for(let t=0;t<this.noiseShape.length;++t)n.push(null==this.noiseShape[t]?e[t]:this.noiseShape[t]);return n}call(t,e){return s(()=>{this.invokeCallHook(t,e);const n=Uo(t);if(0<this.rate&&this.rate<1){const t=null!=e.training&&e.training,s=this.getNoiseShape(n);return co(()=>ho(n,this.rate,s,this.seed),()=>n,t)}return t})}getConfig(){const t={rate:this.rate,noiseShape:this.noiseShape,seed:this.seed},e=super.getConfig();return Object.assign(t,e),t}dispose(){return super.dispose()}}$h.className="Dropout",n.registerClass($h);class _h extends $h{constructor(t){super(t),this.inputSpec=[{ndim:3}]}getNoiseShape(t){const e=t.shape;return[e[0],1,e[2]]}}_h.className="SpatialDropout1D",n.registerClass(_h);class Lh extends Qo{constructor(t){if(super(t),this.activation=null,this.useBias=!0,this.kernel=null,this.bias=null,this.DEFAULT_KERNEL_INITIALIZER="glorotNormal",this.DEFAULT_BIAS_INITIALIZER="zeros",null==t.batchInputShape&&null==t.inputShape&&null!=t.inputDim){let e=null;null!=t.batchSize&&(e=t.batchSize),this.batchInputShape=[e,t.inputDim]}this.units=t.units,ya(this.units,"units"),this.activation=Wu(t.activation),null!=t.useBias&&(this.useBias=t.useBias),this.kernelInitializer=_o(t.kernelInitializer||this.DEFAULT_KERNEL_INITIALIZER),this.biasInitializer=_o(t.biasInitializer||this.DEFAULT_BIAS_INITIALIZER),this.kernelConstraint=za(t.kernelConstraint),this.biasConstraint=za(t.biasConstraint),this.kernelRegularizer=Hu(t.kernelRegularizer),this.biasRegularizer=Hu(t.biasRegularizer),this.activityRegularizer=Hu(t.activityRegularizer),this.supportsMasking=!0,this.inputSpec=[{minNDim:2}]}build(t){const e=(t=Ko(t))[t.length-1];null==this.kernel&&(this.kernel=this.addWeight("kernel",[e,this.units],null,this.kernelInitializer,this.kernelRegularizer,!0,this.kernelConstraint),this.useBias&&(this.bias=this.addWeight("bias",[this.units],null,this.biasInitializer,this.biasRegularizer,!0,this.biasConstraint))),this.inputSpec=[{minNDim:2,axes:{[-1]:e}}],this.built=!0}computeOutputShape(t){const e=(t=Ko(t)).slice();return e[e.length-1]=this.units,e}call(t,e){return s(()=>{this.invokeCallHook(t,e);const n=Uo(t),s=ba(this.activation.getClassName());let i;return null!=s?i=ro(n,this.kernel.read(),s,this.bias?this.bias.read():null):(i=ro(n,this.kernel.read()),null!=this.bias&&(i=uo(i,this.bias.read())),null!=this.activation&&(i=this.activation.apply(i))),i})}getConfig(){const t={units:this.units,activation:Bu(this.activation),useBias:this.useBias,kernelInitializer:$o(this.kernelInitializer),biasInitializer:$o(this.biasInitializer),kernelRegularizer:qu(this.kernelRegularizer),biasRegularizer:qu(this.biasRegularizer),activityRegularizer:qu(this.activityRegularizer),kernelConstraint:Aa(this.kernelConstraint),biasConstraint:Aa(this.biasConstraint)},e=super.getConfig();return Object.assign(t,e),t}}Lh.className="Dense",n.registerClass(Lh);class Rh extends Qo{constructor(t){super(t=t||{}),this.inputSpec=[{minNDim:3}],this.dataFormat=t.dataFormat}computeOutputShape(t){t=Ko(t);for(const e of t.slice(1))if(null==e)throw new Qr(`The shape of the input to "Flatten" is not fully defined (got ${t.slice(1)}). Make sure to pass a complete "input_shape" or "batch_input_shape" argument to the first layer in your model.`);return[t[0],Va(t,1)]}call(t,e){return s(()=>{this.invokeCallHook(t,e);let n=Uo(t);if("channelsFirst"===this.dataFormat&&n.rank>1){const t=[0];for(let e=2;e<n.rank;++e)t.push(e);t.push(1),n=n.transpose(t)}return function(t){if(t.rank<=1)throw new Qr(`batchFlatten requires a minimum rank of 2. Got rank: ${t.rank}.`);const e=[t.shape[0],Va(t.shape,1)];return t.reshape(e)}(n)})}getConfig(){const t={};null!=this.dataFormat&&(t.dataFormat=this.dataFormat);const e=super.getConfig();return Object.assign(t,e),t}}Rh.className="Flatten",n.registerClass(Rh);class Mh extends Qo{constructor(t){super(t),this.supportsMasking=!0,this.activation=Wu(t.activation)}call(t,e){return s(()=>{this.invokeCallHook(t,e);const n=Uo(t);return this.activation.apply(n)})}getConfig(){const t={activation:Bu(this.activation)},e=super.getConfig();return Object.assign(t,e),t}}Mh.className="Activation",n.registerClass(Mh);class Oh extends Qo{constructor(t){super(t),this.n=t.n,this.inputSpec=[{ndim:2}]}computeOutputShape(t){return[t[0],this.n,t[1]]}call(t,e){return s(()=>{return t=Uo(t),e=t,n=this.n,s(()=>{if(2!==e.shape.length)throw new Qr(`repeat() expects a rank-2 tensor, but received a rank-${e.shape.length} tensor.`);return so(Xa(e,1),[1,n,1])});var e,n})}getConfig(){const t={n:this.n},e=super.getConfig();return Object.assign(t,e),t}}Oh.className="RepeatVector",n.registerClass(Oh);class Bh extends Qo{constructor(t){super(t),this.targetShape=t.targetShape;for(let t=0;t<this.targetShape.length;++t)this.isUnknown(this.targetShape[t])&&(this.targetShape[t]=null)}isUnknown(t){return t<0||null==t}fixUnknownDimension(t,e){const n="Total size of new array must be unchanged.",s=e.slice();let i=1,r=null;for(let t=0;t<s.length;++t){const e=s[t];if(this.isUnknown(e)){if(null!==r)throw new Qr("Can only specifiy one unknown dimension.");r=t}else i*=e}const a=Va(t);if(null!==r){if(0===i||a%i!=0)throw new Qr(n);s[r]=a/i}else if(a!==i)throw new Qr(n);return s}computeOutputShape(t){let e=!1;for(let n=0;n<t.length;++n)if(this.isUnknown(t[n])){e=!0;break}return e?t.slice(0,1).concat(this.targetShape):t.slice(0,1).concat(this.fixUnknownDimension(t.slice(1),this.targetShape))}call(t,e){return s(()=>{this.invokeCallHook(t,e);const n=Uo(t),s=n.shape,i=s.slice(0,1).concat(this.fixUnknownDimension(s.slice(1),this.targetShape));return n.reshape(i)})}getConfig(){const t={targetShape:this.targetShape},e=super.getConfig();return Object.assign(t,e),t}}Bh.className="Reshape",n.registerClass(Bh);class Ph extends Qo{constructor(t){if(super(t),null==t.dims)throw new Error("Required configuration field `dims` is missing during Permute constructor call.");if(!Array.isArray(t.dims))throw new Error("Permute constructor requires `dims` to be an Array, but received "+t.dims+" instead.");const n=Ja(1,t.dims.length+1);if(!e.arraysEqual(t.dims.slice().sort(),n))throw new Error("Invalid permutation `dims`: "+JSON.stringify(t.dims)+" `dims` must contain consecutive integers starting from 1.");this.dims=t.dims,this.dimsIncludingBatch=[0].concat(this.dims),this.inputSpec=[new Ho({ndim:this.dims.length+1})]}computeOutputShape(t){const e=(t=Ko(t)).slice();return this.dims.forEach((n,s)=>{e[s+1]=t[n]}),e}call(t,e){return yt(Uo(t),this.dimsIncludingBatch)}getConfig(){const t={dims:this.dims},e=super.getConfig();return Object.assign(t,e),t}}Ph.className="Permute",n.registerClass(Ph);class Wh extends Qo{constructor(t){super(null==t?{}:t),this.supportsMasking=!0,this.maskValue=null!=t?null==t.maskValue?0:t.maskValue:0}computeOutputShape(t){return t}getConfig(){const t=super.getConfig(),e={maskValue:this.maskValue};return Object.assign(e,t),e}computeMask(t,e){const n=Uo(t);return Dt(Tt(n,this.maskValue),-1)}call(t,e){return s(()=>{this.invokeCallHook(t,e);const n=Uo(t),s=Dt(Tt(n,this.maskValue),-1,!0);return n.mul(s.asType(n.dtype))})}}Wh.className="Masking",n.registerClass(Wh);class Uh extends Qo{constructor(t){if(super(t),this.embeddings=null,this.DEFAULT_EMBEDDINGS_INITIALIZER="randomUniform",null==t.batchInputShape&&null==t.inputShape){let e=null;null!=t.batchSize&&(e=t.batchSize),null==t.inputLength?this.batchInputShape=[e,null]:this.batchInputShape=[e].concat(aa(t.inputLength))}this.inputDim=t.inputDim,ya(this.inputDim,"inputDim"),this.outputDim=t.outputDim,ya(this.outputDim,"outputDim"),this.embeddingsInitializer=_o(t.embeddingsInitializer||this.DEFAULT_EMBEDDINGS_INITIALIZER),this.embeddingsRegularizer=Hu(t.embeddingsRegularizer),this.activityRegularizer=Hu(t.activityRegularizer),this.embeddingsConstraint=za(t.embeddingsConstraint),this.maskZero=t.maskZero,this.supportsMasking=t.maskZero,this.inputLength=t.inputLength}build(t){this.embeddings=this.addWeight("embeddings",[this.inputDim,this.outputDim],this.dtype,this.embeddingsInitializer,this.embeddingsRegularizer,!0,this.embeddingsConstraint),this.built=!0}warnOnIncompatibleInputShape(t){}computeMask(t,e){return s(()=>this.maskZero?(t=Uo(t),Tt(t,Et(t))):null)}computeOutputShape(t){if(t=Ko(t),null==this.inputLength)return[...t,this.outputDim];const e=aa(this.inputLength);if(e.length!==t.length-1)throw new Qr(`"inputLength" is ${this.inputLength}, but received input shape has shape `+t);{let n=0;for(let s=0;s<e.length;++s){const i=e[s],r=t[s+1];if(null!=i&&null!=r&&i!==r)throw new Qr(`"inputLength" is ${this.inputLength}, but received input shape has shape `+t);null==i&&(e[n]=r),n++}}return[t[0],...e,this.outputDim]}call(t,e){return s(()=>{this.invokeCallHook(t,e);let n=Uo(t);"int32"!==n.dtype&&(n=Za(n,"int32"));return ao(this.embeddings.read(),n.as1D()).reshape(Ko(this.computeOutputShape(n.shape)))})}getConfig(){const t={inputDim:this.inputDim,outputDim:this.outputDim,embeddingsInitializer:$o(this.embeddingsInitializer),embeddingsRegularizer:qu(this.embeddingsRegularizer),activityRegularizer:qu(this.activityRegularizer),embeddingsConstraint:Aa(this.embeddingsConstraint),maskZero:this.maskZero,inputLength:this.inputLength},e=super.getConfig();return Object.assign(t,e),t}}Uh.className="Embedding",n.registerClass(Uh);class Kh extends Qo{constructor(t){super(t||{}),this.supportsMasking=!0}mergeFunction(t){throw new ta}computeElementwiseOpOutputShape(t,e){if(null==t||null==e)return null;if(t.length<e.length)return this.computeElementwiseOpOutputShape(e,t);if(0===e.length)return t;const n=t.slice(0,t.length-e.length);for(let s=0;s<e.length;++s){const i=t[t.length-e.length+s],r=e[s];if(null==i||null==r||i<0||r<0)n.push(null);else if(1===i)n.push(r);else if(1===r)n.push(i);else{if(i!==r)throw new Qr("Operands could not be broadcast together with shapes "+JSON.stringify(t)+" "+JSON.stringify(e));n.push(i)}}return n}build(t){if(Array.isArray(t)&&!Array.isArray(t[0])&&(t=[Ko(t)]),(t=t).length<2)throw new Qr(`A merge layer should be called on an Array of at least 2 inputs. Got ${t.length} input(s).`);let e=[];for(const n of t)null!=n&&null!==n[0]&&e.push(n[0]);if(e=da(e),e.length>1)throw new Qr(`Can not merge tensors with different batch sizes. Got tensors with shapes: ${JSON.stringify(t)}.`);let n=null==t[0]?null:t[0].slice(1);for(let e=1;e<t.length;++e){const s=null==t[e]?null:t[e].slice(1);n=this.computeElementwiseOpOutputShape(n,s)}const s=t.map(t=>t.length);-1===t.indexOf(null)&&1===da(s).length?this.reshapeRequired=!1:this.reshapeRequired=!0}call(t,e){return s(()=>{if(t=t,this.reshapeRequired){const e=[],n=t.map(t=>t.rank);if(-1===n.indexOf(null)){const s=Ha(n);for(let n of t){const t=n.rank;for(let e=0;e<s-t;++e)n=Xa(n,1);e.push(n)}return this.mergeFunction(e)}{let n=!1;for(const s of t){const t=s.rank;if(null==t){const t=s.shape,i=t[0],r=t.slice(1).concat([i]);let a=s.reshape([i].concat(Va(t.slice(1))));a=yt(a,[1,0]),a=a.reshape(r),e.push(a),n=!0}else if(t>1){const i=Ja(1,t).concat([0]);e.push(yt(s,i)),n=!0}else e.push(s)}let s=this.mergeFunction(e);const i=s.rank;if(n)if(null==i){const t=s.shape,e=t[t.length-1],n=[e].concat(t.slice(0,t.length-1));s=yt(s.reshape([-1,e]),[1,0]).reshape(n)}else if(i>1){const t=[i-1].concat(Ja(0,i-1));s=yt(s,t)}return s}}return this.mergeFunction(t)})}computeOutputShape(t){let e;e=null==(t=t)[0]?null:t[0].slice(1);for(let n=1;n<t.length;++n){const s=null==t[n]?null:t[n].slice(1);e=this.computeElementwiseOpOutputShape(e,s)}let n=[];for(const e of t)null!=e&&null!==e[0]&&n.push(e[0]);return n=da(n),e=1===n.length?n.concat(e):[null].concat(e),e}computeMask(t,e){return s(()=>{if(null==e)return null;if(!Array.isArray(e))throw new Qr("`mask` should be an Array");if(!Array.isArray(t))throw new Qr("`inputs` should be an Array");if(e.length!==t.length)throw new Qr(`The Array 'inputs' and 'mask' are expected to have the same length, but have different lengths (${t.length} vs ${e.length})`);if(e.every(t=>null==t))return null;let n=(e=e.map(t=>null==t?t:St(t,0)))[0];for(let t=1;t<e.length-1;++t)n=nt(n,e[t]);return n})}}class jh extends Kh{constructor(t){super(t)}mergeFunction(t){return s(()=>{let e=t[0].clone();for(let n=1;n<t.length;++n)e=u(e,t[n]);return e})}}jh.className="Add",n.registerClass(jh);class Vh extends Kh{constructor(t){super(t)}mergeFunction(t){return s(()=>{let e=t[0].clone();for(let n=1;n<t.length;++n)e=a(e,t[n]);return e})}}Vh.className="Multiply",n.registerClass(Vh);class qh extends Kh{constructor(t){super(t)}mergeFunction(t){return s(()=>{let e=t[0].clone();for(let n=1;n<t.length;++n)e=u(e,t[n]);return a(1/t.length,e)})}}qh.className="Average",n.registerClass(qh);class Gh extends Kh{constructor(t){super(t)}mergeFunction(t){return s(()=>{let e=t[0];for(let n=1;n<t.length;++n)e=H(e,t[n]);return e})}}Gh.className="Maximum",n.registerClass(Gh);class Hh extends Kh{constructor(t){super(t)}mergeFunction(t){return s(()=>{let e=t[0];for(let n=1;n<t.length;++n)e=ct(e,t[n]);return e})}}Hh.className="Minimum",n.registerClass(Hh);class Jh extends Kh{constructor(t){super(t),this.DEFAULT_AXIS=-1,null==t&&(t={}),this.axis=null==t.axis?this.DEFAULT_AXIS:t.axis,this.supportsMasking=!0,this.reshapeRequired=!1}build(t){if(!Array.isArray(t)||!Array.isArray(t[0])||1===t.length)throw new Qr("A `Concatenate` layer should be called on a list of at least 2 inputs");t=t;let n=!0;for(const e of t)if(null!=e){n=!1;break}if(n)return;const s=[];for(let n=0;n<t.length;++n){const i=t[n].slice();i.splice(this.axis,1);let r=!1;for(const t of s)if(e.arraysEqual(t,i)){r=!0;break}r||s.push(i)}if(s.length>1)throw new Qr("A `Concatenate` layer requires inputs with matching shapes except for the concat axis. Got input shapes: "+JSON.stringify(t))}mergeFunction(t){return s(()=>eo(t,this.axis))}computeOutputShape(t){if(!Array.isArray(t)||!Array.isArray(t[0]))throw new Qr("A `Concatenate` layer should be called on a list of inputs.");const e=t,n=e[0].slice(),s=this.axis<0?n.length+this.axis:this.axis;for(const t of e.slice(1)){if(null==n[s]||null==t[s]){n[s]=null;break}n[s]+=t[s]}return n}computeMask(t,e){if(null==e)return null;if(!Array.isArray(e))throw new Qr("`mask` should be an array for Concatenate");if(!Array.isArray(t))throw new Qr("`inputs` should be an array for Concatenate");if(e.length!==t.length)throw new Qr(`Mismatch in the length of mask (${e.length}) and the legnth of inputs (${t.length})`);return s(()=>{let n=!0;if(e.forEach(t=>{null==t||(n=!1)}),n)return null;const s=[];for(let n=0;n<t.length;++n)null==e[n]?s.push(Y(t[n]).asType("bool")):e[n].rank<t[n].rank?s.push(St(e[n],-1)):s.push(e[n]);const i=N(s,this.axis);return Ft(i,-1,!1)})}getConfig(){const t={axis:this.axis},e=super.getConfig();return Object.assign(t,e),t}}function Zh(t,e){for(;t<0;)t+=e;return t}Jh.className="Concatenate",n.registerClass(Jh);class Xh extends Kh{constructor(t){super(t),this.axes=t.axes,this.normalize=null!=t.normalize&&t.normalize,this.supportsMasking=!0,this.reshapeRequired=!1}build(t){e.assert(Array.isArray(t)&&2===t.length&&Array.isArray(t[0])&&Array.isArray(t[1]),()=>"A `Dot` layer should be called on a list of exactly 2 inputs.");const n=t[0],s=t[1];if(n.length>3||s.length>3)throw new ta("Dot layer does not support tensors of 4D or higher rank yet.");const i=this.interpretAxes(n,s);if(n[i[0]]!==s[i[1]])throw new Qr(`Dimension incompatibility: ${n[i[0]]} !== ${s[i[1]]}`)}mergeFunction(t){if(2!==t.length)throw new Qr(`A \`Dot\` layer must be called on exactly 2 inputs, but received ${t.length} input(s).`);let n,i=t[0],r=t[1];return n=Array.isArray(this.axes)?this.axes.map((e,n)=>Zh(e,t[n].shape.length)):[Zh(this.axes,i.shape.length),Zh(this.axes,r.shape.length)],this.normalize&&(i=fl(i,n[0]),r=fl(r,n[1])),function(t,n,i){if(t.shape.length>3||n.shape.length>3)throw new ta("batchDot is not implemented for tensors of 4D or higher rank yet");if(e.assert(t.shape.length>=2,()=>"batchDot requires the rank of x to be >= 2, but got "+t.shape.length),e.assert(t.shape.length>=2,()=>"batchDot requires the rank of y to be >= 2, but got "+n.shape.length),"number"==typeof i&&(i=[i,i]),"complex64"===t.dtype||"complex64"===n.dtype)throw new ta("batchDot is not implemented for complex64-type Tensors yet.");const r=t.shape.length,a=n.shape.length;null==i&&(i=[r-1,a-2]);const o=i;return s(()=>{let e,s;if(r>a){e=r-a;const t=[];for(let n=0;n<e;++n)t.push(1);n=n.reshape(n.shape.concat(t))}else if(a>r){e=a-r;const n=[];for(let t=0;t<e;++t)n.push(1);t=t.reshape(t.shape.concat(n))}else e=0;if(2===t.shape.length&&2===n.shape.length)s=o[0]===o[1]?t.mul(n).sum(o[0]):t.transpose([1,0]).mul(n).sum(o[1]);else{const e=o[0]!==t.shape.length-1,i=o[1]===n.shape.length-1;s=t.matMul(n,e,i)}if(e>0){let t;t=r>a?r+a-3:r-1;const n=[];for(let s=t;s<t+e;++s)n.push(s);s=s.squeeze(n)}return 1===s.shape.length&&(s=s.expandDims(1)),s})}(i,r,n)}interpretAxes(t,e){let n;return n=Array.isArray(this.axes)?this.axes:[Zh(this.axes,t.length),Zh(this.axes,e.length)],n}computeOutputShape(t){e.assert(Array.isArray(t)&&2===t.length&&Array.isArray(t[0])&&Array.isArray(t[1]),()=>"A `Dot` layer should be called on a list of exactly 2 inputs.");const n=t[0].slice(),s=t[1].slice();if(n.length>3||s.length>3)throw new ta("Dot layer does not support tensors of 4D or higher rank yet.");const i=this.interpretAxes(n,s);n.splice(i[0],1),s.splice(i[1],1),s.splice(0,1);const r=n.concat(s);return 1===r.length&&r.push(1),r}computeMask(t,e){return null}getConfig(){const t={axes:this.axes,normalize:this.normalize},e=super.getConfig();return Object.assign(t,e),t}}Xh.className="Dot",n.registerClass(Xh);class Yh extends Qo{constructor(t){super(t),this.supportsMasking=!0,this.stddev=t.stddev}computeOutputShape(t){return t}getConfig(){const t=super.getConfig(),e={stddev:this.stddev};return Object.assign(e,t),e}call(t,e){return s(()=>{this.invokeCallHook(t,e);const n=Uo(t);return co(()=>io(n.shape,0,this.stddev).add(n),()=>n,e.training||!1)})}}Yh.className="GaussianNoise",n.registerClass(Yh);class Qh extends Qo{constructor(t){super(t),this.supportsMasking=!0,this.rate=t.rate}computeOutputShape(t){return t}getConfig(){const t=super.getConfig(),e={rate:this.rate};return Object.assign(e,t),e}call(t,e){return s(()=>{this.invokeCallHook(t,e);const n=Uo(t);if(this.rate>0&&this.rate<1){return co(()=>{const t=Math.sqrt(this.rate/(1-this.rate));return n.mul(io(n.shape,1,t))},()=>n,e.training||!1)}return n})}}Qh.className="GaussianDropout",n.registerClass(Qh);class tc extends Qo{constructor(t){super(t),this.supportsMasking=!0,this.rate=t.rate,this.noiseShape=t.noiseShape}_getNoiseShape(t){return this.noiseShape||Uo(t).shape}computeOutputShape(t){return t}getConfig(){const t=super.getConfig(),e={rate:this.rate};return Object.assign(e,t),e}call(t,e){return s(()=>{if(this.rate<1&&this.rate>0){const n=this._getNoiseShape(t);return co(()=>{const e=Uo(t),s=-1.7580993408473766;let i=$t(_(n),this.rate);i=Za(i,"float32");const r=((1-this.rate)*(1+this.rate*s**2))**-.5,a=-r*s*this.rate;return e.mul(i).add(i.add(-1).mul(s)).mul(r).add(a)},()=>Uo(t),e.training||!1)}return t})}}function ec(t,e,n,s,i,r=.001){let a;if(2===t.rank)a=Lt(t,e,n,s,i,r);else if(3===t.rank)a=Rt(t,e,n,s,i,r);else{if(4!==t.rank)throw new ta(`batchNormalization is not implemented for array of rank ${t.rank} yet`);a=Mt(t,e,n,s,i,r)}return a}function nc(t,n,i,r,a=.001){return e.arraysEqual(r.slice().sort(),Ja(0,t.rank-1))?function(t,e,n,i,r=.001){return s(()=>{const s=_t(t,i),a=s.mean,o=s.variance;return[ec(t,a,o,n,e,r),a,o]})}(t,n,i,r,a):function(t,e,n,i,r=.001){return s(()=>{const s=_t(t,i),a=s.mean,o=s.variance,l=[];for(const e of Ja(0,t.rank))-1!==i.indexOf(e)?l.push(1):l.push(t.shape[e]);const u=a.reshape(l),h=o.reshape(l),c=null==e?null:e.reshape(l),p=null==n?null:n.reshape(l);return[ec(t,u,h,p,c,r),a,o]})}(t,n,i,r,a)}tc.className="AlphaDropout",n.registerClass(tc);class sc extends Qo{constructor(t){null==t&&(t={}),super(t),this.supportsMasking=!0,this.axis=null==t.axis?-1:t.axis,this.momentum=null==t.momentum?.99:t.momentum,this.epsilon=null==t.epsilon?.001:t.epsilon,this.center=null==t.center||t.center,this.scale=null==t.scale||t.scale,this.betaInitializer=_o(t.betaInitializer||"zeros"),this.gammaInitializer=_o(t.gammaInitializer||"ones"),this.movingMeanInitializer=_o(t.movingMeanInitializer||"zeros"),this.movingVarianceInitializer=_o(t.movingVarianceInitializer||"ones"),this.betaConstraint=za(t.betaConstraint),this.gammaConstraint=za(t.gammaConstraint),this.betaRegularizer=Hu(t.betaRegularizer),this.gammaRegularizer=Hu(t.gammaRegularizer)}build(t){t=Ko(t);const e=this.axis>=0?this.axis:this.axis+t.length,n=t[e];if(null==n)throw new Qr(`Axis ${e} of input tensor should have a defined dimension but the layer received an input with shape `+JSON.stringify(t)+".");this.inputSpec=[new Ho({ndim:t.length,axes:{[e]:n}})];const s=[n];this.scale&&(this.gamma=this.addWeight("gamma",s,null,this.gammaInitializer,this.gammaRegularizer,!0,this.gammaConstraint)),this.center&&(this.beta=this.addWeight("beta",s,null,this.betaInitializer,this.betaRegularizer,!0,this.betaConstraint)),this.movingMean=this.addWeight("moving_mean",s,null,this.movingMeanInitializer,null,!1),this.movingVariance=this.addWeight("moving_variance",s,null,this.movingVarianceInitializer,null,!1),this.built=!0}call(t,n){return s(()=>{const i=null!=n.training&&n.training,r=Uo(t),a=r.shape,o=a.length,l=Ja(0,o),u=this.axis>=0?this.axis:this.axis+o;l.splice(u,1);const h=na(1,o);h[u]=a[u];const c=l.slice();c.sort();const p=!e.arraysEqual(c,Ja(0,o).slice(0,o-1));if(!i)return(()=>{if(p){const t=this.movingMean.read().reshape(h),e=this.movingVariance.read().reshape(h),n=this.center?this.beta.read().reshape(h):null,s=this.scale?this.gamma.read().reshape(h):null;return ec(r,t,e,n,s,this.epsilon)}return ec(r,this.movingMean.read(),this.movingVariance.read(),null==this.beta?null:this.beta.read(),null==this.gamma?null:this.gamma.read(),this.epsilon)})();const[d,f,g]=nc(r,this.gamma.read(),this.beta.read(),l,this.epsilon),m=(t,e,n)=>{s(()=>{const s=1-n,i=t.read(),r=i.sub(e).mul(s);t.write(i.sub(r))})};return(()=>{m(this.movingMean,f,this.momentum),m(this.movingVariance,g,this.momentum)})(),d})}getConfig(){const t={axis:this.axis,momentum:this.momentum,epsilon:this.epsilon,center:this.center,scale:this.scale,betaInitializer:$o(this.betaInitializer),gammaInitializer:$o(this.gammaInitializer),movingMeanInitializer:$o(this.movingMeanInitializer),movingVarianceInitializer:$o(this.movingVarianceInitializer),betaRegularizer:qu(this.betaRegularizer),gammaRegularizer:qu(this.gammaRegularizer),betaConstraint:Aa(this.betaConstraint),gammaConstraint:Aa(this.gammaConstraint)},e=super.getConfig();return Object.assign(t,e),t}}sc.className="BatchNormalization",n.registerClass(sc);class ic extends Qo{constructor(t){if(null==t&&(t={}),super(t),this.axis=null==t.axis?-1:t.axis,"number"==typeof this.axis){if(!Number.isInteger(this.axis))throw new Error("Expected axis to be an integer, but received "+this.axis)}else{if(!Array.isArray(this.axis))throw new Error("Expected axis to be an integer or an array of integers, but received "+JSON.stringify(this.axis));for(const t of this.axis)if(!Number.isInteger(t))throw new Error("Expected axis to be an array of integers, but received "+JSON.stringify(this.axis))}this.epsilon=null==t.epsilon?.001:t.epsilon,this.center=null==t.center||t.center,this.scale=null==t.scale||t.scale,this.betaInitializer=_o(t.betaInitializer||"zeros"),this.gammaInitializer=_o(t.gammaInitializer||"ones"),this.betaRegularizer=Hu(t.betaRegularizer),this.gammaRegularizer=Hu(t.gammaRegularizer),this.supportsMasking=!0}build(t){const e=(t=Ko(t)).length;"number"==typeof this.axis&&(this.axis=[this.axis]);for(let t=0;t<this.axis.length;++t)this.axis[t]<0&&(this.axis[t]+=e);for(const t of this.axis)if(t<0||t>=e)throw new Error("Invalid axis: "+t);if(this.axis.length!==da(this.axis).length)throw new Error("Found duplicate axes in: "+this.axis);const n=this.axis.map(e=>t[e]);this.scale?this.gamma=this.addWeight("gamma",n,"float32",this.gammaInitializer,this.gammaRegularizer,!0):this.gamma=null,this.center?this.beta=this.addWeight("beta",n,"float32",this.betaInitializer,this.betaRegularizer,!0):this.beta=null,this.built=!0}call(t,e){const n=Uo(t),i=n.shape,r=i.length;return s(()=>{let{mean:t,variance:e}=_t(n,this.axis,!0);const s=na(1,r);for(const t of this.axis)s[t]=i[t];const a=t=>null!=t&&t.shape.length!==r&&this.axis!==[r-1]?t.reshape(s):t;let o=a(this.gamma.read()),l=a(this.beta.read());const u=[],h=[];for(let t=0;t<r;++t)-1!==this.axis.indexOf(t)?(u.push(i[t]),h.push(1)):(u.push(1),h.push(i[t]));return t=t.tile(u),e=e.tile(u),o=o.tile(h),l=l.tile(h),ec(n,t,e,l,o,this.epsilon)})}getConfig(){const t={axis:this.axis,epsilon:this.epsilon,center:this.center,scale:this.scale,betaInitializer:$o(this.betaInitializer),gammaInitializer:$o(this.gammaInitializer),betaRegularizer:qu(this.betaRegularizer),gammaRegularizer:qu(this.gammaRegularizer)},e=super.getConfig();return Object.assign(t,e),t}}ic.className="LayerNormalization",n.registerClass(ic);class rc extends Qo{constructor(t){if(null==t&&(t={}),super(t),this.dataFormat=null==t.dataFormat?"channelsLast":t.dataFormat,null==t.padding)this.padding=[[1,1],[1,1]];else if("number"==typeof t.padding)this.padding=[[t.padding,t.padding],[t.padding,t.padding]];else{if(t.padding=t.padding,2!==t.padding.length)throw new Qr(`ZeroPadding2D expects padding to be a length-2 array, but received a length-${t.padding.length} array.`);let e,n;if("number"==typeof t.padding[0])e=[t.padding[0],t.padding[0]],n=[t.padding[1],t.padding[1]];else{if(t.padding=t.padding,2!==t.padding[0].length)throw new Qr(`ZeroPadding2D expects height padding to be a length-2 array, but received a length-${t.padding[0].length} array.`);if(e=t.padding[0],2!==t.padding[1].length)throw new Qr(`ZeroPadding2D expects width padding to be a length-2 array, but received a length-${t.padding[1].length} array.`);n=t.padding[1]}this.padding=[e,n]}this.inputSpec=[new Ho({ndim:4})]}computeOutputShape(t){let e,n;return t=Ko(t),"channelsFirst"===this.dataFormat?(e=null!=t[2]&&t[2]>=0?t[2]+this.padding[0][0]+this.padding[0][1]:null,n=null!=t[3]&&t[3]>=0?t[3]+this.padding[1][0]+this.padding[1][1]:null,[t[0],t[1],e,n]):(e=null!=t[1]&&t[1]>=0?t[1]+this.padding[0][0]+this.padding[0][1]:null,n=null!=t[2]&&t[2]>=0?t[2]+this.padding[1][0]+this.padding[1][1]:null,[t[0],e,n,t[3]])}call(t,e){return s(()=>{return e=Uo(t),n=this.padding,i=this.dataFormat,s(()=>{if(4!==e.rank)throw new Qr("temporalPadding expects input tensor to be 4-D, but received a "+e.rank+"-D tensor.");if(null==n&&(n=[[1,1],[1,1]]),2!==n.length||2!==n[0].length||2!==n[1].length)throw new Qr("spatial2dPadding expects `padding` to be an Array of two Arrays, each of which is an Array of two integers.");if(null==i&&(i="channelsLast"),"channelsLast"!==i&&"channelsFirst"!==i)throw new Qr(`Unknown data format: ${i}. Supported data formats are 'channelsLast' and 'channelsFirst.`);let t;return t="channelsFirst"===i?[[0,0],[0,0],n[0],n[1]]:[[0,0],n[0],n[1],[0,0]],Ot(e,t)});var e,n,i})}getConfig(){const t={padding:this.padding,dataFormat:this.dataFormat},e=super.getConfig();return Object.assign(t,e),t}}function ac(t,e,n,i,r,a){return s(()=>{let s;Ra(r),Oa(a),Ma(i),null==n&&(n=[1,1]),null==i&&(i="valid"),null==r&&(r="channelsLast"),null==a&&(a="max"),t=ih(t,r);const o="same"===i?"same":"valid";return s="max"===a?Bt(t,e,n,o):Pt(t,e,n,o),"channelsFirst"===r&&(s=yt(s,[0,3,1,2])),s})}function oc(t,e,n,i,r,a){return s(()=>{let s;Ra(r),Oa(a),Ma(i),null==n&&(n=[1,1,1]),null==i&&(i="valid"),null==r&&(r="channelsLast"),null==a&&(a="max"),t=rh(t,r);const o="same"===i?"same":"valid";return s="max"===a?Wt(t,e,n,o):Ut(t,e,n,o),"channelsFirst"===r&&(s=yt(s,[0,4,1,2,3])),s})}rc.className="ZeroPadding2D",n.registerClass(rc);class lc extends Qo{constructor(t){if(null==t.poolSize&&(t.poolSize=2),super(t),"number"==typeof t.poolSize)this.poolSize=[t.poolSize];else{if(!Array.isArray(t.poolSize)||1!==t.poolSize.length||"number"!=typeof t.poolSize[0])throw new Qr("poolSize for 1D convolutional layer must be a number or an Array of a single number, but received "+JSON.stringify(t.poolSize));this.poolSize=t.poolSize}if(ya(this.poolSize,"poolSize"),null==t.strides)this.strides=this.poolSize;else if("number"==typeof t.strides)this.strides=[t.strides];else{if(!Array.isArray(t.strides)||1!==t.strides.length||"number"!=typeof t.strides[0])throw new Qr("strides for 1D convolutional layer must be a number or an Array of a single number, but received "+JSON.stringify(t.strides));this.strides=t.strides}ya(this.strides,"strides"),this.padding=null==t.padding?"valid":t.padding,Ma(this.padding),this.inputSpec=[new Ho({ndim:3})]}computeOutputShape(t){const e=nh((t=Ko(t))[1],this.poolSize[0],this.padding,this.strides[0]);return[t[0],e,t[2]]}call(t,e){return s(()=>{this.invokeCallHook(t,e),t=Xa(Uo(t),2);const n=this.poolingFunction(Uo(t),[this.poolSize[0],1],[this.strides[0],1],this.padding,"channelsLast");return Kt(n,[2])})}getConfig(){const t={poolSize:this.poolSize,padding:this.padding,strides:this.strides},e=super.getConfig();return Object.assign(t,e),t}}class uc extends lc{constructor(t){super(t)}poolingFunction(t,e,n,s,i){return Ra(i),Ma(s),ac(t,e,n,s,i,"max")}}uc.className="MaxPooling1D",n.registerClass(uc);class hc extends lc{constructor(t){super(t)}poolingFunction(t,e,n,s,i){return Ra(i),Ma(s),ac(t,e,n,s,i,"avg")}}hc.className="AveragePooling1D",n.registerClass(hc);class cc extends Qo{constructor(t){if(null==t.poolSize&&(t.poolSize=[2,2]),super(t),this.poolSize=Array.isArray(t.poolSize)?t.poolSize:[t.poolSize,t.poolSize],null==t.strides)this.strides=this.poolSize;else if(Array.isArray(t.strides)){if(2!==t.strides.length)throw new Qr("If the strides property of a 2D pooling layer is an Array, it is expected to have a length of 2, but received length "+t.strides.length+".");this.strides=t.strides}else this.strides=[t.strides,t.strides];ya(this.poolSize,"poolSize"),ya(this.strides,"strides"),this.padding=null==t.padding?"valid":t.padding,this.dataFormat=null==t.dataFormat?"channelsLast":t.dataFormat,Ra(this.dataFormat),Ma(this.padding),this.inputSpec=[new Ho({ndim:4})]}computeOutputShape(t){t=Ko(t);let e="channelsFirst"===this.dataFormat?t[2]:t[1],n="channelsFirst"===this.dataFormat?t[3]:t[2];return e=nh(e,this.poolSize[0],this.padding,this.strides[0]),n=nh(n,this.poolSize[1],this.padding,this.strides[1]),"channelsFirst"===this.dataFormat?[t[0],t[1],e,n]:[t[0],e,n,t[3]]}call(t,e){return s(()=>(this.invokeCallHook(t,e),this.poolingFunction(Uo(t),this.poolSize,this.strides,this.padding,this.dataFormat)))}getConfig(){const t={poolSize:this.poolSize,padding:this.padding,strides:this.strides,dataFormat:this.dataFormat},e=super.getConfig();return Object.assign(t,e),t}}class pc extends cc{constructor(t){super(t)}poolingFunction(t,e,n,s,i){return Ra(i),Ma(s),ac(t,e,n,s,i,"max")}}pc.className="MaxPooling2D",n.registerClass(pc);class dc extends cc{constructor(t){super(t)}poolingFunction(t,e,n,s,i){return Ra(i),Ma(s),ac(t,e,n,s,i,"avg")}}dc.className="AveragePooling2D",n.registerClass(dc);class fc extends Qo{constructor(t){if(null==t.poolSize&&(t.poolSize=[2,2,2]),super(t),this.poolSize=Array.isArray(t.poolSize)?t.poolSize:[t.poolSize,t.poolSize,t.poolSize],null==t.strides)this.strides=this.poolSize;else if(Array.isArray(t.strides)){if(3!==t.strides.length)throw new Qr("If the strides property of a 3D pooling layer is an Array, it is expected to have a length of 3, but received length "+t.strides.length+".");this.strides=t.strides}else this.strides=[t.strides,t.strides,t.strides];ya(this.poolSize,"poolSize"),ya(this.strides,"strides"),this.padding=null==t.padding?"valid":t.padding,this.dataFormat=null==t.dataFormat?"channelsLast":t.dataFormat,Ra(this.dataFormat),Ma(this.padding),this.inputSpec=[new Ho({ndim:5})]}computeOutputShape(t){t=Ko(t);let e="channelsFirst"===this.dataFormat?t[2]:t[1],n="channelsFirst"===this.dataFormat?t[3]:t[2],s="channelsFirst"===this.dataFormat?t[4]:t[3];return e=nh(e,this.poolSize[0],this.padding,this.strides[0]),n=nh(n,this.poolSize[1],this.padding,this.strides[1]),s=nh(s,this.poolSize[2],this.padding,this.strides[2]),"channelsFirst"===this.dataFormat?[t[0],t[1],e,n,s]:[t[0],e,n,s,t[4]]}call(t,e){return s(()=>(this.invokeCallHook(t,e),this.poolingFunction(Uo(t),this.poolSize,this.strides,this.padding,this.dataFormat)))}getConfig(){const t={poolSize:this.poolSize,padding:this.padding,strides:this.strides,dataFormat:this.dataFormat},e=super.getConfig();return Object.assign(t,e),t}}class gc extends fc{constructor(t){super(t)}poolingFunction(t,e,n,s,i){return Ra(i),Ma(s),oc(t,e,n,s,i,"max")}}gc.className="MaxPooling3D",n.registerClass(gc);class mc extends fc{constructor(t){super(t)}poolingFunction(t,e,n,s,i){return Ra(i),Ma(s),oc(t,e,n,s,i,"avg")}}mc.className="AveragePooling3D",n.registerClass(mc);class yc extends Qo{constructor(t){super(t),this.inputSpec=[new Ho({ndim:3})]}computeOutputShape(t){return[t[0],t[2]]}call(t,e){throw new ta}}class bc extends yc{constructor(t){super(t||{})}call(t,e){return s(()=>{const e=Uo(t);return j(e,1)})}}bc.className="GlobalAveragePooling1D",n.registerClass(bc);class wc extends yc{constructor(t){super(t||{})}call(t,e){return s(()=>{const e=Uo(t);return c(e,1)})}}wc.className="GlobalMaxPooling1D",n.registerClass(wc);class kc extends Qo{constructor(t){super(t),this.dataFormat=null==t.dataFormat?"channelsLast":t.dataFormat,Ra(this.dataFormat),this.inputSpec=[new Ho({ndim:4})]}computeOutputShape(t){return t=t,"channelsLast"===this.dataFormat?[t[0],t[3]]:[t[0],t[1]]}call(t,e){throw new ta}getConfig(){const t={dataFormat:this.dataFormat},e=super.getConfig();return Object.assign(t,e),t}}class xc extends kc{call(t,e){return s(()=>{const e=Uo(t);return"channelsLast"===this.dataFormat?j(e,[1,2]):j(e,[2,3])})}}xc.className="GlobalAveragePooling2D",n.registerClass(xc);class vc extends kc{call(t,e){return s(()=>{const e=Uo(t);return"channelsLast"===this.dataFormat?c(e,[1,2]):c(e,[2,3])})}}vc.className="GlobalMaxPooling2D",n.registerClass(vc);class Sc extends Qo{constructor(t){super(t),this.layer=t.layer}build(t){this.built=!0}get trainable(){return null!=this.layer&&this.layer.trainable}set trainable(t){null!=this.layer&&(this.layer.trainable=t)}get trainableWeights(){return this.layer.trainableWeights}get nonTrainableWeights(){return this.layer.nonTrainableWeights}get updates(){return this.layer._updates}get losses(){return this.layer.losses}getWeights(){return this.layer.getWeights()}setWeights(t){this.layer.setWeights(t)}getConfig(){const t={layer:{className:this.layer.getClassName(),config:this.layer.getConfig()}},e=super.getConfig();return Object.assign(t,e),t}setFastWeightInitDuringBuild(t){super.setFastWeightInitDuringBuild(t),null!=this.layer&&this.layer.setFastWeightInitDuringBuild(t)}static fromConfig(t,e,n={}){const s=dl(e.layer,n);delete e.layer;const i={layer:s};return Object.assign(i,e),new t(i)}}class Ic extends Sc{constructor(t){super(t),this.supportsMasking=!0}build(t){if((t=Ko(t)).length<3)throw new Qr("TimeDistributed layer expects an input shape >= 3D, but received input shape "+JSON.stringify(t));this.inputSpec=[{shape:t}];const e=[t[0]].concat(t.slice(2));this.layer.built||(this.layer.build(e),this.layer.built=!0),super.build(t)}computeOutputShape(t){const e=[(t=Ko(t))[0]].concat(t.slice(2)),n=this.layer.computeOutputShape(e),s=t[1];return[n[0],s].concat(n.slice(1))}call(t,e){return s(()=>wh((t,n)=>[Uo(this.layer.call(t,e)),[]],t=Uo(t),[],!1,null,null,!1,!0)[1])}}Ic.className="TimeDistributed",n.registerClass(Ic);class Nc extends Sc{constructor(t){super(t);const e=t.layer.getConfig(),n={};n.className=t.layer.getClassName(),n.config=e,this.forwardLayer=dl(n),e.goBackwards=!0!==e.goBackwards;const s={};var i;if(s.className=t.layer.getClassName(),s.config=e,this.backwardLayer=dl(s),this.forwardLayer.name="forward_"+this.forwardLayer.name,this.backwardLayer.name="backward_"+this.backwardLayer.name,this.mergeMode=void 0===t.mergeMode?"concat":t.mergeMode,i=this.mergeMode,ga(_a,"BidirectionalMergeMode",i),t.weights)throw new ta("weights support is not implemented for Bidirectional layer yet.");this._stateful=t.layer.stateful,this.returnSequences=t.layer.returnSequences,this.returnState=t.layer.returnState,this.supportsMasking=!0,this._trainable=!0,this.inputSpec=t.layer.inputSpec,this.numConstants=null}get trainable(){return this._trainable}set trainable(t){this._trainable=t,null!=this.forwardLayer&&(this.forwardLayer.trainable=t),null!=this.backwardLayer&&(this.backwardLayer.trainable=t)}getWeights(){return this.forwardLayer.getWeights().concat(this.backwardLayer.getWeights())}setWeights(t){const e=t.length,n=Math.floor(e/2);this.forwardLayer.setWeights(t.slice(0,n)),this.backwardLayer.setWeights(t.slice(n))}computeOutputShape(t){let e,n,s,i=this.forwardLayer.computeOutputShape(t);return Array.isArray(i)&&Array.isArray(i[0])||(i=[i]),i=i,this.returnState?(s=i.slice(1),e=i[0]):e=i[0],e=e,"concat"===this.mergeMode?(e[e.length-1]*=2,n=[e]):n=null==this.mergeMode?[e,e.slice()]:[e],this.returnState?null==this.mergeMode?n.concat(s).concat(s.slice()):[e].concat(s).concat(s.slice()):ra(n)}apply(t,e){let n=null==e?null:e.initialState,s=null==e?null:e.constants;null==e&&(e={});const i=bh(t,n,s,this.numConstants);if(t=i.inputs,n=i.initialState,s=i.constants,Array.isArray(t)&&(n=t.slice(1),t=t[0]),(null==n||0===n.length)&&null==s)return super.apply(t,e);const r=[],a=[];if(null!=n){const t=n.length;if(t%2>0)throw new Qr("When passing `initialState` to a Bidrectional RNN, the state should be an Array containing the states of the underlying RNNs.");e.initialState=n,r.push(...n);const s=n.map(t=>new Ho({shape:t.shape}));this.forwardLayer.stateSpec=s.slice(0,t/2),this.backwardLayer.stateSpec=s.slice(t/2),a.push(...s)}if(null!=s)throw new ta("Support for constants in Bidirectional layers is not implemented yet.");const o=r[0]instanceof Jo;for(const t of r)if(t instanceof Jo!==o)throw new Qr("The initial state of a Bidirectional layer cannot be specified as a mix of symbolic and non-symbolic tensors");if(o){const n=[t].concat(r),s=this.inputSpec.concat(a),i=this.inputSpec;this.inputSpec=s;const o=super.apply(n,e);return this.inputSpec=i,o}return super.apply(t,e)}call(t,e){return s(()=>{const n=e.initialState;let s,i,r,o;if(null==n)s=this.forwardLayer.call(t,e),i=this.backwardLayer.call(t,e);else{const r=n.slice(0,n.length/2),a=n.slice(n.length/2);s=this.forwardLayer.call(t,Object.assign(e,{initialState:r})),i=this.backwardLayer.call(t,Object.assign(e,{initialState:a}))}return this.returnState&&(Array.isArray(s)&&(r=s.slice(1).concat(i.slice(1))),s=s[0],i=i[0]),this.returnSequences&&(i=It(i,1)),"concat"===this.mergeMode?o=eo([s,i]):"sum"===this.mergeMode?o=u(s,i):"ave"===this.mergeMode?o=a(.5,u(s,i)):"mul"===this.mergeMode?o=a(s,i):null==this.mergeMode&&(o=[s,i]),this.returnState?null==this.mergeMode?o.concat(r):[o].concat(r):o})}resetStates(t){this.forwardLayer.resetStates(),this.backwardLayer.resetStates()}build(t){Pa(this.forwardLayer.name,()=>{this.forwardLayer.build(t)}),Pa(this.backwardLayer.name,()=>{this.backwardLayer.build(t)}),this.built=!0}computeMask(t,e){let n;if(Array.isArray(e)&&(e=e[0]),n=this.returnSequences?null==this.mergeMode?[e,e]:e:null==this.mergeMode?[null,null]:null,this.returnState){const t=this.forwardLayer.states.map(t=>null);return Array.isArray(n)?n.concat(t).concat(t):[n].concat(t).concat(t)}return n}get trainableWeights(){return this.forwardLayer.trainableWeights.concat(this.backwardLayer.trainableWeights)}get nonTrainableWeights(){return this.forwardLayer.nonTrainableWeights.concat(this.backwardLayer.nonTrainableWeights)}setFastWeightInitDuringBuild(t){super.setFastWeightInitDuringBuild(t),null!=this.forwardLayer&&this.forwardLayer.setFastWeightInitDuringBuild(t),null!=this.backwardLayer&&this.backwardLayer.setFastWeightInitDuringBuild(t)}getConfig(){const t={mergeMode:this.mergeMode},e=super.getConfig();return Object.assign(t,e),t}static fromConfig(t,e){const n=dl(e.layer);if(delete e.layer,null!=e.numConstants)throw new ta("Deserialization of a Bidirectional layer with numConstants present is not supported yet.");const s=e;return s.layer=n,new t(s)}}function Ac(t){return new hc(t)}function Cc(t){return new dc(t)}function zc(t){return new mc(t)}function Dc(t){return new wc(t)}function Tc(t){return new vc(t)}function Ec(t){return new uc(t)}function Fc(t){return new pc(t)}Nc.className="Bidirectional",n.registerClass(Nc);const $c=Dc,_c=Tc,Lc=Ec,Rc=Fc;var Mc=Object.freeze({__proto__:null,inputLayer:function(t){return new tl(t)},elu:function(t){return new Yu(t)},reLU:function(t){return new Ju(t)},leakyReLU:function(t){return new Zu(t)},prelu:function(t){return new Xu(t)},softmax:function(t){return new th(t)},thresholdedReLU:function(t){return new Qu(t)},conv1d:function(t){return new fh(t)},conv2d:function(t){return new uh(t)},conv2dTranspose:function(t){return new ch(t)},conv3d:function(t){return new hh(t)},separableConv2d:function(t){return new dh(t)},cropping2D:function(t){return new gh(t)},upSampling2d:function(t){return new mh(t)},depthwiseConv2d:function(t){return new yh(t)},activation:function(t){return new Mh(t)},dense:function(t){return new Lh(t)},dropout:function(t){return new $h(t)},spatialDropout1d:function(t){return new _h(t)},flatten:function(t){return new Rh(t)},repeatVector:function(t){return new Oh(t)},reshape:function(t){return new Bh(t)},permute:function(t){return new Ph(t)},embedding:function(t){return new Uh(t)},add:function(t){return new jh(t)},average:function(t){return new qh(t)},concatenate:function(t){return new Jh(t)},maximum:function(t){return new Gh(t)},minimum:function(t){return new Hh(t)},multiply:function(t){return new Vh(t)},dot:function(t){return new Xh(t)},batchNormalization:function(t){return new sc(t)},layerNormalization:function(t){return new ic(t)},zeroPadding2d:function(t){return new rc(t)},averagePooling1d:Ac,avgPool1d:function(t){return Ac(t)},avgPooling1d:function(t){return Ac(t)},averagePooling2d:Cc,avgPool2d:function(t){return Cc(t)},avgPooling2d:function(t){return Cc(t)},averagePooling3d:zc,avgPool3d:function(t){return zc(t)},avgPooling3d:function(t){return zc(t)},globalAveragePooling1d:function(t){return new bc(t)},globalAveragePooling2d:function(t){return new xc(t)},globalMaxPooling1d:Dc,globalMaxPooling2d:Tc,maxPooling1d:Ec,maxPooling2d:Fc,maxPooling3d:function(t){return new gc(t)},gru:function(t){return new Nh(t)},gruCell:function(t){return new Ih(t)},lstm:function(t){return new Ch(t)},lstmCell:function(t){return new Ah(t)},simpleRNN:function(t){return new Sh(t)},simpleRNNCell:function(t){return new vh(t)},convLstm2d:function(t){return new Fh(t)},convLstm2dCell:function(t){return new Eh(t)},rnn:function(t){return new kh(t)},stackedRNNCells:function(t){return new zh(t)},bidirectional:function(t){return new Nc(t)},timeDistributed:function(t){return new Ic(t)},globalMaxPool1d:$c,globalMaxPool2d:_c,maxPool1d:Lc,maxPool2d:Rc,Layer:Qo,RNN:kh,RNNCell:xh,input:Su,gaussianNoise:function(t){return new Yh(t)},gaussianDropout:function(t){return new Qh(t)},alphaDropout:function(t){return new tc(t)},masking:function(t){return new Wh(t)}});var Oc=Object.freeze({__proto__:null,binaryAccuracy:function(t,e){return Il(t,e)},binaryCrossentropy:function(t,e){return Dl(t,e)},sparseCategoricalAccuracy:function(t,e){return Tl(t,e)},categoricalAccuracy:function(t,e){return Nl(t,e)},categoricalCrossentropy:function(t,e){return El(t,e)},precision:function(t,e){return Cl(t,e)},recall:function(t,e){return zl(t,e)},cosineProximity:function(t,e){return xl(t,e)},meanAbsoluteError:function(t,e){return ml(t,e)},meanAbsolutePercentageError:function(t,e){return yl(t,e)},MAPE:function(t,e){return yl(t,e)},mape:function(t,e){return yl(t,e)},meanSquaredError:function(t,e){return gl(t,e)},MSE:function(t,e){return gl(t,e)},mse:function(t,e){return gl(t,e)}}),Bc=Object.freeze({__proto__:null,modelFromJSON:async function(t,e){"modelTopology"in t||(t={modelTopology:t});let n=(t=t).modelTopology;null!=n.model_config&&(n=n.model_config);const s=dl(Ul(n),e);if(null!=t.weightsManifest){const e=await ut.loadWeights(t.weightsManifest,t.pathPrefix,s.weights.map(t=>t.originalName)),n={};for(const t of s.weights)n[t.originalName]=e[t.originalName];s.loadWeights(n),B(e)}return s}});var Pc=Object.freeze({__proto__:null,l1l2:function(t){return new ju(t)},l1:function(t){return Uu(e=t),new ju({l1:null!=e?e.l1:null,l2:0});var e},l2:function(t){return Uu(e=t),new ju({l2:null!=e?e.l2:null,l1:0});var e}});class Wc extends rl{constructor(){super(...arguments),this.model=null}setModel(t){if(!(t instanceof mu))throw new Error("model must be a LayersModel, not some other Container");this.model=t}}function Uc(t,e){return t<e}function Kc(t,e){return t>e}class jc extends Wc{constructor(t){if(super(),null==t&&(t={}),t.restoreBestWeights)throw new ta("restoreBestWeights = True is not implemented in EarlyStopping yet.");this.monitor=t.monitor||"val_loss",this.minDelta=Math.abs(t.minDelta||0),this.patience=t.patience||0,this.verbose=t.verbose||0,this.mode=t.mode||"auto",this.baseline=t.baseline,-1===["auto","min","max"].indexOf(this.mode)&&(console.warn(`EarlyStopping mode '${this.mode}' is invalid. Falling back to mode 'auto'.`),this.mode="auto"),"min"===this.mode?this.monitorFunc=Uc:"max"===this.mode||-1!==this.monitor.indexOf("acc")?this.monitorFunc=Kc:this.monitorFunc=Uc,this.monitorFunc===Uc&&(this.minDelta*=-1)}async onTrainBegin(t){this.wait=0,this.stoppedEpoch=0,null!=this.baseline?this.best=this.baseline:this.best=this.monitorFunc===Uc?1/0:-1/0}async onEpochEnd(t,e){await nl(e);const n=this.getMonitorValue(e);null!=n&&(this.monitorFunc(n-this.minDelta,this.best)?(this.best=n,this.wait=0):(this.wait++,this.wait>=this.patience&&(this.stoppedEpoch=t,this.model.stopTraining=!0)))}async onTrainEnd(t){this.stoppedEpoch>0&&this.verbose&&console.log(`Epoch ${this.stoppedEpoch}: early stopping.`)}getMonitorValue(t){null==t&&(t={});const e=t[this.monitor];return null==e&&console.warn(`Metric for EarlyStopping ${this.monitor} is not available. Available metrics are: `+Object.keys(t)),e}}const Vc={earlyStopping:function(t){return new jc(t)}};export{Wc as Callback,al as CallbackList,ul as CustomCallback,jc as EarlyStopping,ll as History,Ho as InputSpec,Vo as LayerVariable,mu as LayersModel,kh as RNN,wu as Sequential,Jo as SymbolicTensor,Vc as callbacks,Da as constraints,Lo as initializers,Su as input,Mc as layers,vu as loadLayersModel,Oc as metrics,ku as model,Bc as models,Iu as registerCallbackConstructor,Pc as regularizers,xu as sequential,Kl as version_layers};
//# sourceMappingURL=tf-layers.fesm.min.js.map

/**
 * @license
 * Copyright 2021 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
!function(t,e){"object"==typeof exports&&"undefined"!=typeof module?e(exports,require("@tensorflow/tfjs-core")):"function"==typeof define&&define.amd?define(["exports","@tensorflow/tfjs-core"],e):e((t=t||self).tf=t.tf||{},t.tf)}(this,(function(t,e){"use strict";function n(t){throw new Error(`'${t}' not yet implemented or not found in the registry. This kernel may not be supported by the tfjs backend you have chosen`)}function s(t,e){if(!t)throw new Error("string"==typeof e?e:e())}function i(t,e,n=""){s(o(t,e),()=>n+` Shapes ${t} and ${e} must match`)}function r(t,e=[],n=!1){if(null==e&&(e=[]),Array.isArray(t)||c(t)&&!n)for(let s=0;s<t.length;++s)r(t[s],e,n);else e.push(t);return e}function a(t){if(0===t.length)return 1;let e=t[0];for(let n=1;n<t.length;n++)e*=t[n];return e}function o(t,e){if(t===e)return!0;if(null==t||null==e)return!1;if(t.length!==e.length)return!1;for(let n=0;n<t.length;n++)if(t[n]!==e[n])return!1;return!0}function l(t){return t%1==0}function u(t,e){return e<=t.length?t:t+" ".repeat(e-t.length)}function h(t,e){const n=e.length;return s((t=null==t?e.map((t,e)=>e):[].concat(t)).every(t=>t>=-n&&t<n),()=>`All values in axis param must be in range [-${n}, ${n}) but got axis `+t),s(t.every(t=>l(t)),()=>"All values in axis param must be integers but got axis "+t),t.map(t=>t<0?n+t:t)}function c(t){return t instanceof Float32Array||t instanceof Int32Array||t instanceof Uint8Array}function p(t){return"string"==typeof t||t instanceof String}function d(t){return Array.isArray(t)?d(t[0]):t instanceof Float32Array?"float32":t instanceof Int32Array||t instanceof Uint8Array?"int32":"number"==typeof t?"float32":p(t)?"string":function(t){return"boolean"==typeof t}(t)?"bool":"float32"}function f(t){return!!(t&&t.constructor&&t.call&&t.apply)}function g(t){const e=t.length;if(e<2)return[];const n=new Array(e-1);n[e-2]=t[e-1];for(let s=e-3;s>=0;--s)n[s]=n[s+1]*t[s+1];return n}function m(t,e){if(0===t.length)return e[0];const n=t.reduce((t,e)=>t*e);if(0===n)return[];if(n!==e.length)throw new Error(`[${t}] does not match the input size ${e.length}.`);return function t(e,n,s){const i=new Array;if(1===n.length){const t=n[0];for(let n=0;n<t;n++)i[n]=s[e+n]}else{const r=n[0],a=n.slice(1),o=a.reduce((t,e)=>t*e);for(let n=0;n<r;n++)i[n]=t(e+n*o,a,s)}return i}(0,t,e)}function y(t,e){const n=b(t,e);for(let t=0;t<n.length;t++)n[t]=1;return n}function b(t,e){if(null==e||"float32"===e||"complex64"===e)return new Float32Array(t);if("int32"===e)return new Int32Array(t);if("bool"===e)return new Uint8Array(t);throw new Error("Unknown data type "+e)}function w(t){return t&&t.then&&"function"==typeof t.then}class k{constructor(t){this.global=t,this.flags={},this.flagRegistry={},this.urlFlags={},this.populateURLFlags()}setPlatform(t,e){null!=this.platform&&console.warn(`Platform ${this.platformName} has already been set. Overwriting the platform with ${e}.`),this.platformName=t,this.platform=e}registerFlag(t,e,n){if(this.flagRegistry[t]={evaluationFn:e,setHook:n},null!=this.urlFlags[t]){const e=this.urlFlags[t];console.warn(`Setting feature override from URL ${t}: ${e}.`),this.set(t,e)}}async getAsync(t){return t in this.flags||(this.flags[t]=await this.evaluateFlag(t)),this.flags[t]}get(t){if(t in this.flags)return this.flags[t];const e=this.evaluateFlag(t);if(w(e))throw new Error(`Flag ${t} cannot be synchronously evaluated. Please use getAsync() instead.`);return this.flags[t]=e,this.flags[t]}getNumber(t){return this.get(t)}getBool(t){return this.get(t)}getFlags(){return this.flags}get features(){return this.flags}set(t,e){if(null==this.flagRegistry[t])throw new Error(`Cannot set flag ${t} as it has not been registered.`);this.flags[t]=e,null!=this.flagRegistry[t].setHook&&this.flagRegistry[t].setHook(e)}evaluateFlag(t){if(null==this.flagRegistry[t])throw new Error(`Cannot evaluate flag '${t}': no evaluation function found.`);return this.flagRegistry[t].evaluationFn()}setFlags(t){this.flags=Object.assign({},t)}reset(){this.flags={},this.urlFlags={},this.populateURLFlags()}populateURLFlags(){if(void 0===this.global||void 0===this.global.location||void 0===this.global.location.search)return;const t=function(t){const e={};return t.replace(/[?&]([^=?&]+)(?:=([^&]*))?/g,(t,...n)=>(function(t,e,n){t[decodeURIComponent(e)]=decodeURIComponent(n||"")}(e,n[0],n[1]),n.join("="))),e}(this.global.location.search);if("tfjsflags"in t){t.tfjsflags.split(",").forEach(t=>{const[e,n]=t.split(":");this.urlFlags[e]=function(t,e){if("true"===(e=e.toLowerCase())||"false"===e)return"true"===e;if(""+ +e===e)return+e;throw new Error(`Could not parse value flag value ${e} for flag ${t}.`)}(e,n)})}}}function x(){return S}let v,S=null;function I(){if(null==v){let t;if("undefined"!=typeof window)t=window;else if("undefined"!=typeof global)t=global;else if("undefined"!=typeof process)t=process;else{if("undefined"==typeof self)throw new Error("Could not find a global object");t=self}v=t}return v}function N(t,e){const n=function(){const t=I();return null==t._tfGlobals&&(t._tfGlobals=new Map),t._tfGlobals}();if(n.has(t))return n.get(t);{const s=e();return n.set(t,s),n.get(t)}}const z=N("kernelRegistry",()=>new Map),A=N("gradRegistry",()=>new Map);function C(t,e){const n=function(t,e){return`${e}_${t}`}(t,e);return z.get(n)}function D(t){return A.get(t)}function T(t){const e=z.entries(),n=[];for(;;){const{done:s,value:i}=e.next();if(s)break;const[r,a]=i,[o]=r.split("_");o===t&&n.push(a)}return n}function E(t){const{kernelName:e}=t;A.has(e)&&x().getBool("DEBUG")&&console.warn(`Overriding the gradient for '${e}'`),A.set(e,t)}function F(t,e){if("string"===e)throw new Error("Cannot convert a string[] to a TypedArray");if(Array.isArray(t)&&(t=r(t)),x().getBool("DEBUG")&&function(t,e){for(let n=0;n<t.length;n++){const s=t[n];if(isNaN(s)||!isFinite(s))throw Error(`A tensor of type ${e} being uploaded contains ${s}.`)}}(t,e),function(t,e){return t instanceof Float32Array&&"float32"===e||t instanceof Int32Array&&"int32"===e||t instanceof Uint8Array&&"bool"===e}(t,e))return t;if(null==e||"float32"===e||"complex64"===e)return new Float32Array(t);if("int32"===e)return new Int32Array(t);if("bool"===e){const e=new Uint8Array(t.length);for(let n=0;n<e.length;++n)0!==Math.round(t[n])&&(e[n]=1);return e}throw new Error("Unknown data type "+e)}function L(){return x().platform.now()}function $(t,e="utf-8"){return e=e||"utf-8",x().platform.decode(t,e)}class _{constructor(t,e){this.backendTimer=t,this.logger=e,null==e&&(this.logger=new M)}profileKernel(t,e,n){let s;const i=this.backendTimer.time(()=>{s=n()});if(x().getBool("CHECK_COMPUTATION_FOR_ERRORS"))for(let e=0;e<s.length;e++){const n=s[e];n.data().then(e=>{R(e,n.dtype,t)})}return{kernelName:t,outputs:s,inputs:e,timeMs:i.then(t=>t.kernelMs),extraInfo:i.then(t=>null!=t.getExtraProfileInfo?t.getExtraProfileInfo():"")}}logKernelProfile(t){const{kernelName:e,outputs:n,timeMs:s,inputs:i,extraInfo:r}=t;n.forEach(t=>{Promise.all([t.data(),s,r]).then(n=>{this.logger.logKernelProfile(e,t,n[0],n[1],i,n[2])})})}}function R(t,e,n){if("float32"!==e)return!1;for(let e=0;e<t.length;e++){const s=t[e];if(isNaN(s)||!isFinite(s))return console.warn(`Found ${s} in the result of '${n}'`),!0}return!1}class M{logKernelProfile(t,e,n,s,i,r){const a="number"==typeof s?u(s+"ms",9):s.error,o=u(t,25),l=e.rank,h=e.size,c=u(e.shape.toString(),14);let p="";for(const t in i){const n=i[t];if(null!=n){const s=n.shape||e.shape,i=s.length;p+=`${t}: ${i}D ${i>0?s:""} `}}console.log(`%c${o}\t%c${a}\t%c${l}D ${c}\t%c${h}\t%c${p}\t%c${r}`,"font-weight:bold","color:red","color:blue","color: orange","color: green","color: steelblue")}}function O(t,e,n,s){const i=g(e),r=function(t,e,n,s){const i=a(e),r=s[s.length-1],o=new Array(r).fill(0),l=e.length,u="complex64"===n?W(t):t;if(l>1)for(let t=0;t<i/r;t++){const e=t*r;for(let t=0;t<r;t++)o[t]=Math.max(o[t],B(u[e+t],0,n).length)}return o}(t,e,n,i),o=e.length,l=function t(e,n,s,i,r,a=!0){const o="complex64"===s?2:1,l=n[0],u=n.length;if(0===u){if("complex64"===s){return[B(W(e)[0],0,s)]}return"bool"===s?[P(e[0])]:[e[0].toString()]}if(1===u){if(l>20){const t=3*o;let n=Array.from(e.slice(0,t)),i=Array.from(e.slice((l-3)*o,l*o));return"complex64"===s&&(n=W(n),i=W(i)),["["+n.map((t,e)=>B(t,r[e],s)).join(", ")+", ..., "+i.map((t,e)=>B(t,r[l-3+e],s)).join(", ")+"]"]}return["["+("complex64"===s?W(e):Array.from(e)).map((t,e)=>B(t,r[e],s)).join(", ")+"]"]}const h=n.slice(1),c=i.slice(1),p=i[0]*o,d=[];if(l>20){for(let n=0;n<3;n++){const i=n*p,a=i+p;d.push(...t(e.slice(i,a),h,s,c,r,!1))}d.push("...");for(let n=l-3;n<l;n++){const i=n*p,a=i+p;d.push(...t(e.slice(i,a),h,s,c,r,n===l-1))}}else for(let n=0;n<l;n++){const i=n*p,a=i+p;d.push(...t(e.slice(i,a),h,s,c,r,n===l-1))}const f=2===u?",":"";d[0]="["+d[0]+f;for(let t=1;t<d.length-1;t++)d[t]=" "+d[t]+f;let g=",\n";for(let t=2;t<u;t++)g+="\n";return d[d.length-1]=" "+d[d.length-1]+"]"+(a?"":g),d}(t,e,n,i,r),u=["Tensor"];return s&&(u.push("  dtype: "+n),u.push("  rank: "+o),u.push(`  shape: [${e}]`),u.push("  values:")),u.push(l.map(t=>"    "+t).join("\n")),u.join("\n")}function B(t,e,n){let s;return s=Array.isArray(t)?parseFloat(t[0].toFixed(7))+" + "+parseFloat(t[1].toFixed(7))+"j":p(t)?`'${t}'`:"bool"===n?P(t):parseFloat(t.toFixed(7)).toString(),u(s,e)}function P(t){return 0===t?"false":"true"}function W(t){const e=[];for(let n=0;n<t.length;n+=2)e.push([t[n],t[n+1]]);return e}let U=null;class K{constructor(t,e,n,s){this.kept=!1,this.isDisposedInternal=!1,this.shape=t.slice(),this.dtype=e||"float32",this.size=a(t),this.strides=g(t),this.dataId=n,this.id=s,this.rankType=this.rank<5?this.rank.toString():"higher"}get rank(){return this.shape.length}async buffer(){const t=await this.data();return null.buffer(this.shape,this.dtype,t)}bufferSync(){return null.buffer(this.shape,this.dtype,this.dataSync())}async array(){const t=await this.data();return m(this.shape,t)}arraySync(){return m(this.shape,this.dataSync())}async data(){this.throwIfDisposed();const t=U().read(this.dataId);if("string"===this.dtype){const e=await t;try{return e.map(t=>$(t))}catch(t){throw new Error("Failed to decode the string bytes into utf-8. To get the original bytes, call tensor.bytes().")}}return t}dataSync(){this.throwIfDisposed();const t=U().readSync(this.dataId);if("string"===this.dtype)try{return t.map(t=>$(t))}catch(t){throw new Error("Failed to decode the string bytes into utf-8. To get the original bytes, call tensor.bytes().")}return t}async bytes(){this.throwIfDisposed();const t=await U().read(this.dataId);return"string"===this.dtype?t:new Uint8Array(t.buffer)}dispose(){this.isDisposed||(U().disposeTensor(this),this.isDisposedInternal=!0)}get isDisposed(){return this.isDisposedInternal}throwIfDisposed(){if(this.isDisposed)throw new Error("Tensor is disposed.")}print(t=!1){return null.print(this,t)}clone(){return this.throwIfDisposed(),null.clone(this)}toString(t=!1){return O(this.dataSync(),this.shape,this.dtype,t)}cast(t){return this.throwIfDisposed(),null.cast(this,t)}variable(t=!0,e,n){return this.throwIfDisposed(),U().makeVariable(this,t,e,n)}}function V(){return N("Tensor",()=>K)}Object.defineProperty(K,Symbol.hasInstance,{value:t=>!!t&&null!=t.data&&null!=t.dataSync&&null!=t.throwIfDisposed}),V();class j extends K{constructor(t,e,n,s){super(t.shape,t.dtype,t.dataId,s),this.trainable=e,this.name=n}assign(t){if(t.dtype!==this.dtype)throw new Error(`dtype of the new value (${t.dtype}) and previous value (${this.dtype}) must match`);if(!o(t.shape,this.shape))throw new Error(`shape of the new value (${t.shape}) and previous value (${this.shape}) must match`);U().disposeTensor(this),this.dataId=t.dataId,U().incRef(this,null)}dispose(){U().disposeVariable(this),this.isDisposedInternal=!0}}var q,G,H,J,Z;Object.defineProperty(j,Symbol.hasInstance,{value:t=>t instanceof K&&null!=t.assign&&t.assign instanceof Function}),function(t){t.R0="R0",t.R1="R1",t.R2="R2",t.R3="R3",t.R4="R4",t.R5="R5",t.R6="R6"}(q||(q={})),function(t){t.float32="float32",t.int32="int32",t.bool="int32",t.complex64="complex64"}(G||(G={})),function(t){t.float32="float32",t.int32="int32",t.bool="bool",t.complex64="complex64"}(H||(H={})),function(t){t.float32="float32",t.int32="float32",t.bool="float32",t.complex64="complex64"}(J||(J={})),function(t){t.float32="complex64",t.int32="complex64",t.bool="complex64",t.complex64="complex64"}(Z||(Z={}));const X={float32:J,int32:G,bool:H,complex64:Z};function Y(t,e){if(t.dtype===e.dtype)return[t,e];const n=function(t,e){if("string"===t||"string"===e){if("string"===t&&"string"===e)return"string";throw new Error(`Can not upcast ${t} with ${e}`)}return X[t][e]}(t.dtype,e.dtype);return[t.cast(n),e.cast(n)]}function Q(t){const e=[];return function t(e,n,s){if(null==e)return;if(e instanceof K)return void n.push(e);if(i=e,!Array.isArray(i)&&"object"!=typeof i)return;var i;const r=e;for(const e in r){const i=r[e];s.has(i)||(s.add(i),t(i,n,s))}}(t,e,new Set),e}function tt(t){return null!=t.kernelName}class et{constructor(){this.registeredVariables={},this.nextTapeNodeId=0,this.numBytes=0,this.numTensors=0,this.numStringTensors=0,this.numDataBuffers=0,this.gradientDepth=0,this.kernelDepth=0,this.scopeStack=[],this.numDataMovesStack=[],this.nextScopeId=0,this.tensorInfo=new WeakMap,this.profiling=!1,this.activeProfile={newBytes:0,newTensors:0,peakBytes:0,kernels:[],result:null,get kernelNames(){return Array.from(new Set(this.kernels.map(t=>t.name)))}}}dispose(){for(const t in this.registeredVariables)this.registeredVariables[t].dispose()}}class nt{constructor(t){this.ENV=t,this.registry={},this.registryFactory={},this.pendingBackendInitId=0,this.state=new et}async ready(){if(null!=this.pendingBackendInit)return this.pendingBackendInit.then(()=>{});if(null!=this.backendInstance)return;const t=this.getSortedBackends();for(let e=0;e<t.length;e++){const n=t[e];if(await this.initializeBackend(n).success)return void await this.setBackend(n)}throw new Error("Could not initialize any backends, all backend initializations failed.")}get backend(){if(null!=this.pendingBackendInit)throw new Error(`Backend '${this.backendName}' has not yet been initialized. Make sure to await tf.ready() or await tf.setBackend() before calling other methods`);if(null==this.backendInstance){const{name:t,asyncInit:e}=this.initializeBackendsAndReturnBest();if(e)throw new Error(`The highest priority backend '${t}' has not yet been initialized. Make sure to await tf.ready() or await tf.setBackend() before calling other methods`);this.setBackend(t)}return this.backendInstance}backendNames(){return Object.keys(this.registryFactory)}findBackend(t){if(!(t in this.registry)){if(!(t in this.registryFactory))return null;{const{asyncInit:e}=this.initializeBackend(t);if(e)return null}}return this.registry[t]}findBackendFactory(t){return t in this.registryFactory?this.registryFactory[t].factory:null}registerBackend(t,e,n=1){return t in this.registryFactory?(console.warn(t+" backend was already registered. Reusing existing backend factory."),!1):(this.registryFactory[t]={factory:e,priority:n},!0)}async setBackend(t){if(null==this.registryFactory[t])throw new Error(`Backend name '${t}' not found in registry`);if(this.backendName=t,null==this.registry[t]){this.backendInstance=null;const{success:e,asyncInit:n}=this.initializeBackend(t);if(!(n?await e:e))return!1}return this.backendInstance=this.registry[t],this.setupRegisteredKernels(),this.profiler=new _(this.backendInstance),!0}setupRegisteredKernels(){T(this.backendName).forEach(t=>{null!=t.setupFunc&&t.setupFunc(this.backendInstance)})}disposeRegisteredKernels(t){T(t).forEach(e=>{null!=e.disposeFunc&&e.disposeFunc(this.registry[t])})}initializeBackend(t){const e=this.registryFactory[t];if(null==e)throw new Error(`Cannot initialize backend ${t}, no registration found.`);try{const s=e.factory();if(!s||s instanceof class{decComplexRef(t){}time(t){return n("time")}read(t){return n("read")}readSync(t){return n("readSync")}numDataIds(){return n("numDataIds")}disposeData(t){return n("disposeData")}write(t,e,s){return n("write")}move(t,e,s,i){return n("move")}memory(){return n("memory")}floatPrecision(){return n("floatPrecision")}epsilon(){return 32===this.floatPrecision()?1e-7:1e-4}dispose(){return n("dispose")}}||"function"!=typeof s.then)return this.registry[t]=s,{success:!0,asyncInit:!1};{const e=++this.pendingBackendInitId,n=s.then(n=>!(e<this.pendingBackendInitId)&&(this.registry[t]=n,this.pendingBackendInit=null,!0)).catch(n=>(e<this.pendingBackendInitId||(this.pendingBackendInit=null,console.warn(`Initialization of backend ${t} failed`),console.warn(n.stack||n.message)),!1));return this.pendingBackendInit=n,{success:n,asyncInit:!0}}}catch(e){return console.warn(`Initialization of backend ${t} failed`),console.warn(e.stack||e.message),{success:!1,asyncInit:!1}}}removeBackend(t){if(!(t in this.registryFactory))throw new Error(t+" backend not found in registry");this.backendName===t&&null!=this.pendingBackendInit&&this.pendingBackendInitId++,t in this.registry&&(this.disposeRegisteredKernels(t),this.registry[t].dispose(),delete this.registry[t]),delete this.registryFactory[t],this.backendName===t&&(this.pendingBackendInit=null,this.backendName=null,this.backendInstance=null)}getSortedBackends(){if(0===Object.keys(this.registryFactory).length)throw new Error("No backend found in registry.");return Object.keys(this.registryFactory).sort((t,e)=>this.registryFactory[e].priority-this.registryFactory[t].priority)}initializeBackendsAndReturnBest(){const t=this.getSortedBackends();for(let e=0;e<t.length;e++){const n=t[e],{success:s,asyncInit:i}=this.initializeBackend(n);if(i||s)return{name:n,asyncInit:i}}throw new Error("Could not initialize any backends, all backend initializations failed.")}moveData(t,e){const n=this.state.tensorInfo.get(e),s=n.backend,i=this.readSync(e);s.disposeData(e),n.backend=t,t.move(e,i,n.shape,n.dtype),this.shouldCheckForMemLeaks()&&this.state.numDataMovesStack[this.state.numDataMovesStack.length-1]++}tidy(t,e){let n,s=null;if(null==e){if("function"!=typeof t)throw new Error("Please provide a function to tidy()");e=t}else{if("string"!=typeof t&&!(t instanceof String))throw new Error("When calling with two arguments, the first argument to tidy() must be a string");if("function"!=typeof e)throw new Error("When calling with two arguments, the 2nd argument to tidy() must be a function");s=t}return this.scopedRun(()=>this.startScope(s),()=>this.endScope(n),()=>(n=e(),n instanceof Promise&&console.error("Cannot return a Promise inside of tidy."),n))}scopedRun(t,e,n){t();try{const t=n();return e(),t}catch(t){throw e(),t}}nextTensorId(){return nt.nextTensorId++}nextVariableId(){return nt.nextVariableId++}clone(t){const e=this.makeTensorFromDataId(t.dataId,t.shape,t.dtype),n={x:t};return this.addTapeNode(this.state.activeScope.name,n,[e],t=>({x:()=>{const e={x:t},n={dtype:"float32"};return st.runKernel("Cast",e,n)}}),[],{}),e}runKernel(t,e,n){if(!(null!=C(t,this.backendName)))throw new Error(`Kernel '${t}' not registered for backend '${this.backendName}'`);return this.runKernelFunc({kernelName:t,inputs:e,attrs:n})}shouldCheckForMemLeaks(){return this.ENV.getBool("IS_TEST")}checkKernelForMemLeak(t,e,n){const s=this.backend.numDataIds();let i=0;n.forEach(t=>{i+="complex64"===t.dtype?3:1});const r=this.state.numDataMovesStack[this.state.numDataMovesStack.length-1],a=s-e-i-r;if(a>0)throw new Error(`Backend '${this.backendName}' has an internal memory leak (${a} data ids) after running '${t}'`)}runKernelFunc(t){let e,n=[];const i=this.isTapeOn(),r=this.state.numBytes,a=this.state.numTensors;let o,l;this.shouldCheckForMemLeaks()&&this.state.numDataMovesStack.push(0),null==this.backendName&&this.backend;const u=tt(t)?t.kernelName:null!=this.state.activeScope?this.state.activeScope.name:"";if(tt(t)){const{kernelName:e,inputs:r,attrs:a}=t;null==this.backendName&&this.backend;const u=C(e,this.backendName);s(null!=u,()=>`Cannot find registered kernel '${e}' for backend '${this.backendName}'`),o=()=>{const t=this.backend.numDataIds();l=u.kernelFunc({inputs:r,attrs:a,backend:this.backend});const s=Array.isArray(l)?l:[l];this.shouldCheckForMemLeaks()&&this.checkKernelForMemLeak(e,t,s);const o=s.map(t=>{if(null!=t.rank)return t;const{dataId:e,shape:n,dtype:s}=t;return this.makeTensorFromDataId(e,n,s)});if(i){const t=this.getTensorsForGradient(e,r,o);n=this.saveTensorsForBackwardMode(t)}return o}}else{const{forwardFunc:e}=t,s=t=>{i&&(n=t.map(t=>this.keep(this.clone(t))))};o=()=>{const t=this.backend.numDataIds();l=this.tidy(()=>e(this.backend,s));const n=Array.isArray(l)?l:[l];return this.shouldCheckForMemLeaks()&&this.checkKernelForMemLeak(u,t,n),n}}const{inputs:h,attrs:c}=t,p=tt(t)?null:t.backwardsFunc;let d;return this.scopedRun(()=>this.state.kernelDepth++,()=>this.state.kernelDepth--,()=>{this.ENV.getBool("DEBUG")||this.state.profiling?(d=this.profiler.profileKernel(u,h,()=>o()),this.ENV.getBool("DEBUG")&&this.profiler.logKernelProfile(d),e=d.outputs):e=o()}),i&&this.addTapeNode(u,h,e,p,n,c),this.state.profiling&&this.state.activeProfile.kernels.push({name:u,bytesAdded:this.state.numBytes-r,totalBytesSnapshot:this.state.numBytes,tensorsAdded:this.state.numTensors-a,totalTensorsSnapshot:this.state.numTensors,inputShapes:Object.keys(h).map(t=>null!=h[t]?h[t].shape:null),outputShapes:e.map(t=>t.shape),kernelTimeMs:d.timeMs,extraInfo:d.extraInfo}),Array.isArray(l)?e:e[0]}saveTensorsForBackwardMode(t){return t.map(t=>this.keep(this.clone(t)))}getTensorsForGradient(t,e,n){const i=D(t);if(null!=i){const t=i.inputsToSave||[],r=i.outputsToSave||[];let a;i.saveAllInputs?(s(Array.isArray(e),()=>"saveAllInputs is true, expected inputs to be an array."),a=Object.keys(e).map(t=>e[t])):a=t.map(t=>e[t]);const o=n.filter((t,e)=>r[e]);return a.concat(o)}return[]}makeTensor(t,e,n,s){if(null==t)throw new Error("Values passed to engine.makeTensor() are null");n=n||"float32",s=s||this.backend;let i=t;"string"===n&&p(t[0])&&(i=t.map(t=>function(t,e="utf-8"){return e=e||"utf-8",x().platform.encode(t,e)}(t)));const r=s.write(i,e,n),a=new K(e,n,r,this.nextTensorId());if(this.incRef(a,s),"string"===n){const t=this.state.tensorInfo.get(r),e=function(t){if(null==t)return 0;let e=0;return t.forEach(t=>e+=t.length),e}(i);this.state.numBytes+=e-t.bytes,t.bytes=e}return a}makeTensorFromDataId(t,e,n,s){const i=new K(e,n=n||"float32",t,this.nextTensorId());return this.incRef(i,s),i}makeVariable(t,e=!0,n,s){n=n||this.nextVariableId().toString(),null!=s&&s!==t.dtype&&(t=t.cast(s));const i=new j(t,e,n,this.nextTensorId());if(null!=this.state.registeredVariables[i.name])throw new Error(`Variable with name ${i.name} was already registered`);return this.state.registeredVariables[i.name]=i,this.incRef(i,this.backend),i}incRef(t,e){const n=this.state.tensorInfo.has(t.dataId)?this.state.tensorInfo.get(t.dataId).refCount:0;if(this.state.numTensors++,"string"===t.dtype&&this.state.numStringTensors++,0===n){this.state.numDataBuffers++;let n=0;"complex64"!==t.dtype&&"string"!==t.dtype&&(n=t.size*function(t){if("float32"===t||"int32"===t)return 4;if("complex64"===t)return 8;if("bool"===t)return 1;throw new Error("Unknown dtype "+t)}(t.dtype)),this.state.tensorInfo.set(t.dataId,{backend:e||this.backend,dtype:t.dtype,shape:t.shape,bytes:n,refCount:0}),this.state.numBytes+=n}this.state.tensorInfo.get(t.dataId).refCount++,t instanceof j||this.track(t)}disposeTensor(t){if(!this.state.tensorInfo.has(t.dataId))return;this.state.numTensors--,"string"===t.dtype&&this.state.numStringTensors--;const e=this.state.tensorInfo.get(t.dataId);e.refCount<=1?("complex64"!==t.dtype&&(this.state.numBytes-=e.bytes),this.state.numDataBuffers--,e.backend.disposeData(t.dataId),this.state.tensorInfo.delete(t.dataId)):(e.backend.decComplexRef(t.dataId),this.state.tensorInfo.get(t.dataId).refCount--)}disposeVariables(){for(const t in this.state.registeredVariables){const e=this.state.registeredVariables[t];this.disposeVariable(e)}}disposeVariable(t){this.disposeTensor(t),null!=this.state.registeredVariables[t.name]&&delete this.state.registeredVariables[t.name]}memory(){const t=this.backend.memory();return t.numTensors=this.state.numTensors,t.numDataBuffers=this.state.numDataBuffers,t.numBytes=this.state.numBytes,this.state.numStringTensors>0&&(t.unreliable=!0,null==t.reasons&&(t.reasons=[]),t.reasons.push("Memory usage by string tensors is approximate (2 bytes per character)")),t}async profile(t){this.state.profiling=!0;const e=this.state.numBytes,n=this.state.numTensors;this.state.activeProfile.kernels=[],this.state.activeProfile.result=await t(),this.state.profiling=!1,this.state.activeProfile.peakBytes=Math.max(...this.state.activeProfile.kernels.map(t=>t.totalBytesSnapshot)),this.state.activeProfile.newBytes=this.state.numBytes-e,this.state.activeProfile.newTensors=this.state.numTensors-n;for(const t of this.state.activeProfile.kernels)t.kernelTimeMs=await t.kernelTimeMs,t.extraInfo=await t.extraInfo;return this.state.activeProfile}isTapeOn(){return this.state.gradientDepth>0&&0===this.state.kernelDepth}addTapeNode(t,e,n,s,i,r){const a={id:this.state.nextTapeNodeId++,kernelName:t,inputs:e,outputs:n,saved:i},o=D(t);null!=o&&(s=o.gradFunc),null!=s&&(a.gradient=t=>(t=t.map((t,e)=>{if(null==t){const t=n[e],s=b(t.size,t.dtype);return this.makeTensor(s,t.shape,t.dtype)}return t}),s(t.length>1?t:t[0],i,r))),this.state.activeTape.push(a)}keep(t){return t.kept=!0,t}startTape(){0===this.state.gradientDepth&&(this.state.activeTape=[]),this.state.gradientDepth++}endTape(){this.state.gradientDepth--}startScope(t){const e={track:[],name:"unnamed scope",id:this.state.nextScopeId++};t&&(e.name=t),this.state.scopeStack.push(e),this.state.activeScope=e}endScope(t){const e=Q(t),n=new Set(e.map(t=>t.id));for(let t=0;t<this.state.activeScope.track.length;t++){const e=this.state.activeScope.track[t];e.kept||n.has(e.id)||e.dispose()}const s=this.state.scopeStack.pop();this.state.activeScope=0===this.state.scopeStack.length?null:this.state.scopeStack[this.state.scopeStack.length-1],e.forEach(t=>{t.kept||t.scopeId!==s.id||this.track(t)})}gradients(t,e,n,i=!1){if(s(e.length>0,()=>"gradients() received an empty list of xs."),null!=n&&"float32"!==n.dtype)throw new Error(`dy must have 'float32' dtype, but has '${n.dtype}'`);const r=this.scopedRun(()=>this.startTape(),()=>this.endTape(),()=>this.tidy("forward",t));s(r instanceof K,()=>"The result y returned by f() must be a tensor.");const l=function(t,e,n){const s={},i={};for(let t=0;t<e.length;t++)s[e[t].id]=!0;for(let n=0;n<t.length;n++){const r=t[n],a=r.inputs;for(const t in a){const n=a[t];let o=!1;for(let t=0;t<e.length;t++)if(s[n.id]){r.outputs.forEach(t=>s[t.id]=!0),o=!0,i[r.id]=!0;break}if(o)break}}const r={};r[n.id]=!0;const a={};for(let e=t.length-1;e>=0;e--){const n=t[e],s=n.inputs;for(let t=0;t<n.outputs.length;t++)if(r[n.outputs[t].id]){for(const t in s)r[s[t].id]=!0,a[n.id]=!0;break}}const o=[];for(let e=0;e<t.length;e++){const n=t[e];if(i[n.id]&&a[n.id]){const t={};for(const e in n.inputs){const i=n.inputs[e];s[i.id]&&(t[e]=i)}const e=Object.assign({},n);e.inputs=t,e.outputs=n.outputs,o.push(e)}}return o}(this.state.activeTape,e,r);if(!i&&0===l.length&&e.length>0)throw new Error("Cannot compute gradient of y=f(x) with respect to x. Make sure that the f you passed encloses all operations that lead from x to y.");return this.tidy("backward",()=>{const t={};t[r.id]=null==n?function(t){const e=y(a(t),"float32");return st.makeTensor(e,t,"float32")}(r.shape):n,function(t,e,n,s){for(let i=e.length-1;i>=0;i--){const r=e[i],a=[];if(r.outputs.forEach(e=>{const n=t[e.id];null!=n?a.push(n):a.push(null)}),null==r.gradient)throw new Error(`Cannot compute gradient: gradient function not found for ${r.kernelName}.`);const l=r.gradient(a);for(const e in r.inputs){if(!(e in l))throw new Error(`Cannot backprop through input ${e}. Available gradients found: ${Object.keys(l)}.`);const i=n(()=>l[e]());if("float32"!==i.dtype)throw new Error(`Error in gradient for op ${r.kernelName}. The gradient of input ${e} must have 'float32' dtype, but has '${i.dtype}'`);const a=r.inputs[e];if(!o(i.shape,a.shape))throw new Error(`Error in gradient for op ${r.kernelName}. The gradient of input '${e}' has shape '${i.shape}', which does not match the shape of the input '${a.shape}'`);if(null==t[a.id])t[a.id]=i;else{const e=t[a.id];t[a.id]=s(e,i),e.dispose()}}}}(t,l,t=>this.tidy(t),it);const s=e.map(e=>t[e.id]);return 0===this.state.gradientDepth&&(this.state.activeTape.forEach(t=>{for(const e of t.saved)e.dispose()}),this.state.activeTape=null),{value:r,grads:s}})}customGrad(t){return s(f(t),()=>"The f passed in customGrad(f) must be a function."),(...e)=>{let n;s(e.every(t=>t instanceof K),()=>"The args passed in customGrad(f)(x1, x2,...) must all be tensors");const i={};e.forEach((t,e)=>{i[e]=t});return this.runKernelFunc({forwardFunc:(i,r)=>(n=t(...e,r),s(n.value instanceof K,()=>"The function f passed in customGrad(f) must return an object where `obj.value` is a tensor"),s(f(n.gradFunc),()=>"The function f passed in customGrad(f) must return an object where `obj.gradFunc` is a function."),n.value),backwardsFunc:(t,i)=>{const r=n.gradFunc(t,i),a=Array.isArray(r)?r:[r];s(a.length===e.length,()=>"The function f passed in customGrad(f) must return an object where `obj.gradFunc` is a function that returns the same number of tensors as inputs passed to f(...)."),s(a.every(t=>t instanceof K),()=>"The function f passed in customGrad(f) must return an object where `obj.gradFunc` is a function that returns a list of only tensors.");const o={};return a.forEach((t,e)=>{o[e]=()=>t}),o},inputs:i})}}readSync(t){return this.state.tensorInfo.get(t).backend.readSync(t)}read(t){return this.state.tensorInfo.get(t).backend.read(t)}async time(t){const e=L(),n=await this.backend.time(t);return n.wallMs=L()-e,n}track(t){return null!=this.state.activeScope&&(t.scopeId=this.state.activeScope.id,this.state.activeScope.track.push(t)),t}get registeredVariables(){return this.state.registeredVariables}reset(){this.pendingBackendInitId++,this.state.dispose(),this.ENV.reset(),this.state=new et;for(const t in this.registry)this.disposeRegisteredKernels(t),this.registry[t].dispose(),delete this.registry[t];this.backendName=null,this.backendInstance=null,this.pendingBackendInit=null}}nt.nextTensorId=0,nt.nextVariableId=0;const st=function(){const t=I();if(null==t._tfengine){const e=new k(t);t._tfengine=new nt(e)}var e;return e=t._tfengine.ENV,S=e,U=()=>t._tfengine,t._tfengine}();function it(t,e){const n={a:t,b:e};return st.runKernel("Add",n)}function rt(t,e){let n=t;if(c(t))return"string"===e?[]:[t.length];if(!Array.isArray(t))return[];const i=[];for(;Array.isArray(n)||c(n)&&"string"!==e;)i.push(n.length),n=n[0];return Array.isArray(t)&&x().getBool("TENSORLIKE_CHECK_SHAPE_CONSISTENCY")&&function t(e,n,i){if(i=i||[],!Array.isArray(e)&&!c(e))return void s(0===n.length,()=>`Element arr[${i.join("][")}] is a primitive, but should be an array/TypedArray of ${n[0]} elements`);s(n.length>0,()=>`Element arr[${i.join("][")}] should be a primitive, but is an array of ${e.length} elements`),s(e.length===n[0],()=>`Element arr[${i.join("][")}] should have ${n[0]} elements, but has ${e.length} elements`);const r=n.slice(1);for(let n=0;n<e.length;++n)t(e[n],r,i.concat(n))}(t,i,[]),i}function at(t,e,n,s){if("string_or_numeric"!==t){if(null==t)throw new Error("Expected dtype cannot be null.");if("numeric"!==t&&t!==e||"numeric"===t&&"string"===e)throw new Error(`Argument '${n}' passed to '${s}' must be ${t} tensor, but got ${e} tensor`)}}function ot(t,e,n,s="numeric"){if(t instanceof K)return at(s,t.dtype,e,n),t;let i=d(t);if("string"!==i&&["bool","int32","float32"].indexOf(s)>=0&&(i=s),at(s,i,e,n),null==t||!c(t)&&!Array.isArray(t)&&"number"!=typeof t&&"boolean"!=typeof t&&"string"!=typeof t){const s=null==t?"null":t.constructor.name;throw new Error(`Argument '${e}' passed to '${n}' must be a Tensor or TensorLike, but got '${s}'`)}const a=rt(t,i);c(t)||Array.isArray(t)||(t=[t]);const o="string"!==i?F(t,i):r(t,[],!0);return st.makeTensor(o,a,i)}function lt(t,e,n,s="numeric"){if(!Array.isArray(t))throw new Error(`Argument ${e} passed to ${n} must be a \`Tensor[]\` or \`TensorLike[]\``);return t.map((t,i)=>ot(t,`${e}[${i}]`,n,s))}function ut(t){const e=Object.keys(t);if(1!==e.length)throw new Error("Please provide an object with a single key (operation name) mapping to a function. Got an object with "+e.length+" keys.");let n=e[0];const s=t[n];n.endsWith("_")&&(n=n.substring(0,n.length-1)),n+="__op";const i=(...t)=>{st.startScope(n);try{const e=s(...t);return w(e)&&console.error("Cannot return a Promise inside of tidy."),st.endScope(e),e}catch(t){throw st.endScope(null),t}};return Object.defineProperty(i,"name",{value:n,configurable:!0}),i}const ht=ut({abs_:function(t){const e=ot(t,"x","abs");if("complex64"===e.dtype){const t={x:e};return st.runKernel("ComplexAbs",t)}{const t={x:e};return st.runKernel("Abs",t)}}});const ct=ut({acos_:function(t){const e={x:ot(t,"x","acos")};return st.runKernel("Acos",e)}});const pt=ut({acosh_:function(t){const e={x:ot(t,"x","acosh")};return st.runKernel("Acosh",e)}});const dt=ut({add_:function(t,e){let n=ot(t,"a","add"),s=ot(e,"b","add");[n,s]=Y(n,s);const i={a:n,b:s};return st.runKernel("Add",i)}});const ft=ut({all_:function(t,e=null,n=!1){const s={x:ot(t,"x","all","bool")},i={axis:e,keepDims:n};return st.runKernel("All",s,i)}});const gt=ut({any_:function(t,e=null,n=!1){const s={x:ot(t,"x","any","bool")},i={axis:e,keepDims:n};return st.runKernel("Any",s,i)}});const mt=ut({argMax_:function(t,e=0){const n={x:ot(t,"x","argMax")},s={axis:e};return st.runKernel("ArgMax",n,s)}});const yt=ut({argMin_:function(t,e=0){const n={x:ot(t,"x","argMin")},s={axis:e};return st.runKernel("ArgMin",n,s)}});const bt=ut({asin_:function(t){const e={x:ot(t,"x","asin")};return st.runKernel("Asin",e)}});const wt=ut({asinh_:function(t){const e={x:ot(t,"x","asinh")};return st.runKernel("Asinh",e)}});const kt=ut({atan_:function(t){const e={x:ot(t,"x","atan")};return st.runKernel("Atan",e)}});const xt=ut({atan2_:function(t,e){let n=ot(t,"a","atan2"),s=ot(e,"b","atan2");[n,s]=Y(n,s);const i={a:n,b:s};return st.runKernel("Atan2",i)}});const vt=ut({atanh_:function(t){const e={x:ot(t,"x","atanh")};return st.runKernel("Atanh",e)}});const St=ut({cast_:function(t,e){const n=ot(t,"x","cast");if(!function(t){return"bool"===t||"complex64"===t||"float32"===t||"int32"===t||"string"===t}(e))throw new Error("Failed to cast to unknown dtype "+e);if("string"===e&&"string"!==n.dtype||"string"!==e&&"string"===n.dtype)throw new Error("Only strings can be casted to strings");const s={x:n},i={dtype:e};return st.runKernel("Cast",s,i)}});function It(t,e,n,s,i,r,a="channelsLast"){const[o,l]=Nt(e);let u;if("channelsLast"===a)u=[o,l,t[3],t[3]];else{if("channelsFirst"!==a)throw new Error("Unknown dataFormat "+a);u=[o,l,t[1],t[1]]}return function(t,e,n,s,i,r,a=!1,o="channelsLast"){let[l,u,h,c]=[-1,-1,-1,-1];if("channelsLast"===o)[l,u,h,c]=t;else{if("channelsFirst"!==o)throw new Error("Unknown dataFormat "+o);[l,c,u,h]=t}const[p,d,,f]=e,[g,m]=Nt(n),[y,b]=Nt(s),w=zt(p,y),k=zt(d,b),{padInfo:x,outHeight:v,outWidth:S}=function(t,e,n,s,i,r,a,o,l){let u,h,c;if("number"==typeof t){u={top:t,bottom:t,left:t,right:t,type:0===t?"VALID":"NUMBER"};const i=function(t,e,n,s,i){null==s&&(s=function(t,e,n,s=1){const i=zt(e,s);return Math.floor((t[0]*(n-1)-n+i)/2)}(t,e,n));const r=t[0],a=t[1],o=At((r-e+2*s)/n+1,i),l=At((a-e+2*s)/n+1,i);return[o,l]}([e,n],r,s,t,o);h=i[0],c=i[1]}else if("same"===t){h=Math.ceil(e/s),c=Math.ceil(n/i);const t=Math.max(0,(h-1)*s+r-e),o=Math.max(0,(c-1)*i+a-n),l=Math.floor(t/2),p=t-l,d=Math.floor(o/2);u={top:l,bottom:p,left:d,right:o-d,type:"SAME"}}else if("valid"===t)u={top:0,bottom:0,left:0,right:0,type:"VALID"},h=Math.ceil((e-r+1)/s),c=Math.ceil((n-a+1)/i);else{if("object"!=typeof t)throw Error("Unknown padding parameter: "+t);{const p="channelsLast"===l?t[1][0]:t[2][0],d="channelsLast"===l?t[1][1]:t[2][1],f="channelsLast"===l?t[2][0]:t[3][0],g="channelsLast"===l?t[2][1]:t[3][1];u={top:p,bottom:d,left:f,right:g,type:0===p&&0===d&&0===f&&0===g?"VALID":"EXPLICIT"},h=At((e-r+p+d)/s+1,o),c=At((n-a+f+g)/i+1,o)}}return{padInfo:u,outHeight:h,outWidth:c}}(i,u,h,g,m,w,k,r,o),I=a?f*c:f;let N;"channelsFirst"===o?N=[l,I,v,S]:"channelsLast"===o&&(N=[l,v,S,I]);return{batchSize:l,dataFormat:o,inHeight:u,inWidth:h,inChannels:c,outHeight:v,outWidth:S,outChannels:I,padInfo:x,strideHeight:g,strideWidth:m,filterHeight:p,filterWidth:d,effectiveFilterHeight:w,effectiveFilterWidth:k,dilationHeight:y,dilationWidth:b,inShape:t,outShape:N,filterShape:e}}(t,u,n,s,i,r,!1,a)}function Nt(t){return"number"==typeof t?[t,t,t]:2===t.length?[t[0],t[1],1]:t}function zt(t,e){return e<=1?t:t+(t-1)*(e-1)}function At(t,e){if(!e)return Math.trunc(t);switch(e){case"round":return Math.round(t);case"ceil":return Math.ceil(t);case"floor":return Math.floor(t);default:throw new Error("Unknown roundingMode "+e)}}function Ct(t){const[e,n,s]=Nt(t);return 1===e&&1===n&&1===s}function Dt(t,e){return Ct(t)||Ct(e)}const Tt=ut({reshape_:function(t,e){const n={x:ot(t,"x","reshape","string_or_numeric")},s={shape:e};return st.runKernel("Reshape",n,s)}});const Et=ut({avgPool_:function(t,e,n,i,r){const a=ot(t,"x","avgPool","float32");s(Dt(n,1),()=>`Error in avgPool: Either strides or dilations must be 1. Got strides ${n} and dilations '1'`);let o=a,u=!1;3===a.rank&&(u=!0,o=Tt(a,[1,a.shape[0],a.shape[1],a.shape[2]])),s(4===o.rank,()=>`Error in avgPool: x must be rank 4 but got rank ${o.rank}.`),null!=r&&s(l(i),()=>`Error in avgPool: pad must be an integer when using, dimRoundingMode ${r} but got pad ${i}.`);const h={x:o},c={filterSize:e,strides:n,pad:i,dimRoundingMode:r};let p=st.runKernel("AvgPool",h,c);return p=St(p,a.dtype),u?Tt(p,[p.shape[1],p.shape[2],p.shape[3]]):p}});const Ft=ut({clone_:function(t){const e={x:ot(t,"x","clone","string_or_numeric")};return st.runKernel("Identity",e)}});const Lt=ut({concat_:function(t,e=0){s(t.length>=1,()=>"Pass at least one tensor to concat");const n=lt(t,"tensors","concat","string_or_numeric");if("complex64"===n[0].dtype&&n.forEach(t=>{if("complex64"!==t.dtype)throw new Error(`Cannot concatenate complex64 tensors with a tensor\n          with dtype ${t.dtype}. `)}),1===n.length)return Ft(n[0]);const i=n,r={axis:e};return st.runKernel("Concat",i,r)}});const $t=ut({matMul_:function(t,e,n=!1,s=!1){let i=ot(t,"a","matMul"),r=ot(e,"b","matMul");[i,r]=Y(i,r);const a={a:i,b:r},o={transposeA:n,transposeB:s};return st.runKernel("BatchMatMul",a,o)}});const _t=ut({mul_:function(t,e){let n=ot(t,"a","mul"),s=ot(e,"b","mul");[n,s]=Y(n,s);const i={a:n,b:s};return st.runKernel("Multiply",i)}});const Rt=ut({sigmoid_:function(t){const e={x:ot(t,"x","sigmoid")};return st.runKernel("Sigmoid",e)}});const Mt=ut({slice_:function(t,e,n){const s=ot(t,"x","slice","string_or_numeric");if(0===s.rank)throw new Error("Slicing scalar is not possible");const i={x:s},r={begin:e,size:n};return st.runKernel("Slice",i,r)}});const Ot=ut({tanh_:function(t){const e={x:ot(t,"x","tanh")};return st.runKernel("Tanh",e)}});const Bt=ut({batchToSpaceND_:function(t,e,n){const i=ot(t,"x","batchToSpaceND"),r=e.reduce((t,e)=>t*e);s(i.rank>=1+e.length,()=>`input rank is ${i.rank} but should be > than blockShape.length ${e.length}`),s(n.length===e.length,()=>`crops.length is ${n.length} but should be equal to blockShape.length  ${e.length}`),s(i.shape[0]%r==0,()=>`input tensor batch is ${i.shape[0]} but is not divisible by the product of the elements of blockShape ${e.join(" * ")} === ${r}`);const a={x:i},o={blockShape:e,crops:n};return st.runKernel("BatchToSpaceND",a,o)}});const Pt=ut({batchNorm_:function(t,e,n,i,r,a){null==a&&(a=.001);const o=ot(t,"x","batchNorm"),l=ot(e,"mean","batchNorm"),u=ot(n,"variance","batchNorm");let h,c;null!=r&&(h=ot(r,"scale","batchNorm")),null!=i&&(c=ot(i,"offset","batchNorm")),s(l.rank===u.rank,()=>"Batch normalization gradient requires mean and variance to have equal ranks."),s(null==c||l.rank===c.rank,()=>"Batch normalization gradient requires mean and offset to have equal ranks."),s(null==h||l.rank===h.rank,()=>"Batch normalization gradient requires mean and scale to have equal ranks.");const p={x:function(t){let e;return e=0===t.rank||1===t.rank?Tt(t,[1,1,1,t.size]):2===t.rank?Tt(t,[1,1,t.shape[0],t.shape[1]]):3===t.rank?Tt(t,[1,t.shape[0],t.shape[1],t.shape[2]]):t,e}(o),scale:h,offset:c,mean:l,variance:u},d={varianceEpsilon:a},f=st.runKernel("FusedBatchNorm",p,d);return Tt(f,o.shape)}});const Wt=ut({broadcastTo_:function(t,e){let n=ot(t,"broadcastTo","x");const s=n.shape;if(e.some(t=>!(t>0)||t%1!=0))throw new Error(`broadcastTo(): Invalid broadcast shape [${e}].`);if(e.length<n.rank)throw new Error(`broadcastTo(): shape.length=${e.length} < input.rank=${n.rank}.`);if(e.length>n.rank){const t=n.shape.slice();for(;t.length<e.length;)t.unshift(1);n=Tt(n,t)}const i=n.shape,r=Array.from(e);for(let t=e.length-1;t>=0;t--)if(i[t]===e[t])r[t]=1;else if(1!==n.shape[t])throw new Error(`broadcastTo(): [${s}] cannot be broadcast to [${e}].`);if(0===r.map((t,e)=>t>1?e:-1).filter(t=>t>=0).length)return Ft(n);const a={x:n},o={reps:r};return st.runKernel("Tile",a,o)}});const Ut=ut({ceil_:function(t){const e={x:ot(t,"x","ceil")};return st.runKernel("Ceil",e)}});const Kt=ut({clipByValue_:function(t,e,n){const i=ot(t,"x","clipByValue");s(e<=n,()=>`Error in clip: min (${e}) must be less than or equal to max (${n}).`);const r={x:i},a={clipValueMin:e,clipValueMax:n};return st.runKernel("ClipByValue",r,a)}});const Vt=ut({complex_:function(t,e){const n=ot(t,"real","complex"),s=ot(e,"imag","complex");i(n.shape,s.shape,`real and imag shapes, ${n.shape} and ${s.shape}, must match in call to tf.complex().`);const r={real:n,imag:s};return st.runKernel("Complex",r)}});const jt=ut({conv2d_:function(t,e,n,i,r="NHWC",a=[1,1],o){const u=ot(t,"x","conv2d"),h=ot(e,"filter","conv2d");let c=u,p=!1;3===u.rank&&(p=!0,c=Tt(u,[1,u.shape[0],u.shape[1],u.shape[2]])),s(4===c.rank,()=>`Error in conv2d: input must be rank 4, but got rank ${c.rank}.`),s(4===h.rank,()=>"Error in conv2d: filter must be rank 4, but got rank "+h.rank+"."),null!=o&&s(l(i),()=>`Error in conv2d: pad must be an integer when using, dimRoundingMode ${o} but got pad ${i}.`);const d="NHWC"===r?c.shape[3]:c.shape[1];s(d===h.shape[2],()=>`Error in conv2d: depth of input (${d}) must match input depth for filter ${h.shape[2]}.`),s(Dt(n,a),()=>`Error in conv2D: Either strides or dilations must be 1. Got strides ${n} and dilations '${a}'`);const f={x:c,filter:h},g={strides:n,pad:i,dataFormat:r,dilations:a,dimRoundingMode:o},m=st.runKernel("Conv2D",f,g);return p?Tt(m,[m.shape[1],m.shape[2],m.shape[3]]):m}});const qt=ut({conv1d_:function(t,e,n,i,r="NWC",a=1,o){const u=ot(t,"x","conv1d"),h=ot(e,"filter","conv1d");let c=u,p=!1;2===u.rank&&(p=!0,c=Tt(u,[1,u.shape[0],u.shape[1]])),s(3===c.rank,()=>`Error in conv1d: input must be rank 3, but got rank ${c.rank}.`),s(3===h.rank,()=>"Error in conv1d: filter must be rank 3, but got rank "+h.rank+"."),null!=o&&s(l(i),()=>`Error in conv1d: pad must be an integer when using, dimRoundingMode ${o} but got pad ${i}.`),s(c.shape[2]===h.shape[1],()=>`Error in conv1d: depth of input (${c.shape[2]}) must match input depth for filter ${h.shape[1]}.`),s(Dt(n,a),()=>`Error in conv1D: Either stride or dilation must be 1. Got stride ${n} and dilation '${a}'`),s("NWC"===r,()=>`Error in conv1d: got dataFormat of ${r} but only NWC is currently supported.`);const d=Tt(h,[1,h.shape[0],h.shape[1],h.shape[2]]),f=Tt(c,[c.shape[0],1,c.shape[1],c.shape[2]]),g=jt(f,d,[1,n],i,"NHWC",[1,a],o);return Tt(g,p?[g.shape[2],g.shape[3]]:[g.shape[0],g.shape[2],g.shape[3]])}});const Gt=ut({conv2DBackpropInput_:function(t,e,n,i,r,a="NHWC",o){s(t.length===e.rank,()=>`Length of inShape (${t.length}) and rank of dy (${e.rank}) must match`);let u=t,h=e,c=!1;3===e.rank&&(c=!0,h=Tt(e,[1,e.shape[0],e.shape[1],e.shape[2]]),u=[1,t[0],t[1],t[2]]),s(4===u.length,()=>"Error in conv2dDerInput: inShape must be length 4, but got length "+u.length+"."),s(4===h.rank,()=>"Error in conv2dDerInput: dy must be rank 4, but got rank "+h.rank),s(4===n.rank,()=>"Error in conv2dDerInput: filter must be rank 4, but got rank "+n.rank);const p="NHWC"===a?u[3]:u[1],d="NHWC"===a?h.shape[3]:h.shape[1];s(p===n.shape[2],()=>`Error in conv2dDerInput: depth of input (${p}) must match input depth for filter ${n.shape[2]}.`),s(d===n.shape[3],()=>`Error in conv2dDerInput: depth of output (${d}) must match output depth for filter ${n.shape[3]}.`),null!=o&&s(l(r),()=>`Error in conv2dDerInput: pad must be an integer when using, dimRoundingMode ${o} but got pad ${r}.`);const f={dy:h,filter:n},g={strides:i,pad:r,dataFormat:a,dimRoundingMode:o,inputShape:u},m=st.runKernel("Conv2DBackpropInput",f,g);return c?Tt(m,[m.shape[1],m.shape[2],m.shape[3]]):m}});const Ht=ut({conv2dTranspose_:function(t,e,n,s,i,r){const a=ot(t,"x","conv2dTranspose"),o=ot(e,"filter","conv2dTranspose");return Gt(n,a,o,s,i,"NHWC",r)}});const Jt=ut({conv3DBackpropInput_:function(t,e,n,i,r){s(t.length===e.rank,()=>`Length of inShape (${t.length}) and rank of dy (${e.rank}) must match`);let a=t,o=e,l=!1;4===e.rank&&(l=!0,o=Tt(e,[1,e.shape[0],e.shape[1],e.shape[2],e.shape[3]]),a=[1,t[0],t[1],t[2],t[3]]);const u=a[4],h=o.shape[4];s(5===a.length,()=>"Error in conv3dDerInput: inShape must be length 5, but got length "+a.length+"."),s(5===o.rank,()=>"Error in conv3dDerInput: dy must be rank 5, but got rank "+o.rank),s(5===n.rank,()=>"Error in conv3dDerInput: filter must be rank 5, but got rank "+n.rank),s(u===n.shape[3],()=>`Error in conv3dDerInput: depth of input (${u}) must match input depth for filter ${n.shape[3]}.`),s(h===n.shape[4],()=>`Error in conv3dDerInput: depth of output (${h}) must match output depth for filter ${n.shape[4]}.`);const c={dy:o,filter:n},p={pad:r,strides:i,inputShape:a},d=st.runKernel("Conv3DBackpropInputV2",c,p);return l?Tt(d,[d.shape[1],d.shape[2],d.shape[3],d.shape[4]]):d}});const Zt=ut({cos_:function(t){const e={x:ot(t,"x","cos")};return st.runKernel("Cos",e)}});const Xt=ut({cosh_:function(t){const e={x:ot(t,"x","cosh")};return st.runKernel("Cosh",e)}});const Yt=ut({cumsum_:function(t,e=0,n=!1,s=!1){const i={x:ot(t,"x","cumsum")},r={axis:e,exclusive:n,reverse:s};return st.runKernel("Cumsum",i,r)}});const Qt=ut({depthToSpace_:function(t,e,n="NHWC"){const i=ot(t,"x","depthToSpace"),r="NHWC"===n?i.shape[1]:i.shape[2],a="NHWC"===n?i.shape[2]:i.shape[3],o="NHWC"===n?i.shape[3]:i.shape[1];s(r*e>=0,()=>`Negative dimension size caused by overflow when multiplying\n    ${r} and ${e}  for depthToSpace with input shape\n    ${i.shape}`),s(a*e>=0,()=>`Negative dimension size caused by overflow when multiplying\n    ${a} and ${e} for depthToSpace with input shape\n        ${i.shape}`),s(o%(e*e)==0,()=>`Dimension size must be evenly divisible by ${e*e} but is ${o} for depthToSpace with input shape ${i.shape}`);const l={x:i},u={blockSize:e,dataFormat:n};return st.runKernel("DepthToSpace",l,u)}});const te=ut({depthwiseConv2d_:function(t,e,n,i,r="NHWC",a=[1,1],o){const u=ot(t,"x","depthwiseConv2d"),h=ot(e,"filter","depthwiseConv2d");let c=u,p=!1;3===u.rank&&(p=!0,c=Tt(u,[1,u.shape[0],u.shape[1],u.shape[2]])),s(4===c.rank,()=>`Error in depthwiseConv2d: input must be rank 4, but got rank ${c.rank}.`),s(4===h.rank,()=>"Error in depthwiseConv2d: filter must be rank 4, but got rank "+h.rank+"."),s(c.shape[3]===h.shape[2],()=>`Error in depthwiseConv2d: number of input channels (${c.shape[3]}) must match the inChannels dimension in filter ${h.shape[2]}.`),null!=o&&s(l(i),()=>`Error in depthwiseConv2d: pad must be an integer when using, dimRoundingMode ${o} but got pad ${i}.`);const d={x:c,filter:h},f={strides:n,pad:i,dataFormat:r,dilations:a,dimRoundingMode:o},g=st.runKernel("DepthwiseConv2dNative",d,f);return p?Tt(g,[g.shape[1],g.shape[2],g.shape[3]]):g}});const ee=ut({dilation2d_:function(t,e,n,i,r=[1,1],a="NHWC"){const o=ot(t,"x","dilation2d"),l=ot(e,"filter","dilation2d");s(3===o.rank||4===o.rank,()=>"Error in dilation2d: input must be rank 3 or 4, but got rank "+o.rank+"."),s(3===l.rank,()=>"Error in dilation2d: filter must be rank 3, but got rank "+l.rank+"."),s("NHWC"===a,()=>"Error in dilation2d: Only NHWC is currently supported, but got dataFormat of "+a);let u=o,h=!1;3===o.rank&&(u=Tt(o,[1,o.shape[0],o.shape[1],o.shape[2]]),h=!0);const c={x:u,filter:l},p={strides:n,pad:i,dilations:r},d=st.runKernel("Dilation2D",c,p);return h?Tt(d,[d.shape[1],d.shape[2],d.shape[3]]):d}});const ne=ut({floorDiv_:function(t,e){let n=ot(t,"a","floorDiv"),s=ot(e,"b","floorDiv");[n,s]=Y(n,s);const i={a:n,b:s};return st.runKernel("FloorDiv",i)}});const se=ut({div_:function(t,e){let n=ot(t,"a","div"),s=ot(e,"b","div");if([n,s]=Y(n,s),"int32"===n.dtype&&"int32"===s.dtype)return ne(n,s);const i={a:n,b:s};return st.runKernel("RealDiv",i,{})}});function ie(t,e){const n=[];for(let s=0;s<e.length;s++){const i=t[t.length-s-1],r=e.length-s-1,a=e[r];(null==i||1===i&&a>1)&&n.unshift(r)}return n}function re(t,e){const n=[],s=Math.max(t.length,e.length);for(let i=0;i<s;i++){let s=t[t.length-i-1];null==s&&(s=1);let r=e[e.length-i-1];if(null==r&&(r=1),1===s)n.unshift(r);else if(1===r)n.unshift(s);else{if(s!==r){throw Error(`Operands could not be broadcast together with shapes ${t} and ${e}.`)}n.unshift(s)}}return n}const ae=ut({equal_:function(t,e){let n=ot(t,"a","equal"),s=ot(e,"b","equal");[n,s]=Y(n,s),re(n.shape,s.shape);const i={a:n,b:s};return st.runKernel("Equal",i)}});const oe=ut({where_:function(t,e,n){const r=ot(e,"a","where"),a=ot(n,"b","where"),o=ot(t,"condition","where","bool"),l=re(r.shape,a.shape),u=Wt(r,l),h=Wt(a,l);1===o.rank&&s(o.shape[0]===r.shape[0],()=>"The first dimension of `a` must match the size of `condition`."),1!==o.rank&&i(o.shape,h.shape,"Error in where: ");const c={condition:o,t:u,e:h};return st.runKernel("Select",c)}});const le=ut({zerosLike_:function(t){const e={x:ot(t,"x","zerosLike")};return st.runKernel("ZerosLike",e)}});const ue=ut({divNoNan_:function(t,e){let n=ot(t,"a","div"),s=ot(e,"b","div");[n,s]=Y(n,s);const i=se(n,s),r=le(i),a=ae(s,r);return oe(a,r,i)}});const he=ut({dot_:function(t,e){const n=ot(t,"t1","dot"),i=ot(e,"t2","dot");s(!(1!==n.rank&&2!==n.rank||1!==i.rank&&2!==i.rank),()=>`Error in dot: inputs must all be rank 1 or 2, but got ranks ${n.rank} and ${i.rank}.`);const r=1===n.rank?n.size:n.shape[1],a=1===i.rank?i.size:i.shape[0];if(s(r===a,()=>`Error in dot: inner dimensions of inputs must match, but got ${r} and ${a}.`),1===n.rank&&1===i.rank){const t=Tt(n,[1,-1]),e=Tt(i,[-1,1]),s=$t(t,e);return Tt(s,[])}if(1===n.rank&&2===i.rank){const t=Tt(n,[1,-1]),e=Tt(i,[i.shape[0],i.shape[1]]),s=$t(t,e);return Tt(s,[s.size])}if(2===n.rank&&1===i.rank){const t=Tt(i,[-1,1]),e=$t(n,t);return Tt(e,[e.size])}{const t=Tt(i,[i.shape[0],i.shape[1]]);return $t(n,t)}}});const ce=ut({elu_:function(t){const e={x:ot(t,"x","elu")};return st.runKernel("Elu",e)}});const pe=ut({erf_:function(t){let e=ot(t,"x","erf");s("int32"===e.dtype||"float32"===e.dtype,()=>"Input dtype must be `int32` or `float32`."),"int32"===e.dtype&&(e=St(e,"float32"));const n={x:e};return st.runKernel("Erf",n)}});const de=ut({exp_:function(t){const e={x:ot(t,"x","exp")};return st.runKernel("Exp",e)}});const fe=ut({expandDims_:function(t,e=0){const n=ot(t,"x","expandDims","string_or_numeric");s(e<=n.rank,()=>"Axis must be <= rank of the tensor");const i={input:n},r={dim:e};return st.runKernel("ExpandDims",i,r)}});const ge=ut({expm1_:function(t){const e={x:ot(t,"x","expm1")};return st.runKernel("Expm1",e)}});const me=ut({tile_:function(t,e){const n=ot(t,"x","tile","string_or_numeric");s(n.rank===e.length,()=>`Error in transpose: rank of input ${n.rank} must match length of reps ${e}.`);const i={x:n},r={reps:e};return st.runKernel("Tile",i,r)}});const ye=ut({floor_:function(t){const e={x:ot(t,"x","floor")};return st.runKernel("Floor",e)}});const be=ut({gather_:function(t,e,n=0,s=0){const i={x:ot(t,"x","gather"),indices:ot(e,"indices","gather","int32")},r={axis:n,batchDims:s};return st.runKernel("GatherV2",i,r)}});const we=ut({greater_:function(t,e){let n=ot(t,"a","greater"),s=ot(e,"b","greater");[n,s]=Y(n,s),re(n.shape,s.shape);const i={a:n,b:s};return st.runKernel("Greater",i)}});const ke=ut({greaterEqual_:function(t,e){let n=ot(t,"a","greaterEqual"),s=ot(e,"b","greaterEqual");[n,s]=Y(n,s),re(n.shape,s.shape);const i={a:n,b:s};return st.runKernel("GreaterEqual",i)}});const xe=ut({imag_:function(t){const e={input:ot(t,"input","imag")};return st.runKernel("Imag",e)}});const ve=ut({isFinite_:function(t){const e={x:ot(t,"x","isFinite")};return st.runKernel("IsFinite",e)}});const Se=ut({isInf_:function(t){const e={x:ot(t,"x","isInf")};return st.runKernel("IsInf",e)}});const Ie=ut({isNaN_:function(t){const e={x:ot(t,"x","isNaN")};return st.runKernel("IsNan",e)}});const Ne=ut({leakyRelu_:function(t,e=.2){const n={x:ot(t,"x","leakyRelu")},s={alpha:e};return st.runKernel("LeakyRelu",n,s)}});const ze=ut({less_:function(t,e){let n=ot(t,"a","less"),s=ot(e,"b","less");[n,s]=Y(n,s),re(n.shape,s.shape);const i={a:n,b:s};return st.runKernel("Less",i)}});const Ae=ut({lessEqual_:function(t,e){let n=ot(t,"a","lessEqual"),s=ot(e,"b","lessEqual");[n,s]=Y(n,s),re(n.shape,s.shape);const i={a:n,b:s};return st.runKernel("LessEqual",i)}});const Ce=ut({localResponseNormalization_:function(t,e=5,n=1,i=1,r=.5){const a=ot(t,"x","localResponseNormalization");s(4===a.rank||3===a.rank,()=>`Error in localResponseNormalization: x must be rank 3 or 4 but got\n               rank ${a.rank}.`),s(l(e),()=>`Error in localResponseNormalization: depthRadius must be an integer but got depthRadius ${e}.`);let o=a,u=!1;3===a.rank&&(u=!0,o=Tt(a,[1,a.shape[0],a.shape[1],a.shape[2]]));const h={x:o},c={depthRadius:e,bias:n,alpha:i,beta:r},p=st.runKernel("LRN",h,c);return u?Tt(p,[p.shape[1],p.shape[2],p.shape[3]]):p}});const De=ut({log_:function(t){const e={x:ot(t,"x","log")};return st.runKernel("Log",e)}});const Te=ut({log1p_:function(t){const e={x:ot(t,"x","log1p")};return st.runKernel("Log1p",e)}});function Ee(t){return st.customGrad(t)}const Fe=ut({neg_:function(t){const e={x:ot(t,"x","neg")};return st.runKernel("Neg",e)}});const Le=ut({softplus_:function(t){const e={x:ot(t,"x","softplus")};return st.runKernel("Softplus",e)}});const $e=ut({logSigmoid_:function(t){const e=ot(t,"x","logSigmoid");return Ee(t=>({value:Fe(Le(Fe(t))),gradFunc:e=>_t(e,Rt(Fe(t)))}))(e)}});const _e=ut({max_:function(t,e=null,n=!1){const s={x:ot(t,"x","max")},i={reductionIndices:e,keepDims:n};return st.runKernel("Max",s,i)}});const Re=ut({sub_:function(t,e){let n=ot(t,"a","sub"),s=ot(e,"b","sub");[n,s]=Y(n,s);const i={a:n,b:s};return st.runKernel("Sub",i)}});const Me=ut({sum_:function(t,e=null,n=!1){let s=ot(t,"x","sum");"bool"===s.dtype&&(s=St(s,"int32"));const i={x:s},r={axis:e,keepDims:n};return st.runKernel("Sum",i,r)}});const Oe=ut({logSoftmax_:function(t,e=-1){const n=ot(t,"logits","logSoftmax");if(-1===e&&(e=n.rank-1),e!==n.rank-1)throw Error(`Log Softmax along a non-last dimension is not yet supported. Logits was rank ${n.rank} and axis was ${e}`);return Ee((t,n)=>{const s=_e(t,e,!0),i=Re(t,s),r=Re(St(i,"float32"),De(Me(de(i),e,!0)));n([r]);return{value:r,gradFunc:(t,n)=>{const[s]=n,i=de(s);return Re(t,_t(Me(t,e,!0),i))}}})(n)}});function Be(t,e){return function(t,e,n){const s=t.length+e.length,i=[];let r=0,a=0;for(let o=0;o<s;o++)-1===n.indexOf(o)?i.push(t[r++]):i.push(e[a++]);return i}(t,e.map(t=>1),e)}function Pe(t){return t.map((t,e)=>[e,t]).sort((t,e)=>t[1]-e[1]).map(t=>t[0])}const We=ut({logSumExp_:function(t,e=null,n=!1){const s=ot(t,"x","logSumExp"),i=h(e,s.shape),r=_e(s,i,!0),a=Re(s,r),o=de(a),l=Me(o,i),u=De(l),c=dt(Tt(r,u.shape),u);if(n){const t=Be(c.shape,i);return Tt(c,t)}return c}});const Ue=ut({logicalAnd_:function(t,e){const n=ot(t,"a","logicalAnd","bool"),s=ot(e,"b","logicalAnd","bool");re(n.shape,s.shape);const i={a:n,b:s};return st.runKernel("LogicalAnd",i)}});const Ke=ut({logicalNot_:function(t){const e={x:ot(t,"x","logicalNot","bool")};return st.runKernel("LogicalNot",e)}});const Ve=ut({logicalOr_:function(t,e){const n=ot(t,"a","logicalOr","bool"),s=ot(e,"b","logicalOr","bool");re(n.shape,s.shape);const i={a:n,b:s};return st.runKernel("LogicalOr",i)}});const je=ut({logicalXor_:function(t,e){const n=ot(t,"a","logicalXor","bool"),s=ot(e,"b","logicalXor","bool");return re(n.shape,s.shape),Ue(Ve(t,e),Ke(Ue(t,e)))}});const qe=ut({maxPool_:function(t,e,n,i,r){const a=ot(t,"x","maxPool");let o=a,u=!1;3===a.rank&&(u=!0,o=Tt(a,[1,a.shape[0],a.shape[1],a.shape[2]])),s(4===o.rank,()=>`Error in maxPool: input must be rank 4 but got rank ${o.rank}.`),s(Dt(n,1),()=>`Error in maxPool: Either strides or dilations must be 1. Got strides ${n} and dilations '1'`),null!=r&&s(l(i),()=>`Error in maxPool: pad must be an integer when using, dimRoundingMode ${r} but got pad ${i}.`);const h={x:o},c={filterSize:e,strides:n,pad:i,dimRoundingMode:r},p=st.runKernel("MaxPool",h,c);return u?Tt(p,[p.shape[1],p.shape[2],p.shape[3]]):p}});const Ge=ut({maximum_:function(t,e){let n=ot(t,"a","maximum"),s=ot(e,"b","maximum");[n,s]=Y(n,s),"bool"===n.dtype&&(n=St(n,"int32"),s=St(s,"int32")),re(n.shape,s.shape);const i={a:n,b:s};return st.runKernel("Maximum",i)}});const He=ut({mean_:function(t,e=null,n=!1){const s={x:ot(t,"x","mean")},i={axis:e,keepDims:n};return st.runKernel("Mean",s,i)}});const Je=ut({min_:function(t,e=null,n=!1){const s={x:ot(t,"x","min")},i={axis:e,keepDims:n};return st.runKernel("Min",s,i)}});const Ze=ut({minimum_:function(t,e){let n=ot(t,"a","minimum"),s=ot(e,"b","minimum");[n,s]=Y(n,s),"bool"===n.dtype&&(n=St(n,"int32"),s=St(s,"int32")),re(n.shape,s.shape);const i={a:n,b:s};return st.runKernel("Minimum",i)}});const Xe=ut({mirrorPad_:function(t,e,n){s("reflect"===n||"symmetric"===n,()=>`Invalid mode. Mode must be either reflect or symmetric. Got ${n}.`);const i=ot(t,"x","mirrorPad");if(0===i.rank)throw new Error("mirrorPad(scalar) is not defined. Pass non-scalar to mirrorPad");s(e.length===i.rank,()=>`Padding doesn't match input. Must be ${i.rank}. Got ${e.length}.`);const r="reflect"===n?1:0;for(let t=0;t<i.rank;t++)s(2===e[t].length,()=>"Invalid number of paddings. Must be length of 2 each."),s(e[t][0]>=0&&e[t][0]<=i.shape[t]-r&&e[t][1]>=0&&e[t][1]<=i.shape[t]-r,()=>`Padding in dimension ${t} cannot be greater than or equal to ${i.shape[t]-r} or less than 0 for input of shape `+i.shape);const a={paddings:e,mode:n},o={x:i};return st.runKernel("MirrorPad",o,a)}});const Ye=ut({mod_:function(t,e){let n=ot(t,"a","mod"),s=ot(e,"b","mod");[n,s]=Y(n,s);const i={a:n,b:s};return st.runKernel("Mod",i)}});const Qe=ut({square_:function(t){const e=ot(t,"x","square");return st.runKernel("Square",{x:e},{})}});const tn=ut({notEqual_:function(t,e){let n=ot(t,"a","notEqual"),s=ot(e,"b","notEqual");[n,s]=Y(n,s),re(n.shape,s.shape);const i={a:n,b:s};return st.runKernel("NotEqual",i)}});const en=ut({oneHot_:function(t,e,n=1,s=0){if(e<2)throw new Error("Error in oneHot: depth must be >=2, but it is "+e);const i={indices:ot(t,"indices","oneHot","int32")},r={depth:e,onValue:n,offValue:s};return st.runKernel("OneHot",i,r)}});function nn(t,e="float32"){if("complex64"===e){const e=nn(t,"float32"),n=nn(t,"float32");return Vt(e,n)}const n=b(a(t),e);return st.makeTensor(n,t,e)}function sn(t,e="float32"){if("complex64"===e){const e=sn(t,"float32"),n=nn(t,"float32");return Vt(e,n)}const n=y(a(t),e);return st.makeTensor(n,t,e)}const rn=ut({onesLike_:function(t){const e={x:ot(t,"x","onesLike")};return st.runKernel("OnesLike",e)}});const an=ut({pad_:function(t,e,n=0){const s=ot(t,"x","pad");if(0===s.rank)throw new Error("pad(scalar) is not defined. Pass non-scalar to pad");const i={paddings:e,constantValue:n},r={x:s};return st.runKernel("PadV2",r,i)}});const on=ut({spaceToBatchND_:function(t,e,n){const i=ot(t,"x","spaceToBatchND");s(i.rank>=1+e.length,()=>`input rank ${i.rank} should be > than [blockShape] ${e.length}`),s(n.length===e.length,()=>`paddings.shape[0] ${n.length} must be equal to [blockShape] ${e.length}`),s(i.shape.reduce((t,s,i)=>i>0&&i<=e.length?t&&(s+n[i-1][0]+n[i-1][1])%e[i-1]==0:t,!0),()=>`input spatial dimensions ${i.shape.slice(1)} with paddings ${n.toString()} must be divisible by blockShapes ${e.toString()}`);const r={x:i},a={blockShape:e,paddings:n};return st.runKernel("SpaceToBatchND",r,a)}});const ln=ut({pool_:function(t,e,n,i,r,a){null==r&&(r=[1,1]),null==a&&(a=1),0===i&&(i="valid");const o=ot(t,"x","maxPool");let l=o,u=!1;3===o.rank&&(u=!0,l=Tt(o,[1,o.shape[0],o.shape[1],o.shape[2]])),s(Dt(a,r),()=>`Error in pool: Either strides or dilations must be 1. Got strides ${a} and dilations '${r}'`);const h=It(l.shape,e,a,r,i),c=[h.dilationHeight,h.dilationWidth];let p;p="same"===i?function(t,e){const n=t.map((t,n)=>t+(t-1)*(e[n]-1)).map(t=>t-1),s=n.map(t=>Math.floor(t/2)),i=n.map((t,e)=>t-s[e]);return n.map((t,e)=>[s[e],i[e]])}([h.filterHeight,h.filterWidth],c):[[0,0],[0,0]];const d=1===c[0]&&1===c[1],[f,g]=function(t,e,n){const s=n.map(t=>t[0]),i=n.map(t=>t[1]),r=t.concat(s,i),a=e.map((t,e)=>(t-r[e]%t)%t),o=i.map((t,e)=>t+a[e]),l=e.map((t,e)=>[s[e],o[e]]),u=e.map((t,e)=>[0,a[e]]);return[l,u]}([h.inHeight,h.inWidth],c,p),m=d?i:"valid",y=d?l:on(l,c,f),b=("avg"===n?()=>Et(y,e,a,m):()=>qe(y,e,a,m))(),w=d?b:Bt(b,c,g);return u?Tt(w,[w.shape[1],w.shape[2],w.shape[3]]):w}});const un=ut({pow_:function(t,e){let n=ot(t,"base","pow"),s=ot(e,"exp","pow");[n,s]=Y(n,s);const i={a:n,b:s};return st.runKernel("Pow",i)}});const hn=ut({prelu_:function(t,e){const n={x:ot(t,"x","prelu"),alpha:ot(e,"alpha","prelu")};return st.runKernel("Prelu",n)}});const cn=ut({prod_:function(t,e=null,n=!1){let s=ot(t,"x","prod");"bool"===s.dtype&&(s=St(s,"int32"));const i={x:s},r={axis:e,keepDims:n};return st.runKernel("Prod",i,r)}});const pn=ut({real_:function(t){const e={input:ot(t,"input","real")};return st.runKernel("Real",e)}});const dn=ut({reciprocal_:function(t){const e={x:ot(t,"x","reciprocal")};return st.runKernel("Reciprocal",e)}});const fn=ut({relu_:function(t){const e={x:ot(t,"x","relu")};return st.runKernel("Relu",e)}});const gn=ut({relu6_:function(t){const e={x:ot(t,"x","relu6")};return st.runKernel("Relu6",e)}});const mn=ut({reverse_:function(t,e){const n={x:ot(t,"x","reverse")},s={dims:e};return st.runKernel("Reverse",n,s)}});const yn=ut({round_:function(t){const e={x:ot(t,"x","round")};return st.runKernel("Round",e)}});const bn=ut({rsqrt_:function(t){const e={x:ot(t,"x","rsqrt")};return st.runKernel("Rsqrt",e)}});function wn(t,e,n,i){if(null==i&&(i=d(t)),"complex64"===i)throw new Error("Cannot construct a complex64 tensor directly. Please use tf.complex(real, imag).");if(!c(t)&&!Array.isArray(t)&&"number"!=typeof t&&"boolean"!=typeof t&&"string"!=typeof t)throw new Error("values passed to tensor(values) must be a number/boolean/string or an array of numbers/booleans/strings, or a TypedArray");if(null!=e){!function(t){t.forEach(e=>{s(Number.isInteger(e)&&e>=0,()=>`Tensor must have a shape comprised of positive integers but got shape [${t}].`)})}(e);const t=a(e),i=a(n);s(t===i,()=>`Based on the provided shape, [${e}], the tensor should have ${t} values but has ${i}`);for(let t=0;t<n.length;++t){const i=n[t],r=t!==n.length-1||i!==a(e.slice(t));s(n[t]===e[t]||!r,()=>`Error creating a new Tensor. Inferred shape (${n}) does not match the provided shape (${e}). `)}}return c(t)||Array.isArray(t)||(t=[t]),e=e||n,t="string"!==i?F(t,i):r(t,[],!0),st.makeTensor(t,e,i)}function kn(t,e){if((c(t)&&"string"!==e||Array.isArray(t))&&"complex64"!==e)throw new Error("Error creating a new Scalar: value must be a primitive (number|boolean|string)");if("string"===e&&c(t)&&!(t instanceof Uint8Array))throw new Error("When making a scalar from encoded string, the value must be `Uint8Array`.");return wn(t,[],[],e)}const xn=ut({selu_:function(t){const e={x:ot(t,"x","selu")};return st.runKernel("Selu",e)}});const vn=ut({separableConv2d_:function(t,e,n,i,r,a=[1,1],o="NHWC"){const l=ot(t,"x","separableConv2d"),u=ot(e,"depthwiseFilter","separableConv2d"),h=ot(n,"pointwiseFilter","separableConv2d");let c=l,p=!1;if(3===l.rank&&(p=!0,c=Tt(l,[1,l.shape[0],l.shape[1],l.shape[2]])),"NCHW"===o)throw new Error("separableConv2d currently does not support dataFormat NCHW; only NHWC is supported");s(4===c.rank,()=>`Error in separableConv2d: input must be rank 4, but got rank ${c.rank}.`),s(4===u.rank,()=>`Error in separableConv2d: depthwise filter must be rank 4, but got rank ${u.rank}.`),s(4===h.rank,()=>`Error in separableConv2d: pointwise filter must be rank 4, but got rank ${u.rank}.`),s(1===h.shape[0],()=>`Error in separableConv2d: the first dimension of pointwise filter  must be 1, but got ${h.shape[0]}.`),s(1===h.shape[1],()=>`Error in separableConv2d: the second dimension of pointwise filter must be 1, but got ${h.shape[1]}.`);const d=u.shape[2],f=u.shape[3];s(h.shape[2]===d*f,()=>`Error in separableConv2d: the third dimension of pointwise filter must be ${d*f}, but got ${h.shape[2]}.`);const g=te(c,u,i,r,o,a),m=jt(g,h,1,"valid",o);return p?Tt(m,[m.shape[1],m.shape[2],m.shape[3]]):m}});const Sn=ut({sign_:function(t){const e={x:ot(t,"x","sign")};return st.runKernel("Sign",e)}});const In=ut({sin_:function(t){const e={x:ot(t,"x","sin")};return st.runKernel("Sin",e)}});const Nn=ut({sinh_:function(t){const e={x:ot(t,"x","sinh")};return st.runKernel("Sinh",e)}});const zn=ut({softmax_:function(t,e=-1){const n=ot(t,"logits","softmax","float32");if(-1===e&&(e=n.rank-1),e!==n.rank-1)throw Error(`Softmax along a non-last dimension is not yet supported. Logits was rank ${n.rank} and dim was ${e}`);const s={logits:n},i={dim:e};return st.runKernel("Softmax",s,i)}});const An=ut({fft_:function(t){s("complex64"===t.dtype,()=>`The dtype for tf.spectral.fft() must be complex64 but got ${t.dtype}.`);const e={input:t};return st.runKernel("FFT",e)}});const Cn=ut({ifft_:function(t){s("complex64"===t.dtype,()=>`The dtype for tf.spectral.ifft() must be complex64 but got ${t.dtype}.`);const e={input:t};return st.runKernel("IFFT",e)}});const Dn=ut({irfft_:function(t){const e=t.shape[t.shape.length-1],n=t.size/e;let s;if(e<=2){const i=Tt(t,[n,e]);s=Cn(i)}else{const i=[n,2*(e-1)],r=Tt(pn(t),[n,e]),a=Tt(xe(t),[n,e]),o=mn(Mt(r,[0,1],[n,e-2]),1),l=_t(mn(Mt(a,[0,1],[n,e-2]),1),kn(-1)),u=Lt([r,o],1),h=Lt([a,l],1),c=Tt(Vt(u,h),[i[0],i[1]]);s=Cn(c)}if(s=pn(s),3===t.rank&&0!==t.shape[0]){const e=s,n=t.shape[0];s=Tt(s,[n,s.shape[0]/n,s.shape[1]]),e.dispose()}return s}});const Tn=ut({split_:function(t,e,n=0){const s={x:ot(t,"x","split")},i={numOrSizeSplits:e,axis:n};return st.runKernel("SplitV",s,i)}});const En=ut({rfft_:function(t,e){s("float32"===t.dtype,()=>"The dtype for rfft() must be real value but got "+t.dtype);let n=t.shape[t.shape.length-1];const i=t.size/n;let r;if(null!=e&&e<n){const s=t.shape.map(t=>0),i=t.shape.map(t=>t);i[t.shape.length-1]=e,r=Mt(t,s,i),n=e}else if(null!=e&&e>n){const s=t.shape.map(t=>t);s[t.shape.length-1]=e-n,r=Lt([t,nn(s)],t.shape.length-1),n=e}else r=t;const a=le(r),o=Tt(Vt(r,a),[i,n]),l=An(o),u=Math.floor(n/2)+1,h=pn(l),c=xe(l),p=Tn(h,[u,n-u],h.shape.length-1),d=Tn(c,[u,n-u],c.shape.length-1),f=r.shape.slice();return f[r.shape.length-1]=u,Tt(Vt(p[0],d[0]),f)}});const Fn=ut({sqrt_:function(t){const e={x:ot(t,"x","sqrt")};return st.runKernel("Sqrt",e)}});const Ln=ut({squaredDifference_:function(t,e){let n=ot(t,"a","squaredDifference"),s=ot(e,"b","squaredDifference");[n,s]=Y(n,s),re(n.shape,s.shape);const i={a:n,b:s};return st.runKernel("SquaredDifference",i,{})}});const $n=ut({squeeze_:function(t,e){const n=ot(t,"x","squeeze");return Tt(n,function(t,e){const n=[],s=[],i=null!=e&&Array.isArray(e)&&0===e.length,r=null==e||i?null:h(e,t).sort();let a=0;for(let e=0;e<t.length;++e){if(null!=r){if(r[a]===e&&1!==t[e])throw new Error(`Can't squeeze axis ${e} since its dim '${t[e]}' is not 1`);(null==r[a]||r[a]>e)&&1===t[e]&&(n.push(t[e]),s.push(e)),r[a]<=e&&a++}1!==t[e]&&(n.push(t[e]),s.push(e))}return{newShape:n,keptDims:s}}(n.shape,e).newShape)}});const _n=ut({stack_:function(t,e=0){const n=lt(t,"tensors","stack","string_or_numeric");s(n.length>=1,()=>"Pass at least one tensor to tf.stack"),n.length>0&&s(e<=n[0].rank,()=>"Axis must be <= rank of the tensor");const i=n,r={axis:e};return st.runKernel("Pack",i,r)}});const Rn=ut({step_:function(t,e=0){const n={x:ot(t,"x","step")},s={alpha:e};return st.runKernel("Step",n,s)}});const Mn=ut({stridedSlice_:function(t,e,n,s,i=0,r=0,a=0,o=0,l=0){const u={x:ot(t,"x","stridedSlice")},h={begin:e,end:n,strides:s,beginMask:i,endMask:r,ellipsisMask:a,newAxisMask:o,shrinkAxisMask:l};return st.runKernel("StridedSlice",u,h)}});const On=ut({tan_:function(t){const e={x:ot(t,"x","tan")};return st.runKernel("Tan",e)}});const Bn=ut({topk_:function(t,e=1,n=!0){const s=ot(t,"x","topk");if(0===s.rank)throw new Error("topk() expects the input to be of rank 1 or higher");const i=s.shape[s.shape.length-1];if(e>i)throw new Error(`'k' passed to topk() must be <= the last dimension (${i}) but got `+e);const r={x:s},a={k:e,sorted:n},[o,l]=st.runKernel("TopK",r,a);return{values:o,indices:l}}});const Pn=ut({unique_:function(t,e=0){const n=ot(t,"x","unique","string_or_numeric");s(n.rank>0,()=>"The input tensor must be at least 1D");const i={x:n},r={axis:e},[a,o]=st.runKernel("Unique",i,r);return{values:a,indices:o}}});const Wn=ut({unsortedSegmentSum_:function(t,e,n){const i=ot(t,"x","unsortedSegmentSum"),r=ot(e,"segmentIds","unsortedSegmentSum","int32");s(l(n),()=>"numSegments must be of dtype int");const a={x:i,segmentIds:r},o={numSegments:n};return st.runKernel("UnsortedSegmentSum",a,o)}});const Un=ut({unstack_:function(t,e=0){const n=ot(t,"x","unstack","string_or_numeric");s(e>=-n.shape.length&&e<n.shape.length,()=>`Axis = ${e} is not in [-${n.shape.length}, ${n.shape.length})`);const i={value:n},r={axis:e};return st.runKernel("Unpack",i,r)}});const Kn=ut({transpose_:function(t,e){const n=ot(t,"x","transpose");if(null==e&&(e=n.shape.map((t,e)=>e).reverse()),s(n.rank===e.length,()=>`Error in transpose: rank of input ${n.rank} must match length of perm ${e}.`),e.forEach(t=>{s(t>=0&&t<n.rank,()=>"All entries in 'perm' must be between 0 and "+(n.rank-1)+" but got "+e)}),n.rank<=1)return n.clone();const i={x:n},r={perm:e};return st.runKernel("Transpose",i,r)}});const Vn=ut({norm_:function(t,e="euclidean",n=null,s=!1){const i=function t(e,n,s=null){if(0===e.rank)return ht(e);if(1!==e.rank&&null===s)return t(Tt(e,[-1]),n,s);if(1===e.rank||"number"==typeof s||Array.isArray(s)&&1===s.length){if(1===n)return Me(ht(e),s);if(n===1/0)return _e(ht(e),s);if(n===-1/0)return Je(ht(e),s);if("euclidean"===n||2===n)return Fn(Me(un(ht(e),kn(2,"int32")),s));throw new Error("Error in norm: invalid ord value: "+n)}if(Array.isArray(s)&&2===s.length){if(1===n)return _e(Me(ht(e),s[0]),s[1]-1);if(n===1/0)return _e(Me(ht(e),s[1]),s[0]);if(n===-1/0)return Je(Me(ht(e),s[1]),s[0]);if("fro"===n||"euclidean"===n)return Fn(Me(Qe(e),s));throw new Error("Error in norm: invalid ord value: "+n)}throw new Error("Error in norm: invalid axis: "+s)}(t=ot(t,"x","norm"),e,n);let r=i.shape;if(s){const e=h(n,t.shape);r=Be(i.shape,e)}return Tt(i,r)}});const jn=ut({conv2DBackpropFilter_:function(t,e,n,i,r,a="NHWC",o){let u=t;3===t.rank&&(u=Tt(t,[1,t.shape[0],t.shape[1],t.shape[2]]));let h=e;3===h.rank&&(h=Tt(e,[1,e.shape[0],e.shape[1],e.shape[2]])),s(4===u.rank,()=>"Error in conv2dDerFilter: input must be rank 4, but got shape "+u.shape+"."),s(4===h.rank,()=>"Error in conv2dDerFilter: dy must be rank 4, but got shape "+h.shape+"."),s(4===n.length,()=>"Error in conv2dDerFilter: filterShape must be length 4, but got "+n+".");const c="NHWC"===a?u.shape[3]:u.shape[1],p="NHWC"===a?h.shape[3]:h.shape[1];s(c===n[2],()=>`Error in conv2dDerFilter: depth of input ${c}) must match input depth in filter (${n[2]}.`),s(p===n[3],()=>`Error in conv2dDerFilter: depth of dy (${p}) must match output depth for filter (${n[3]}).`),null!=o&&s(l(r),()=>`Error in conv2dDerFilter: pad must be an integer when using, dimRoundingMode ${o} but got pad ${r}.`);const d={x:u,dy:h},f={strides:i,pad:r,dataFormat:a,dimRoundingMode:o,filterShape:n};return st.runKernel("Conv2DBackpropFilter",d,f)}});const qn=ut({depthwiseConv2dNativeBackpropFilter_:function(t,e,n,s,i,r=[1,1],a){let o=t;3===t.rank&&(o=Tt(t,[1,t.shape[0],t.shape[1],t.shape[2]]));let l=e;3===l.rank&&(l=Tt(e,[1,e.shape[0],e.shape[1],e.shape[2]]));const u={x:o,dy:l},h={strides:s,pad:i,dimRoundingMode:a,dilations:r,filterShape:n};return st.runKernel("DepthwiseConv2dNativeBackpropFilter",u,h)}});const Gn=ut({depthwiseConv2dNativeBackpropInput_:function(t,e,n,s,i,r=[1,1],a){let o=e,l=!1;3===e.rank&&(l=!0,o=Tt(e,[1,e.shape[0],e.shape[1],e.shape[2]]));const u={dy:o,filter:n},h={strides:s,pad:i,dimRoundingMode:a,dilations:r,inputShape:t},c=st.runKernel("DepthwiseConv2dNativeBackpropInput",u,h);return l?Tt(c,[c.shape[1],c.shape[2],c.shape[3]]):c}});const Hn=ut({resizeBilinear_:function(t,e,n=!1,i=!1){const r=ot(t,"images","resizeBilinear");s(3===r.rank||4===r.rank,()=>`Error in resizeBilinear: x must be rank 3 or 4, but got rank ${r.rank}.`),s(2===e.length,()=>"Error in resizeBilinear: new shape must 2D, but got shape "+e+"."),s(!1===i||!1===n,()=>"Error in resizeBilinear: If halfPixelCenters is true, alignCorners must be false.");let a=r,o=!1;3===r.rank&&(o=!0,a=Tt(r,[1,r.shape[0],r.shape[1],r.shape[2]]));const l={images:a},u={alignCorners:n,halfPixelCenters:i,size:e},h=st.runKernel("ResizeBilinear",l,u);return o?Tt(h,[h.shape[1],h.shape[2],h.shape[3]]):h}});const Jn=ut({resizeNearestNeighbor_:function(t,e,n=!1,i=!1){const r=ot(t,"images","resizeNearestNeighbor");s(3===r.rank||4===r.rank,()=>`Error in resizeNearestNeighbor: x must be rank 3 or 4, but got rank ${r.rank}.`),s(2===e.length,()=>"Error in resizeNearestNeighbor: new shape must 2D, but got shape "+e+"."),s("float32"===r.dtype||"int32"===r.dtype,()=>"`images` must have `int32` or `float32` as dtype"),s(!1===i||!1===n,()=>"Error in resizeNearestNeighbor: If halfPixelCenters is true, alignCorners must be false.");let a=r,o=!1;3===r.rank&&(o=!0,a=Tt(r,[1,r.shape[0],r.shape[1],r.shape[2]]));const l={images:a},u={alignCorners:n,halfPixelCenters:i,size:e},h=st.runKernel("ResizeNearestNeighbor",l,u);return o?Tt(h,[h.shape[1],h.shape[2],h.shape[3]]):h}});V().prototype.abs=function(){return this.throwIfDisposed(),ht(this)},V().prototype.acos=function(){return this.throwIfDisposed(),ct(this)},V().prototype.acosh=function(){return this.throwIfDisposed(),pt(this)},V().prototype.add=function(t){return this.throwIfDisposed(),dt(this,t)},V().prototype.all=function(t,e){return this.throwIfDisposed(),ft(this,t,e)},V().prototype.any=function(t,e){return this.throwIfDisposed(),gt(this,t,e)},V().prototype.argMax=function(t){return this.throwIfDisposed(),mt(this,t)},V().prototype.argMin=function(t){return this.throwIfDisposed(),yt(this,t)},V().prototype.asScalar=function(){return this.throwIfDisposed(),s(1===this.size,()=>"The array must have only 1 element."),Tt(this,[])},V().prototype.asType=function(t){return this.throwIfDisposed(),St(this,t)},V().prototype.as1D=function(){return this.throwIfDisposed(),Tt(this,[this.size])},V().prototype.as2D=function(t,e){return this.throwIfDisposed(),Tt(this,[t,e])},V().prototype.as3D=function(t,e,n){return this.throwIfDisposed(),Tt(this,[t,e,n])},V().prototype.as4D=function(t,e,n,s){return this.throwIfDisposed(),Tt(this,[t,e,n,s])},V().prototype.as5D=function(t,e,n,s,i){return this.throwIfDisposed(),Tt(this,[t,e,n,s,i])},V().prototype.asin=function(){return this.throwIfDisposed(),bt(this)},V().prototype.asinh=function(){return this.throwIfDisposed(),wt(this)},V().prototype.atan=function(){return this.throwIfDisposed(),kt(this)},V().prototype.atan2=function(t){return this.throwIfDisposed(),xt(this,t)},V().prototype.atanh=function(){return this.throwIfDisposed(),vt(this)},V().prototype.avgPool=function(t,e,n,s){return this.throwIfDisposed(),Et(this,t,e,n,s)},V().prototype.batchToSpaceND=function(t,e){return this.throwIfDisposed(),Bt(this,t,e)},V().prototype.batchNorm=function(t,e,n,s,i){return this.throwIfDisposed(),Pt(this,t,e,n,s,i)},V().prototype.broadcastTo=function(t){return this.throwIfDisposed(),Wt(this,t)},V().prototype.cast=function(t){return this.throwIfDisposed(),St(this,t)},V().prototype.ceil=function(){return this.throwIfDisposed(),Ut(this)},V().prototype.clipByValue=function(t,e){return this.throwIfDisposed(),Kt(this,t,e)},V().prototype.concat=function(t,e){return this.throwIfDisposed(),t instanceof K&&(t=[t]),Lt([this,...t],e)},V().prototype.conv1d=function(t,e,n,s,i,r){return this.throwIfDisposed(),qt(this,t,e,n,s,i,r)},V().prototype.conv2dTranspose=function(t,e,n,s,i){return this.throwIfDisposed(),Ht(this,t,e,n,s,i)},V().prototype.conv2d=function(t,e,n,s,i,r){return this.throwIfDisposed(),jt(this,t,e,n,s,i,r)},V().prototype.cos=function(){return this.throwIfDisposed(),Zt(this)},V().prototype.cosh=function(){return this.throwIfDisposed(),Xt(this)},V().prototype.cumsum=function(t,e,n){return this.throwIfDisposed(),Yt(this,t,e,n)},V().prototype.depthToSpace=function(t,e){return this.throwIfDisposed(),Qt(this,t,e)},V().prototype.depthwiseConv2d=function(t,e,n,s,i,r){return this.throwIfDisposed(),te(this,t,e,n,s,i,r)},V().prototype.dilation2d=function(t,e,n,s,i){return this.throwIfDisposed(),ee(this,t,e,n,s,i)},V().prototype.divNoNan=function(t){return this.throwIfDisposed(),ue(this,t)},V().prototype.div=function(t){return this.throwIfDisposed(),se(this,t)},V().prototype.dot=function(t){return this.throwIfDisposed(),he(this,t)},V().prototype.elu=function(){return this.throwIfDisposed(),ce(this)},V().prototype.equal=function(t){return this.throwIfDisposed(),ae(this,t)},V().prototype.erf=function(){return this.throwIfDisposed(),pe(this)},V().prototype.exp=function(){return this.throwIfDisposed(),de(this)},V().prototype.expandDims=function(t){return this.throwIfDisposed(),fe(this,t)},V().prototype.expm1=function(){return this.throwIfDisposed(),ge(this)},V().prototype.fft=function(){return this.throwIfDisposed(),An(this)},V().prototype.flatten=function(){return this.throwIfDisposed(),Tt(this,[this.size])},V().prototype.floor=function(){return this.throwIfDisposed(),ye(this)},V().prototype.floorDiv=function(t){return this.throwIfDisposed(),ne(this,t)},V().prototype.gather=function(t,e){return this.throwIfDisposed(),be(this,t,e)},V().prototype.greaterEqual=function(t){return this.throwIfDisposed(),ke(this,t)},V().prototype.greater=function(t){return this.throwIfDisposed(),we(this,t)},V().prototype.ifft=function(){return this.throwIfDisposed(),Cn(this)},V().prototype.irfft=function(){return this.throwIfDisposed(),Dn(this)},V().prototype.isFinite=function(){return this.throwIfDisposed(),ve(this)},V().prototype.isInf=function(){return this.throwIfDisposed(),Se(this)},V().prototype.isNaN=function(){return this.throwIfDisposed(),Ie(this)},V().prototype.leakyRelu=function(t){return this.throwIfDisposed(),Ne(this,t)},V().prototype.lessEqual=function(t){return this.throwIfDisposed(),Ae(this,t)},V().prototype.less=function(t){return this.throwIfDisposed(),ze(this,t)},V().prototype.localResponseNormalization=function(t,e,n,s){return this.throwIfDisposed(),Ce(this,t,e,n,s)},V().prototype.logSigmoid=function(){return this.throwIfDisposed(),$e(this)},V().prototype.logSoftmax=function(t){return this.throwIfDisposed(),Oe(this,t)},V().prototype.logSumExp=function(t,e){return this.throwIfDisposed(),We(this,t,e)},V().prototype.log=function(){return this.throwIfDisposed(),De(this)},V().prototype.log1p=function(){return this.throwIfDisposed(),Te(this)},V().prototype.logicalAnd=function(t){return this.throwIfDisposed(),Ue(this,t)},V().prototype.logicalNot=function(){return this.throwIfDisposed(),Ke(this)},V().prototype.logicalOr=function(t){return this.throwIfDisposed(),Ve(this,t)},V().prototype.logicalXor=function(t){return this.throwIfDisposed(),je(this,t)},V().prototype.matMul=function(t,e,n){return this.throwIfDisposed(),$t(this,t,e,n)},V().prototype.maxPool=function(t,e,n,s){return this.throwIfDisposed(),qe(this,t,e,n,s)},V().prototype.max=function(t,e){return this.throwIfDisposed(),_e(this,t,e)},V().prototype.maximum=function(t){return this.throwIfDisposed(),Ge(this,t)},V().prototype.mean=function(t,e){return this.throwIfDisposed(),He(this,t,e)},V().prototype.min=function(t,e){return this.throwIfDisposed(),Je(this,t,e)},V().prototype.minimum=function(t){return this.throwIfDisposed(),Ze(this,t)},V().prototype.mirrorPad=function(t,e){return this.throwIfDisposed(),Xe(this,t,e)},V().prototype.mod=function(t){return this.throwIfDisposed(),Ye(this,t)},V().prototype.mul=function(t){return this.throwIfDisposed(),_t(this,t)},V().prototype.neg=function(){return this.throwIfDisposed(),Fe(this)},V().prototype.norm=function(t,e,n){return this.throwIfDisposed(),Vn(this,t,e,n)},V().prototype.notEqual=function(t){return this.throwIfDisposed(),tn(this,t)},V().prototype.oneHot=function(t,e=1,n=0){return this.throwIfDisposed(),en(this,t,e,n)},V().prototype.onesLike=function(){return this.throwIfDisposed(),rn(this)},V().prototype.pad=function(t,e){return this.throwIfDisposed(),an(this,t,e)},V().prototype.pool=function(t,e,n,s,i){return this.throwIfDisposed(),ln(this,t,e,n,s,i)},V().prototype.pow=function(t){return this.throwIfDisposed(),un(this,t)},V().prototype.prelu=function(t){return this.throwIfDisposed(),hn(this,t)},V().prototype.prod=function(t,e){return this.throwIfDisposed(),cn(this,t,e)},V().prototype.reciprocal=function(){return this.throwIfDisposed(),dn(this)},V().prototype.relu=function(){return this.throwIfDisposed(),fn(this)},V().prototype.relu6=function(){return this.throwIfDisposed(),gn(this)},V().prototype.reshapeAs=function(t){return this.throwIfDisposed(),Tt(this,t.shape)},V().prototype.reshape=function(t){return this.throwIfDisposed(),Tt(this,t)},V().prototype.resizeBilinear=function(t,e,n){return this.throwIfDisposed(),Hn(this,t,e,n)},V().prototype.resizeNearestNeighbor=function(t,e,n){return this.throwIfDisposed(),Jn(this,t,e,n)},V().prototype.reverse=function(t){return this.throwIfDisposed(),mn(this,t)},V().prototype.rfft=function(){return this.throwIfDisposed(),En(this)},V().prototype.round=function(){return this.throwIfDisposed(),yn(this)},V().prototype.rsqrt=function(){return this.throwIfDisposed(),bn(this)},V().prototype.selu=function(){return this.throwIfDisposed(),xn(this)},V().prototype.separableConv2d=function(t,e,n,s,i,r){return this.throwIfDisposed(),vn(this,t,e,n,s,i,r)},V().prototype.sigmoid=function(){return this.throwIfDisposed(),Rt(this)},V().prototype.sign=function(){return this.throwIfDisposed(),Sn(this)},V().prototype.sin=function(){return this.throwIfDisposed(),In(this)},V().prototype.sinh=function(){return this.throwIfDisposed(),Nn(this)},V().prototype.slice=function(t,e){return this.throwIfDisposed(),Mt(this,t,e)},V().prototype.softmax=function(t){return this.throwIfDisposed(),zn(this,t)},V().prototype.softplus=function(){return this.throwIfDisposed(),Le(this)},V().prototype.spaceToBatchND=function(t,e){return this.throwIfDisposed(),on(this,t,e)},V().prototype.split=function(t,e){return this.throwIfDisposed(),Tn(this,t,e)},V().prototype.sqrt=function(){return this.throwIfDisposed(),Fn(this)},V().prototype.square=function(){return this.throwIfDisposed(),Qe(this)},V().prototype.squaredDifference=function(t){return this.throwIfDisposed(),Ln(this,t)},V().prototype.squeeze=function(t){return this.throwIfDisposed(),$n(this,t)},V().prototype.stack=function(t,e){this.throwIfDisposed();const n=t instanceof K?[this,t]:[this,...t];return _n(n,e)},V().prototype.step=function(t){return this.throwIfDisposed(),Rn(this,t)},V().prototype.stridedSlice=function(t,e,n,s,i,r,a,o){return this.throwIfDisposed(),Mn(this,t,e,n,s,i,r,a,o)},V().prototype.sub=function(t){return this.throwIfDisposed(),Re(this,t)},V().prototype.sum=function(t,e){return this.throwIfDisposed(),Me(this,t,e)},V().prototype.tan=function(){return this.throwIfDisposed(),On(this)},V().prototype.tanh=function(){return this.throwIfDisposed(),Ot(this)},V().prototype.tile=function(t){return this.throwIfDisposed(),me(this,t)},V().prototype.toBool=function(){return this.throwIfDisposed(),St(this,"bool")},V().prototype.toFloat=function(){return this.throwIfDisposed(),St(this,"float32")},V().prototype.toInt=function(){return this.throwIfDisposed(),St(this,"int32")},V().prototype.topk=function(t,e){return this.throwIfDisposed(),Bn(this,t,e)},V().prototype.transpose=function(t){return this.throwIfDisposed(),Kn(this,t)},V().prototype.unique=function(t){return this.throwIfDisposed(),Pn(this,t)},V().prototype.unsortedSegmentSum=function(t,e){return this.throwIfDisposed(),Wn(this,t,e)},V().prototype.unstack=function(t){return this.throwIfDisposed(),Un(this,t)},V().prototype.where=function(t,e){return this.throwIfDisposed(),oe(t,this,e)},V().prototype.zerosLike=function(){return this.throwIfDisposed(),le(this)};const Zn={kernelName:"Abs",inputsToSave:["x"],gradFunc:(t,e)=>{const[n]=e;return{x:()=>_t(t,Rn(St(n,"float32"),-1))}}},Xn={kernelName:"Acos",inputsToSave:["x"],gradFunc:(t,e)=>{const[n]=e;return{x:()=>{const e=Qe(St(n,"float32")),s=Fn(Re(kn(1),e));return Fe(se(t,s))}}}},Yn={kernelName:"Acosh",inputsToSave:["x"],gradFunc:(t,e)=>{const[n]=e;return{x:()=>{const e=Fn(Re(Qe(St(n,"float32")),1));return se(t,e)}}}},Qn={kernelName:"Add",inputsToSave:["a","b"],gradFunc:(t,e)=>{const[n,s]=e,i=re(n.shape,s.shape);return{a:()=>{let e=t;const s=ie(n.shape,i);return s.length>0&&(e=Me(e,s)),Tt(e,n.shape)},b:()=>{let e=t;const n=ie(s.shape,i);return n.length>0&&(e=Me(e,n)),Tt(e,s.shape)}}}},ts={kernelName:"AddN",saveAllInputs:!0,gradFunc:(t,e)=>{const n={};return e.forEach((e,s)=>{n[s]=()=>t.clone()}),n}},es={kernelName:"ArgMax",inputsToSave:["x"],gradFunc:(t,e)=>{const[n]=e;return{x:()=>le(n)}}},ns={kernelName:"ArgMin",inputsToSave:["x"],gradFunc:(t,e)=>{const[n]=e;return{x:()=>le(n)}}},ss={kernelName:"Asin",inputsToSave:["x"],gradFunc:(t,e)=>{const[n]=e;return{x:()=>se(t,Fn(Re(kn(1),Qe(St(n,"float32")))))}}},is={kernelName:"Asinh",inputsToSave:["x"],gradFunc:(t,e)=>{const[n]=e;return{x:()=>{const e=Fn(dt(kn(1),Qe(St(n,"float32"))));return se(t,e)}}}},rs={kernelName:"Atan2",inputsToSave:["a","b"],gradFunc:(t,e)=>{const[n,s]=e,i=re(n.shape,s.shape);return{a:()=>{const e=dt(Qe(n),Qe(s));let r=_t(t,se(s,e));const a=ie(n.shape,i);return a.length>0&&(r=Me(r,a)),Tt(r,n.shape)},b:()=>{const e=dt(Qe(n),Qe(s));let r=Fe(_t(t,se(n,e)));const a=ie(s.shape,i);return a.length>0&&(r=Me(r,a)),Tt(r,s.shape)}}}},as={kernelName:"Atan",inputsToSave:["x"],gradFunc:(t,e)=>{const[n]=e;return{x:()=>se(t,dt(Qe(St(n,"float32")),1))}}},os={kernelName:"Atanh",inputsToSave:["x"],gradFunc:(t,e)=>{const[n]=e;return{x:()=>se(t,Re(kn(1),Qe(St(n,"float32"))))}}};const ls=ut({avgPool3dGrad_:function(t,e,n,i,r,a){const o=ot(t,"dy","avgPool3dGrad"),u=ot(e,"input","avgPool3dGrad");let h=o,c=u,p=!1;4===u.rank&&(p=!0,h=Tt(o,[1,o.shape[0],o.shape[1],o.shape[2],o.shape[3]]),c=Tt(u,[1,u.shape[0],u.shape[1],u.shape[2],u.shape[3]])),s(5===h.rank,()=>"Error in avgPool3dGrad: dy must be rank 5 but got rank "+h.rank+"."),s(5===c.rank,()=>"Error in avgPool3dGrad: input must be rank 5 but got rank "+c.rank+"."),null!=a&&s(l(r),()=>`Error in avgPool3dGrad: pad must be an integer when using, dimRoundingMode ${a} but got pad ${r}.`);const d={dy:h,input:c},f={filterSize:n,strides:i,pad:r,dimRoundingMode:a},g=st.runKernel("AvgPool3DGrad",d,f);return p?Tt(g,[g.shape[1],g.shape[2],g.shape[3],g.shape[4]]):g}}),us={kernelName:"AvgPool3D",inputsToSave:["x"],gradFunc:(t,e,n)=>{const[s]=e,{filterSize:i,strides:r,pad:a,dimRoundingMode:o}=n;return{x:()=>ls(t,s,i,r,a,o)}}};const hs=ut({avgPoolGrad_:function(t,e,n,i,r){const a=ot(t,"dy","avgPoolGrad"),o=ot(e,"input","avgPoolGrad");s(o.rank===a.rank,()=>`Rank of input (${o.rank}) does not match rank of dy (${a.rank})`);let l=o,u=a,h=!1;3===o.rank&&(h=!0,l=Tt(o,[1,o.shape[0],o.shape[1],o.shape[2]]),u=Tt(a,[1,a.shape[0],a.shape[1],a.shape[2]])),s(4===u.rank,()=>"Error in avgPoolGrad: dy must be rank 4 but got rank "+u.rank+"."),s(4===l.rank,()=>"Error in avgPoolGrad: input must be rank 4 but got rank "+l.rank+".");const c={dy:u,input:l},p={filterSize:n,strides:i,pad:r},d=st.runKernel("AvgPoolGrad",c,p);return h?Tt(d,[d.shape[1],d.shape[2],d.shape[3]]):d}}),cs={kernelName:"AvgPool",inputsToSave:["x"],gradFunc:(t,e,n)=>{const[s]=e,{filterSize:i,strides:r,pad:a}=n;return{x:()=>hs(t,s,i,r,a)}}},ps={kernelName:"BatchMatMul",inputsToSave:["a","b"],gradFunc:(t,e,n)=>{const[s,i]=e,{transposeA:r,transposeB:a}=n;return r||a?!r&&a?{a:()=>$t(t,i,!1,!1),b:()=>$t(t,s,!0,!1)}:r&&!a?{a:()=>$t(i,t,!1,!0),b:()=>$t(s,t,!1,!1)}:{a:()=>$t(i,t,!0,!0),b:()=>$t(t,s,!0,!0)}:{a:()=>$t(t,i,!1,!0),b:()=>$t(s,t,!0,!1)}}},ds={kernelName:"BatchToSpaceND",gradFunc:(t,e,n)=>{const{blockShape:s,crops:i}=n;return{x:()=>on(t,s,i)}}},fs={kernelName:"BroadcastTo",gradFunc:(t,e,n)=>{const s=n,i=s.inputShape,r=s.shape,a=Array.from(r);for(let t=i.length-1;t>=0;t--)if(i[t]===r[t])a[t]=1;else if(1!==i[t])throw new Error(`broadcastTo(): [${i}] cannot be broadcast to [${r}].`);const o=[];for(let t=0;t<a.length;t++)a[t]>1&&o.push(t);return{x:()=>Me(t,o,!0)}}},gs={kernelName:"Cast",gradFunc:t=>({x:()=>t.clone()})},ms={kernelName:"Ceil",gradFunc:t=>({x:()=>le(t)})},ys={kernelName:"ClipByValue",inputsToSave:["x"],gradFunc:(t,e,n)=>{const[s]=e,{clipValueMin:i,clipValueMax:r}=n;return{x:()=>oe(Ue(ke(s,i),Ae(s,r)),t,le(t))}}},bs={kernelName:"ComplexAbs",inputsToSave:["x"],gradFunc:Zn.gradFunc},ws={kernelName:"Concat",saveAllInputs:!0,gradFunc:(t,e,n)=>{const s=e.map(t=>t.shape),{axis:i}=n,r=h(i,e[0].shape)[0],a=s.map(t=>t[r]);return Tn(t,a,r).map(t=>()=>t)}},ks={kernelName:"Conv2D",inputsToSave:["x","filter"],gradFunc:(t,e,n)=>{const[i,r]=e,{dilations:a,strides:o,pad:l,dataFormat:u}=n;return s(Ct(a),()=>`Error in gradient of conv2D: dilation rates greater than 1 are not yet supported in gradients. Got dilations '${a}'`),{x:()=>Gt(i.shape,t,r,o,l,u),filter:()=>jn(i,t,r.shape,o,l,u)}}},xs={kernelName:"Conv2DBackpropInput",inputsToSave:["dy","filter"],gradFunc:(t,e,n)=>{const[s,i]=e,{strides:r,pad:a,dataFormat:o,dimRoundingMode:l}=n;return{dy:()=>jt(t,i,r,a,o,1,l),filter:()=>jn(t,s,i.shape,r,a,o,l)}}};const vs=ut({conv3DBackpropFilter_:function(t,e,n,i,r){let a=t;4===t.rank&&(a=Tt(t,[1,t.shape[0],t.shape[1],t.shape[2],t.shape[3]]));let o=e;4===o.rank&&(o=Tt(e,[1,e.shape[0],e.shape[1],e.shape[2],e.shape[3]])),s(5===a.rank,()=>"Error in conv3dDerFilter: input must be rank 5, but got shape "+a.shape+"."),s(5===o.rank,()=>"Error in conv3dDerFilter: dy must be rank 5, but got shape "+o.shape+"."),s(5===n.length,()=>"Error in conv3dDerFilter: filterShape must be length 5, but got "+n+"."),s(a.shape[4]===n[3],()=>`Error in conv3dDerFilter: depth of input ${a.shape[4]}) must match input depth in filter (${n[3]}.`),s(o.shape[4]===n[4],()=>`Error in conv3dDerFilter: depth of dy (${o.shape[4]}) must match output depth for filter (${n[4]}).`);const l={x:a,dy:o},u={strides:i,pad:r,filterShape:n};return st.runKernel("Conv3DBackpropFilterV2",l,u)}}),Ss={kernelName:"Conv3D",inputsToSave:["x","filter"],gradFunc:(t,e,n)=>{const{dilations:i,strides:r,pad:a}=n;s(Ct(i),()=>`Error in gradient of conv3D: dilation rates greater than 1 are not yet supported in gradients. Got dilations '${i}'`);const[o,l]=e;return{x:()=>Jt(o.shape,t,l,r,a),filter:()=>vs(o,t,l.shape,r,a)}}},Is={kernelName:"Cos",inputsToSave:["x"],gradFunc:(t,e)=>{const[n]=e;return{x:()=>_t(Fe(In(St(n,"float32"))),t)}}},Ns={kernelName:"Cosh",inputsToSave:["x"],gradFunc:(t,e)=>{const[n]=e;return{x:()=>_t(Nn(St(n,"float32")),t)}}},zs={kernelName:"Cumsum",inputsToSave:["x"],gradFunc:(t,e,n)=>{const[s]=e,{axis:i,exclusive:r,reverse:a}=n;return{x:()=>{const e=function(t,e){if(function(t,e){for(let n=0;n<t.length;++n)if(t[t.length-n-1]!==e-1-n)return!1;return!0}(t,e))return null;const n=[];for(let s=0;s<e;++s)-1===t.indexOf(s)&&n.push(s);return t.forEach(t=>n.push(t)),n}([i],s.rank);let n=Yt(t,i,r,!a);return null!=e&&(n=Kn(n,e)),n}}}},As={kernelName:"DepthwiseConv2dNative",inputsToSave:["x","filter"],gradFunc:(t,e,n)=>{const{dilations:i,strides:r,pad:a,dimRoundingMode:o}=n,u=null==i?[1,1]:i;s(Ct(u),()=>`Error in gradient of depthwiseConv2dNative: dilation rates greater than 1 are not yet supported. Got dilations '${u}'`);const[h,c]=e;return s(4===h.rank,()=>`Error in gradient of depthwiseConv2dNative: input must be rank 4, but got rank ${h.rank}.`),s(4===c.rank,()=>`Error in gradient of depthwiseConv2dNative: filter must be rank 4, but got rank ${c.rank}.`),s(h.shape[3]===c.shape[2],()=>`Error in gradient of depthwiseConv2d: number of input channels (${h.shape[3]}) must match the inChannels dimension in filter ${c.shape[2]}.`),s(Dt(r,u),()=>`Error in gradient of depthwiseConv2d: Either strides or dilations must be  1. Got strides ${r} and dilations '${u}'.`),null!=o&&s(l(a),()=>`Error in depthwiseConv2d: pad must be an integer when using, dimRoundingMode ${o} but got pad ${a}.`),{x:()=>Gn(h.shape,t,c,r,a,i,o),filter:()=>qn(h,t,c.shape,r,a,i,o)}}},Cs={kernelName:"Dilation2D",inputsToSave:["x","filter"],gradFunc:(t,e,n)=>{const[s,i]=e,r={x:s,filter:i,dy:t},a={x:s,filter:i,dy:t};return{x:()=>st.runKernel("Dilation2DBackpropInput",r,n),filter:()=>st.runKernel("Dilation2DBackpropFilter",a,n)}}},Ds={kernelName:"Elu",outputsToSave:[!0],gradFunc:(t,e)=>{const[n]=e,s={dy:t,y:n};return{x:()=>st.runKernel("EluGrad",s)}}},Ts={kernelName:"Erf",inputsToSave:["x"],gradFunc:(t,e)=>{const[n]=e,s=_t(de(Fe(Qe(n))),2/Math.sqrt(Math.PI));return{x:()=>_t(t,s)}}},Es={kernelName:"Exp",outputsToSave:[!0],gradFunc:(t,e)=>{const[n]=e;return{x:()=>_t(t,n)}}},Fs={kernelName:"ExpandDims",inputsToSave:["input"],gradFunc:(t,e)=>{const[n]=e;return{input:()=>Tt(t,n.shape)}}},Ls={kernelName:"Expm1",inputsToSave:["x"],gradFunc:(t,e)=>{const[n]=e;return{x:()=>_t(t,de(n))}}},$s={kernelName:"Floor",gradFunc:t=>({x:()=>le(t)})},_s={kernelName:"FloorDiv",inputsToSave:["a","b"],gradFunc:(t,e)=>{const[n,s]=e,i=re(n.shape,s.shape);return{a:()=>{const e=se(t,St(s,"float32")),r=ie(n.shape,i);return r.length>0?Tt(Me(e,r),n.shape):e},b:()=>{let e=_t(t,St(n,"float32"));const r=ie(s.shape,i);r.length>0&&(e=Tt(Me(e,r),s.shape));const a=Qe(s);return Fe(se(e,St(a,"float32")))}}}},Rs={kernelName:"FusedBatchNorm",inputsToSave:["x","mean","variance","scale"],gradFunc:(t,e,n)=>{const{varianceEpsilon:s}=n,[i,r,a,o]=e,l=null==o?kn(1):o,u=ie(r.shape,i.shape),h=[];if(1===r.rank){for(let t=0;t<i.shape.length-1;++t)h.push(i.shape[t]);h.push(1)}const c=Re(i,r),p=_t(t,l),d=bn(dt(a,kn(s))),f=_t(_t(_t(d,d),d),kn(-.5));return{x:()=>1===r.rank?Tt(_t(_t(t,me(Tt(d,[1,1,1,r.shape[0]]),h)),l),i.shape):Tt(_t(_t(t,d),l),i.shape),mean:()=>{let t=_t(_t(d,kn(-1)),p);return 1===r.rank&&(t=Me(t,u)),Tt(t,r.shape)},variance:()=>{let t=_t(_t(f,c),p);return 1===r.rank&&(t=Me(t,u)),Tt(t,r.shape)},scale:()=>{const e=_t(c,d);let n=_t(t,e);return 1===r.rank&&(n=Me(n,u)),Tt(n,r.shape)},offset:()=>{let e=t;return 1===r.rank&&(e=Me(e,u)),Tt(e,r.shape)}}}},Ms={kernelName:"GatherV2",inputsToSave:["x","indices"],gradFunc:(t,e,n)=>{const[s,i]=e,{axis:r}=n,a=h(r,s.shape)[0];return{x:()=>{const e=s.shape,n=i.size,o=e.slice(0,a),l=o.length,u=e.slice(r,e.length).slice(1),h=u.length,c=Os(0,l),p=Os(l+1,l+1+h),d=Bs([o,[n],u]),f=Tt(t,d),g=Tt(i,[n]),m=Bs([[l],c,p]),y=Kn(f,m);let b=Wn(y,g,s.shape[a]);const w=Pe(m);return b=Kn(b,w),b},indices:()=>i}}};function Os(t,e){const n=[];for(let s=t;s<e;++s)n.push(s);return n}function Bs(t){const e=[];for(let n=0;n<t.length;++n)for(let s=0;s<t[n].length;++s)e.push(t[n][s]);return e}const Ps={kernelName:"GreaterEqual",inputsToSave:["a","b"],gradFunc:(t,e)=>{const[n,s]=e;return{a:()=>le(n),b:()=>le(s)}}},Ws={kernelName:"Identity",gradFunc:t=>({x:()=>St(t,"float32")})},Us={kernelName:"IsFinite",gradFunc:t=>({x:()=>le(t)})},Ks={kernelName:"IsInf",gradFunc:t=>({x:()=>le(t)})},Vs={kernelName:"IsNan",gradFunc:t=>({x:()=>le(t)})},js={kernelName:"LeakyRelu",inputsToSave:["x"],gradFunc:(t,e,n)=>{const[s]=e,{alpha:i}=n,r=we(s,0);return{x:()=>oe(r,t,_t(t,i))}}},qs={kernelName:"Log1p",inputsToSave:["x"],gradFunc:(t,e)=>{const[n]=e;return{x:()=>se(t,dt(n,1))}}},Gs={kernelName:"Log",inputsToSave:["x"],gradFunc:(t,e)=>{const[n]=e;return{x:()=>se(t,St(n,"float32"))}}},Hs={kernelName:"LogSoftmax",inputsToSave:[],outputsToSave:[!0],gradFunc:(t,e,n)=>{const[s]=e,{axis:i}=n;return{logits:()=>{const e=de(s);return Re(t,_t(Me(t,i,!0),e))}}}};const Js=ut({localResponseNormalizationBackprop_:function(t,e,n,s=5,i=1,r=1,a=.5){const o={x:t,y:e,dy:n},l={depthRadius:s,bias:i,alpha:r,beta:a};return st.runKernel("LRNGrad",o,l)}}),Zs={kernelName:"LRN",inputsToSave:["x"],outputsToSave:[!0],gradFunc:(t,e,n)=>{const[s,i]=e,{depthRadius:r,bias:a,alpha:o,beta:l}=n;return{x:()=>Js(s,i,t,r,a,o,l)}}};function Xs(t,e,n,s){return e.rank<n.rank&&(e=Tt(e,Be(e.shape,s))),t.rank<n.rank&&(t=Tt(t,Be(t.shape,s))),{x:()=>_t(t,St(ae(n,e),t.dtype))}}const Ys={kernelName:"Max",inputsToSave:["x"],outputsToSave:[!0],gradFunc:(t,e,n)=>{const s=n,{reductionIndices:i}=s,r=e[0],a=Xs(t,e[1],r,h(i,r.shape));return{x:()=>a.x()}}},Qs={kernelName:"Maximum",inputsToSave:["a","b"],gradFunc:(t,e)=>{const[n,s]=e;return{a:()=>_t(t,St(ke(n,s),"float32")),b:()=>_t(t,St(ze(n,s),"float32"))}}};const ti=ut({maxPool3dGrad_:function(t,e,n,i,r,a,o){const u=ot(t,"dy","maxPool3dGrad"),h=ot(e,"input","maxPool3dGrad"),c=ot(n,"output","maxPool3dGrad");let p=u,d=h,f=c,g=!1;4===h.rank&&(g=!0,p=Tt(u,[1,u.shape[0],u.shape[1],u.shape[2],u.shape[3]]),d=Tt(h,[1,h.shape[0],h.shape[1],h.shape[2],h.shape[3]]),f=Tt(c,[1,c.shape[0],c.shape[1],c.shape[2],c.shape[3]])),s(5===p.rank,()=>"Error in maxPool3dGrad: dy must be rank 5 but got rank "+p.rank+"."),s(5===d.rank,()=>"Error in maxPool3dGrad: input must be rank 5 but got rank "+d.rank+"."),s(5===f.rank,()=>"Error in maxPool3dGrad: output must be rank 5 but got rank "+f.rank+"."),null!=o&&s(l(a),()=>`Error in maxPool3dGrad: pad must be an integer when using, dimRoundingMode ${o} but got pad ${a}.`);const m={dy:p,input:d,output:f},y={filterSize:i,strides:r,pad:a,dimRoundingMode:o},b=st.runKernel("MaxPool3DGrad",m,y);return g?Tt(b,[b.shape[1],b.shape[2],b.shape[3],b.shape[4]]):b}}),ei={kernelName:"MaxPool3D",inputsToSave:["x"],outputsToSave:[!0],gradFunc:(t,e,n)=>{const[s,i]=e,{filterSize:r,strides:a,pad:o,dimRoundingMode:l}=n;return{x:()=>ti(t,s,i,r,a,o,l)}}};const ni=ut({maxPoolGrad_:function(t,e,n,i,r,a,o){const u=ot(t,"dy","maxPoolGrad"),h=ot(e,"input","maxPoolGrad"),c=ot(n,"output","maxPoolGrad");s(h.rank===u.rank,()=>`Rank of input (${h.rank}) does not match rank of dy (${u.rank})`),s(4===u.rank,()=>"Error in maxPoolGrad: dy must be rank 4 but got rank "+u.rank+"."),s(4===h.rank,()=>"Error in maxPoolGrad: input must be rank 4 but got rank "+h.rank+"."),null!=o&&s(l(a),()=>`Error in maxPoolGrad: pad must be an integer when using, dimRoundingMode ${o} but got pad ${a}.`);const p={dy:u,input:h,output:c},d={filterSize:i,strides:r,pad:a,dimRoundingMode:o};return st.runKernel("MaxPoolGrad",p,d)}}),si={kernelName:"PadV2",inputsToSave:["x"],gradFunc:(t,e,n)=>{const s=e[0],{paddings:i}=n,r=i.map(t=>t[0]);return{x:()=>Mt(t,r,s.shape)}}};const ii={kernelName:"SpaceToBatchND",gradFunc:(t,e,n)=>{const{blockShape:s,paddings:i}=n;return{x:()=>Bt(t,s,i)}}},ri={kernelName:"SplitV",gradFunc:(t,e,n)=>{const{axis:s}=n;return{x:()=>Lt(t,s)}}};const ai=[Zn,Xn,Yn,Qn,ts,es,ns,ss,is,rs,as,os,us,cs,ps,ds,fs,gs,ms,ys,bs,ws,xs,ks,Ss,Is,Ns,zs,As,Cs,{kernelName:"RealDiv",inputsToSave:["a","b"],gradFunc:(t,e)=>{const[n,s]=e,i=re(n.shape,s.shape);return{a:()=>{const e=se(t,St(s,"float32")),r=ie(n.shape,i);return r.length>0?Tt(Me(e,r),n.shape):e},b:()=>{let e=_t(t,St(n,"float32"));const r=ie(s.shape,i);r.length>0&&(e=Tt(Me(e,r),s.shape));const a=Qe(s);return Fe(se(e,St(a,"float32")))}}}},Ds,Ts,Es,Fs,Ls,_s,$s,Rs,Ms,Ps,Ws,Us,Ks,Vs,js,qs,Gs,Hs,Zs,Ys,Ys,Qs,ei,{kernelName:"MaxPool",inputsToSave:["x"],outputsToSave:[!0],gradFunc:(t,e,n)=>{const[s,i]=e,{filterSize:r,strides:a,pad:o}=n;return{x:()=>ni(t,s,i,r,a,o)}}},{kernelName:"Mean",inputsToSave:["x"],gradFunc:(t,e,n)=>{const[s]=e,{axis:i}=n,r=h(i,s.shape),o=a(function(t,e){const n=[],s=t.length;for(let i=0;i<s;i++)-1===e.indexOf(i)&&n.push(t[i]);return[n,e.map(e=>t[e])]}(s.shape,r)[1]);return{x:()=>{const e=s.shape.slice();r.forEach(t=>{e[t]=1});const n=Tt(t,e);return se(_t(n,sn(s.shape,"float32")),o)}}}},{kernelName:"Min",inputsToSave:["x"],outputsToSave:[!0],gradFunc:(t,e,n)=>{const s=n,{axis:i}=s,[r,a]=e,o=Xs(t,a,r,h(i,r.shape));return{x:()=>o.x()}}},{kernelName:"Minimum",inputsToSave:["a","b"],gradFunc:(t,e)=>{const[n,s]=e;return{a:()=>_t(t,St(Ae(n,s),"float32")),b:()=>_t(t,St(we(n,s),"float32"))}}},{kernelName:"MirrorPad",inputsToSave:["x"],gradFunc:(t,e,n)=>{const s=e[0],{paddings:i}=n,r=i.map(t=>t[0]);return{x:()=>Mt(t,r,s.shape)}}},{kernelName:"Mod",inputsToSave:["a","b"],gradFunc:(t,e)=>{const[n,s]=e,i=re(n.shape,s.shape);return{a:()=>{const e=ie(n.shape,i);return e.length>0?Tt(Me(t,e),n.shape):t},b:()=>{const e=_t(t,Fe(ye(se(n,s)))),r=ie(s.shape,i);return r.length>0?Tt(Me(e,r),s.shape):e}}}},{kernelName:"Multiply",inputsToSave:["a","b"],gradFunc:(t,e)=>{const[n,s]=e,i=re(n.shape,s.shape);return{a:()=>{const e=_t(t,St(s,"float32")),r=ie(n.shape,i);return r.length>0?Tt(Me(e,r),n.shape):e},b:()=>{const e=_t(t,St(n,"float32")),r=ie(s.shape,i);return r.length>0?Tt(Me(e,r),s.shape):e}}}},{kernelName:"Neg",gradFunc:t=>({x:()=>Fe(t)})},{kernelName:"OneHot",inputsToSave:["indices"],gradFunc:(t,e)=>{const n=e[0];return{indices:()=>nn(n.shape,"float32")}}},{kernelName:"OnesLike",gradFunc:t=>({x:()=>le(t)})},{kernelName:"Pack",saveAllInputs:!0,gradFunc:(t,e,n)=>{const{axis:s}=n;return Un(t,s).map(t=>()=>t)}},si,si,{kernelName:"Pow",inputsToSave:["a","b"],outputsToSave:[!0],gradFunc:(t,e)=>{const[n,s,i]=e,r=n,a=s,o=re(r.shape,a.shape);return{a:()=>{const e=St(a,"float32");let n=_t(t,_t(e,un(r,Re(e,kn(1)))));const s=ie(r.shape,o);return s.length>0&&(n=Me(n,s)),Tt(n,r.shape)},b:()=>{const e=we(r,0),n=oe(e,De(r),le(r));let s=_t(t,_t(i,n));const l=ie(a.shape,o);return l.length>0&&(s=Me(s,l)),Tt(s,a.shape)}}}},{kernelName:"Prelu",inputsToSave:["x","alpha"],gradFunc:(t,e)=>{const[n,s]=e,i=we(n,0);return{x:()=>oe(i,t,_t(t,s)),alpha:()=>{let e=oe(i,le(t),_t(t,n));const r=ie(s.shape,t.shape);return r.length>0&&(e=Me(e,r)),Tt(e,s.shape)}}}},{kernelName:"Reciprocal",inputsToSave:["x"],gradFunc:(t,e)=>{const[n]=e;return{x:()=>se(t,Fe(Qe(n)))}}},{kernelName:"Relu6",inputsToSave:["x"],gradFunc:(t,e)=>{const[n]=e,s=_t(Ae(n,6),Rn(n));return{x:()=>_t(t,St(s,"float32"))}}},{kernelName:"Relu",inputsToSave:["x"],gradFunc:(t,e)=>{const[n]=e;return{x:()=>_t(t,St(Rn(n),"float32"))}}},{kernelName:"Reshape",inputsToSave:["x"],gradFunc:(t,e)=>{const[n]=e;return{x:()=>Tt(t,n.shape)}}},{kernelName:"ResizeBilinear",inputsToSave:["images"],gradFunc:(t,e,n)=>{const[s]=e,i={dy:t,images:s};return{images:()=>st.runKernel("ResizeBilinearGrad",i,n)}}},{kernelName:"ResizeNearestNeighbor",inputsToSave:["images"],gradFunc:(t,e,n)=>{const[s]=e,i={dy:t,images:s};return{images:()=>st.runKernel("ResizeNearestNeighborGrad",i,n)}}},{kernelName:"Reverse",gradFunc:(t,e,n)=>{const{dims:s}=n,i=h(s,t.shape);return{x:()=>mn(t,i)}}},{kernelName:"Round",gradFunc:t=>({x:()=>le(t)})},{kernelName:"Rsqrt",inputsToSave:["x"],gradFunc:(t,e)=>{const[n]=e;return{x:()=>Fe(se(t,_t(un(n,1.5),2)))}}},{kernelName:"Select",inputsToSave:["condition"],gradFunc:(t,e)=>{const[n]=e;return{condition:()=>St(le(n),"float32"),t:()=>_t(t,St(n,t.dtype)),e:()=>_t(t,St(Ke(n),t.dtype))}}},{kernelName:"Selu",inputsToSave:["x"],gradFunc:(t,e)=>{const[n]=e;return{x:()=>{const e=we(n,kn(0)),s=kn(1.7580993408473768),i=kn(1.0507009873554805),r=_t(t,i),a=_t(_t(t,s),de(St(n,"float32")));return oe(e,r,a)}}}},{kernelName:"Sigmoid",outputsToSave:[!0],gradFunc:(t,e)=>{const[n]=e;return{x:()=>_t(t,_t(n,Re(kn(1),n)))}}},{kernelName:"Sign",gradFunc:t=>({x:()=>le(t)})},{kernelName:"Sin",inputsToSave:["x"],gradFunc:(t,e)=>{const[n]=e;return{x:()=>_t(Zt(St(n,"float32")),t)}}},{kernelName:"Sinh",inputsToSave:["x"],gradFunc:(t,e)=>{const[n]=e;return{x:()=>_t(Xt(St(n,"float32")),t)}}},{kernelName:"Slice",inputsToSave:["x"],gradFunc:(t,e,n)=>{const[i]=e,{begin:r,size:a}=n,o=i.shape,[l,u]=function(t,e,n){let i;const r=t.shape.length;let a;return i="number"==typeof e?[e,...new Array(r-1).fill(0)]:e.length<r?e.concat(new Array(r-e.length).fill(0)):e.slice(),i.forEach(t=>{s(-1!==t,()=>"slice() does not support negative begin indexing.")}),a=null==n?new Array(r).fill(-1):"number"==typeof n?[n,...new Array(r-1).fill(-1)]:n.length<r?n.concat(new Array(r-n.length).fill(-1)):n,a=a.map((e,n)=>e>=0?e:(s(-1===e,()=>`Negative size values should be exactly -1 but got ${e} for the slice() size at index ${n}.`),t.shape[n]-i[n])),[i,a]}(i,r,a),h=[];for(let e=0;e<t.rank;e++)h.push([l[e],o[e]-l[e]-u[e]]);return{x:()=>an(t,h)}}},{kernelName:"Softmax",outputsToSave:[!0],gradFunc:(t,e,n)=>{const[s]=e,{dim:i}=n,r=_t(t,s);return{logits:()=>Re(r,_t(Me(r,[i],!0),s))}}},{kernelName:"Softplus",inputsToSave:["x"],gradFunc:(t,e)=>{const[n]=e;return{x:()=>_t(t,Rt(n))}}},ii,ii,ri,ri,{kernelName:"Sqrt",inputsToSave:["x"],gradFunc:(t,e)=>{const[n]=e;return{x:()=>se(t,_t(Fn(St(n,"float32")),2))}}},{kernelName:"SquaredDifference",inputsToSave:["a","b"],gradFunc:(t,e)=>{const[n,s]=e,i=kn(2);return{a:()=>_t(t,_t(i,Re(n,s))),b:()=>_t(t,_t(i,Re(s,n)))}}},{kernelName:"Square",inputsToSave:["x"],gradFunc:(t,e)=>{const[n]=e;return{x:()=>_t(t,_t(St(n,"float32"),2))}}},{kernelName:"Step",gradFunc:t=>({x:()=>le(t)})},{kernelName:"Sub",inputsToSave:["a","b"],gradFunc:(t,e)=>{const[n,s]=e,i=re(n.shape,s.shape);return{a:()=>{let e=t;const s=ie(n.shape,i);return s.length>0&&(e=Me(e,s)),Tt(e,n.shape)},b:()=>{let e=t;const n=ie(s.shape,i);return n.length>0&&(e=Me(e,n)),Tt(Fe(e),s.shape)}}}},{kernelName:"Sum",inputsToSave:["x"],gradFunc:(t,e,n)=>{const[s]=e,i=s.shape.slice(),{axis:r}=n;h(r,s.shape).forEach(t=>{i[t]=1});const a=Tt(t,i),o=_t(a,sn(s.shape,"float32"));return{x:()=>o}}},{kernelName:"Tan",inputsToSave:["x"],gradFunc:(t,e)=>{const[n]=e;return{x:()=>se(t,Qe(Zt(n)))}}},{kernelName:"Tanh",outputsToSave:[!0],gradFunc:(t,e)=>{const[n]=e;return{x:()=>_t(Re(kn(1),Qe(n)),t)}}},{kernelName:"Tile",inputsToSave:["x"],gradFunc:(t,e,n)=>{const[s]=e,{reps:i}=n;return{x:()=>{let e=le(s);if(1===s.rank)for(let n=0;n<i[0];++n)e=dt(e,Mt(t,[n*s.shape[0]],[s.shape[0]]));else if(2===s.rank)for(let n=0;n<i[0];++n)for(let r=0;r<i[1];++r)e=dt(e,Mt(t,[n*s.shape[0],r*s.shape[1]],[s.shape[0],s.shape[1]]));else if(3===s.rank)for(let n=0;n<i[0];++n)for(let r=0;r<i[1];++r)for(let a=0;a<i[2];++a)e=dt(e,Mt(t,[n*s.shape[0],r*s.shape[1],a*s.shape[2]],[s.shape[0],s.shape[1],s.shape[2]]));else{if(4!==s.rank)throw new Error("Gradient for tile operation is not implemented for rank-"+s.rank+" tensors yet.");for(let n=0;n<i[0];++n)for(let r=0;r<i[1];++r)for(let a=0;a<i[2];++a)for(let o=0;o<i[3];++o)e=dt(e,Mt(t,[n*s.shape[0],r*s.shape[1],a*s.shape[2],o*s.shape[3]],[s.shape[0],s.shape[1],s.shape[2],s.shape[3]]))}return e}}}},{kernelName:"Transpose",gradFunc:(t,e,n)=>{const s=n,{perm:i}=s,r=Pe(i);return{x:()=>Kn(t,r)}}},{kernelName:"Unpack",gradFunc:(t,e,n)=>{const s=n,{axis:i}=s;return{value:()=>_n(t,i)}}},{kernelName:"UnsortedSegmentSum",inputsToSave:["segmentIds"],gradFunc:(t,e)=>{const[n]=e;return{x:()=>function(t,e){const n=Ge(e,le(e)),s=be(t,n);let i=ke(e,kn(0,"int32"));const r=s.rank-i.rank;for(let t=0;t<r;++t)i=fe(i,t+1);i=Ue(i,sn(s.shape,"bool"));const a=le(s);return oe(i,s,a)}(t,n)}}},{kernelName:"ZerosLike",gradFunc:t=>({x:()=>le(t)})}];for(const t of ai)E(t);let oi;function li(){return null==oi&&(oi=e.backend().epsilon()),oi}class ui extends Error{constructor(t){super(t),Object.setPrototypeOf(this,ui.prototype)}}class hi extends Error{constructor(t){super(t),Object.setPrototypeOf(this,hi.prototype)}}class ci extends Error{constructor(t){super(t),Object.setPrototypeOf(this,ci.prototype)}}class pi extends Error{constructor(t){super(t),Object.setPrototypeOf(this,pi.prototype)}}class di extends Error{constructor(t){super(t),Object.setPrototypeOf(this,di.prototype)}}function fi(t,e){if(Array.isArray(t)){let n=[];for(let s=0;s<e;s++)n=n.concat(t);return n}{const n=new Array(e);return n.fill(t),n}}function gi(t,e){if(!t)throw new di(e)}function mi(t,e){let n=0;for(const s of t)s===e&&n++;return n}function yi(t){return 1===t.length?t[0]:t}function bi(t){return Array.isArray(t)?t:[t]}function wi(t){const e=t.replace(/(.)([A-Z][a-z0-9]+)/g,"$1_$2").replace(/([a-z])([A-Z])/g,"$1_$2").toLowerCase();return"_"!==e[0]?e:"private"+e}function ki(t){return t.length<=1||-1===t.indexOf("_")?t:t.replace(/[_]+(\w|$)/g,(t,e)=>e.toUpperCase())}let xi={};function vi(t){if(null==t)return null;const e={};return e.className=t.getClassName(),e.config=t.getConfig(),e}function Si(t,e={},n={},s="object",i=!1){if("string"==typeof t){const i=t;let r;if(i in n)r=n[i];else if(i in xi)r=xi[i];else if(r=e[i],null==r)throw new ci(`Unknown ${s}: ${t}. This may be due to one of the following reasons:\n1. The ${s} is defined in Python, in which case it needs to be ported to TensorFlow.js or your JavaScript code.\n2. The custom ${s} is defined in JavaScript, but is not registered properly with tf.serialization.registerClass().`);return r}{const r=t;if(null==r.className||null==r.config)throw new ci(s+": Improper config format: "+JSON.stringify(r)+".\n'className' and 'config' must set.");const a=r.className;let o,l;if(a in n?[o,l]=n[a]:a in xi?[o,l]=xi.className:a in e&&([o,l]=e[a]),null==o)throw new ci(`Unknown ${s}: ${a}. This may be due to one of the following reasons:\n1. The ${s} is defined in Python, in which case it needs to be ported to TensorFlow.js or your JavaScript code.\n2. The custom ${s} is defined in JavaScript, but is not registered properly with tf.serialization.registerClass().`);if(null!=l){const t={};for(const e of Object.keys(xi))t[e]=xi[e];for(const e of Object.keys(n))t[e]=n[e];r.config.customObjects=t;const e=Object.assign({},xi);for(const t of Object.keys(n))xi[t]=n[t];!function t(e){if(null!=e&&"object"==typeof e)if(Array.isArray(e))e.forEach(e=>t(e));else{const n=Object.keys(e);for(const s of n){const n=e[s];null!=n&&"object"==typeof n&&(Array.isArray(n)||"ndarray"!==n.type||"number"!=typeof n.value?t(n):e[s]=n.value)}}}(r.config);const s=l(o,r.config,n,i);return xi=Object.assign({},e),s}{const t=Object.assign({},xi);for(const t of Object.keys(n))xi[t]=n[t];const e=new o(r.config);return xi=Object.assign({},t),e}}}function Ii(t,e){return-1*function(t,e){return t<e?-1:t>e?1:0}(t,e)}function Ni(t){if(null==t)return t;const e=[];for(const n of t)-1===e.indexOf(n)&&e.push(n);return e}function zi(t){if(null==t)throw new ci("Invalid value in obj: "+JSON.stringify(t));for(const e in t)if(t.hasOwnProperty(e))return!1;return!0}function Ai(t,e,n){if(null!=n&&t.indexOf(n)<0)throw new ci(`${n} is not a valid ${e}.  Valid values are ${t} or null/undefined.`)}function Ci(t,e,n=0,s=1/0){return gi(n>=0),gi(s>=n),Array.isArray(t)&&t.length>=n&&t.length<=s&&t.every(t=>typeof t===e)}function Di(t,n){Array.isArray(t)?(e.util.assert(t.length>0,()=>n+" is unexpectedly an empty array."),t.forEach((t,e)=>Di(t,`element ${e+1} of ${n}`))):e.util.assert(Number.isInteger(t)&&t>0,()=>`Expected ${n} to be a positive integer, but got `+function t(e){return null===e?"null":Array.isArray(e)?"["+e.map(e=>t(e)).join(",")+"]":"string"==typeof e?`"${e}"`:""+e}(t)+".")}function Ti(t){return"relu"===t?"relu":"linear"===t?"linear":"elu"===t?"elu":null}function Ei(t,n){return e.tidy(()=>e.sqrt(e.sum(e.mul(t,t),n,!0)))}class Fi extends e.serialization.Serializable{getConfig(){return{}}}class Li extends Fi{constructor(t){super(),this.defaultMaxValue=2,this.defaultAxis=0,this.maxValue=null!=t.maxValue?t.maxValue:this.defaultMaxValue,this.axis=null!=t.axis?t.axis:this.defaultAxis}apply(t){return e.tidy(()=>{const n=Ei(t,this.axis),s=e.clipByValue(n,0,this.maxValue);return e.mul(t,e.div(s,e.add(li(),n)))})}getConfig(){return{maxValue:this.maxValue,axis:this.axis}}}Li.className="MaxNorm",e.serialization.registerClass(Li);class $i extends Fi{constructor(t){super(),this.defaultAxis=0,this.axis=null!=t.axis?t.axis:this.defaultAxis}apply(t){return e.tidy(()=>e.div(t,e.add(li(),Ei(t,this.axis))))}getConfig(){return{axis:this.axis}}}$i.className="UnitNorm",e.serialization.registerClass($i);class _i extends Fi{apply(t){return e.relu(t)}}_i.className="NonNeg",e.serialization.registerClass(_i);class Ri extends Fi{constructor(t){super(),this.defaultMinValue=0,this.defaultMaxValue=1,this.defaultRate=1,this.defaultAxis=0,this.minValue=null!=t.minValue?t.minValue:this.defaultMinValue,this.maxValue=null!=t.maxValue?t.maxValue:this.defaultMaxValue,this.rate=null!=t.rate?t.rate:this.defaultRate,this.axis=null!=t.axis?t.axis:this.defaultAxis}apply(t){return e.tidy(()=>{const n=Ei(t,this.axis),s=e.add(e.mul(this.rate,e.clipByValue(n,this.minValue,this.maxValue)),e.mul(1-this.rate,n));return e.mul(t,e.div(s,e.add(li(),n)))})}getConfig(){return{minValue:this.minValue,maxValue:this.maxValue,rate:this.rate,axis:this.axis}}}Ri.className="MinMaxNorm",e.serialization.registerClass(Ri);const Mi={maxNorm:"MaxNorm",minMaxNorm:"MinMaxNorm",nonNeg:"NonNeg",unitNorm:"UnitNorm"};function Oi(t){return vi(t)}function Bi(t,n={}){return Si(t,e.serialization.SerializationMap.getMap().classNameMap,n,"constraint")}function Pi(t){if(null==t)return null;if("string"==typeof t){return Bi({className:t in Mi?Mi[t]:t,config:{}})}return t instanceof Fi?t:Bi(t)}var Wi=Object.freeze({__proto__:null,maxNorm:function(t){return new Li(t)},unitNorm:function(t){return new $i(t)},nonNeg:function(){return new _i},minMaxNorm:function(t){return new Ri(t)}});const Ui=["channelsFirst","channelsLast"],Ki=["nearest","bilinear"],Vi=["valid","same","causal"],ji=["max","avg"],qi=["sum","mul","concat","ave"],Gi=new Map;function Hi(t){Ai(Ui,"DataFormat",t)}function Ji(t){Ai(Vi,"PaddingMode",t)}function Zi(t){Ai(ji,"PoolMode",t)}const Xi=[];function Yi(t,e){Xi.push(t);try{const t=e();return Xi.pop(),t}catch(t){throw Xi.pop(),t}}function Qi(t){if(!nr(t))throw new Error("Not a valid tensor name: '"+t+"'");return(0===Xi.length?"":Xi.join("/")+"/")+t}function tr(t){if(!nr(t))throw new Error("Not a valid tensor name: '"+t+"'");Gi.has(t)||Gi.set(t,0);const e=Gi.get(t);if(Gi.set(t,Gi.get(t)+1),e>0){const n=`${t}_${e}`;return Gi.set(n,1),n}return t}const er=new RegExp(/^[A-Za-z0-9][-A-Za-z0-9\._\/]*$/);function nr(t){return!!t.match(er)}function sr(t,e,n){null==e&&(e=0),null==n&&(n=t.length);let s=1;for(let i=e;i<n;++i)s*=t[i];return s}function ir(t){return t=Array.isArray(t)?new Float32Array(t):t,e.tensor1d(t)}function rr(t){return e.min(ir(t)).dataSync()[0]}function ar(t){return e.max(ir(t)).dataSync()[0]}function or(t,e){if(e<t)throw new ci(`end (${e}) < begin (${t}) is forbidden.`);const n=[];for(let s=t;s<e;++s)n.push(s);return n}function lr(t,e){return t.asType(e)}function ur(t,e=-1){const n=t.shape.slice();return e<0&&(e=n.length+e+1),n.splice(e,0,1),t.reshape(n)}function hr(t,n,s){return e.tidy(()=>{switch(t.rank){case 1:return e.slice1d(t,n,s);case 2:return e.slice2d(t,[n,0],[s,t.shape[1]]);case 3:return e.slice3d(t,[n,0,0],[s,t.shape[1],t.shape[2]]);case 4:return e.slice4d(t,[n,0,0,0],[s,t.shape[1],t.shape[2],t.shape[3]]);case 5:return e.slice(t,[n,0,0,0,0],[s,t.shape[1],t.shape[2],t.shape[3],t.shape[4]]);case 6:return e.slice(t,[n,0,0,0,0,0],[s,t.shape[1],t.shape[2],t.shape[3],t.shape[4],t.shape[5]]);default:throw new ci("sliceAlongFirstAxis() received an unsupported tensor rank: "+t.rank)}})}function cr(t,n,s){return e.tidy(()=>{switch(t.rank){case 1:return e.slice1d(t,n,s);case 2:return e.slice2d(t,[0,n],[t.shape[0],s]);case 3:return e.slice3d(t,[0,0,n],[t.shape[0],t.shape[1],s]);case 4:return e.slice4d(t,[0,0,0,n],[t.shape[0],t.shape[1],t.shape[2],s]);default:throw new ci("sliceAlongLastAxis() received an unsupported tensor rank: "+t.rank)}})}function pr(t,n,s,i){return e.tidy(()=>{switch(t.rank){case 1:return e.slice1d(t,n,s);case 2:switch(i){case 1:return hr(t,n,s);case 2:return cr(t,n,s);default:throw new ci("The axis is not within the rank of the tensor "+i)}case 3:switch(i){case 1:return hr(t,n,s);case 2:return e.slice3d(t,[0,n,0],[t.shape[0],s,t.shape[2]]);case 3:return cr(t,n,s);default:throw new ci("The axis is not within the rank of the tensor "+i)}case 4:switch(i){case 1:return hr(t,n,s);case 2:return e.slice4d(t,[0,n,0,0],[t.shape[0],s,t.shape[2],t.shape[3]]);case 3:return e.slice4d(t,[0,0,n,0],[t.shape[0],t.shape[1],s,t.shape[3]]);case 4:return cr(t,n,s);default:throw new ci("The axis is not within the rank of the tensor "+i)}default:throw new ci("sliceAlongLastAxis() received an unsupported tensor rank: "+t.rank)}})}function dr(t,n=-1){let s;return n<0&&(s=t[0].rank,n=0!==s?s:0),n===t[0].rank&&(n=-1),e.concat(t,n)}function fr(t,n){switch(t.rank){case 1:return e.concat1d([t,n]);case 2:return e.concat2d([t,n],0);case 3:return e.concat3d([t,n],0);case 4:return e.concat4d([t,n],0);default:throw new ci("concatAlongFirstAxis() received an unsupported tensor rank: "+t.rank)}}function gr(t,n){if(Array.isArray(n)||(n=[n]),t.rank!==n.length)throw new ci(`The length of input n (${n.length}) does not match the number of dimensions in input x (${t.rank})`);return e.tile(t,n)}function mr(t,n=0,s=1,i,r){return e.randomNormal(t,n,s,i,r)}function yr(t,n,s,i){if(t.rank<2||n.rank<2)throw new pi(`dot requires both inputs to be rank >= 2 but got x shape = ${t.shape} and y shape = ${n.shape}`);if(n.rank>=3){if(t.shape.slice(-1)[0]!==n.shape.slice(-2)[0])throw new pi(`If rank y >= 3, then the second last dim of y must equal the last dim of x but got x shape = ${t.shape} and  y shape = `+n.shape)}if(2===t.rank&&2===n.rank){const r=!1,a=!1;return e.fused.matMul({a:t,b:n,transposeA:r,transposeB:a,bias:i?kr(t.rank,i,"channelsLast"):null,activation:s})}{const r=t.shape.slice(),a=r.pop();t=t.reshape([-1,a]);const o=n.shape.slice(),l=o.pop(),u=o.pop(),h=[...o,l],c=Array.from({length:n.rank},(t,e)=>0===e?n.rank-2:e<=n.rank-2?e-1:e);n=n.transpose(c).reshape([u,-1]);const p=[...r,...h],d=!1,f=!1;return e.fused.matMul({a:t,b:n,transposeA:d,transposeB:f,bias:i?kr(t.rank,i,"channelsLast"):null,activation:s}).reshape(p)}}function br(t,n,s){return e.tidy(()=>(n=Array.isArray(n)?e.tensor1d(n,"int32"):n.toInt(),e.gather(t,n,s)))}function wr(t){return e.mul(t,t)}function kr(t,e,n){const s=e.shape;if(1!==e.rank&&e.rank!==t)throw new ci("Unexpected bias dimensions: "+e.rank+"; expected it to be 1 or "+t);if(5===t){if("channelsFirst"===n)return 1===s.length?e.reshape([1,s[0],1,1,1]):e.reshape([1,s[3],s[0],s[1],s[2]]);if("channelsLast"===n)return 1===s.length?e.reshape([1,1,1,1,s[0]]):e.reshape([1].concat(s))}else if(4===t){if("channelsFirst"===n)return 1===s.length?e.reshape([1,s[0],1,1]):e.reshape([1,s[2],s[0],s[1]]);if("channelsLast"===n)return 1===s.length?e.reshape([1,1,1,s[0]]):e.reshape([1].concat(s))}else if(3===t){if("channelsFirst"===n)return 1===s.length?e.reshape([1,s[0],1]):e.reshape([1,s[1],s[0]]);if("channelsLast"===n)return 1===s.length?e.reshape([1,1,s[0]]):e.reshape([1].concat(s))}else if(t<3)return e;throw new ci("Unsupported input rank by biasAdd: "+e.rank)}function xr(t,n,s){return e.tidy(()=>(null==s&&(s="channelsLast"),Hi(s),t.add(kr(t.rank,n,s))))}function vr(t,n,s,i){return e.tidy(()=>e.dropout(t,n,s,i))}function Sr(t,e,n=!1){return n?t():e()}const Ir=["fanIn","fanOut","fanAvg"],Nr=["normal","uniform","truncatedNormal"];class zr extends e.serialization.Serializable{fromConfigUsesCustomObjects(){return!1}getConfig(){return{}}}class Ar extends zr{apply(t,n){return e.zeros(t,n)}}Ar.className="Zeros",e.serialization.registerClass(Ar);class Cr extends zr{apply(t,n){return e.ones(t,n)}}Cr.className="Ones",e.serialization.registerClass(Cr);class Dr extends zr{constructor(t){if(super(),"object"!=typeof t)throw new ci("Expected argument of type ConstantConfig but got "+t);if(void 0===t.value)throw new ci("config must have value set but got "+t);this.value=t.value}apply(t,n){return e.tidy(()=>e.mul(e.scalar(this.value),e.ones(t,n)))}getConfig(){return{value:this.value}}}Dr.className="Constant",e.serialization.registerClass(Dr);class Tr extends zr{constructor(t){super(),this.DEFAULT_MINVAL=-.05,this.DEFAULT_MAXVAL=.05,this.minval=t.minval||this.DEFAULT_MINVAL,this.maxval=t.maxval||this.DEFAULT_MAXVAL,this.seed=t.seed}apply(t,n){return e.randomUniform(t,this.minval,this.maxval,n)}getConfig(){return{minval:this.minval,maxval:this.maxval,seed:this.seed}}}Tr.className="RandomUniform",e.serialization.registerClass(Tr);class Er extends zr{constructor(t){super(),this.DEFAULT_MEAN=0,this.DEFAULT_STDDEV=.05,this.mean=t.mean||this.DEFAULT_MEAN,this.stddev=t.stddev||this.DEFAULT_STDDEV,this.seed=t.seed}apply(t,e){if("float32"!==(e=e||"float32")&&"int32"!==e)throw new pi(`randomNormal does not support dType ${e}.`);return mr(t,this.mean,this.stddev,e,this.seed)}getConfig(){return{mean:this.mean,stddev:this.stddev,seed:this.seed}}}Er.className="RandomNormal",e.serialization.registerClass(Er);class Fr extends zr{constructor(t){super(),this.DEFAULT_MEAN=0,this.DEFAULT_STDDEV=.05,this.mean=t.mean||this.DEFAULT_MEAN,this.stddev=t.stddev||this.DEFAULT_STDDEV,this.seed=t.seed}apply(t,n){if("float32"!==(n=n||"float32")&&"int32"!==n)throw new pi(`truncatedNormal does not support dType ${n}.`);return e.truncatedNormal(t,this.mean,this.stddev,n,this.seed)}getConfig(){return{mean:this.mean,stddev:this.stddev,seed:this.seed}}}Fr.className="TruncatedNormal",e.serialization.registerClass(Fr);class Lr extends zr{constructor(t){super(),this.gain=null!=t.gain?t.gain:1}apply(t,n){return e.tidy(()=>{if(2!==t.length||t[0]!==t[1])throw new ci("Identity matrix initializer can only be used for 2D square matrices.");return e.mul(this.gain,e.eye(t[0]))})}getConfig(){return{gain:this.gain}}}Lr.className="Identity",e.serialization.registerClass(Lr);class $r extends zr{constructor(t){if(super(),t.scale<0)throw new ci("scale must be a positive float. Got: "+t.scale);var e;this.scale=null==t.scale?1:t.scale,this.mode=null==t.mode?"fanIn":t.mode,e=this.mode,Ai(Ir,"FanMode",e),this.distribution=null==t.distribution?"normal":t.distribution,function(t){Ai(Nr,"Distribution",t)}(this.distribution),this.seed=t.seed}apply(t,n){const s=function(t,e="channelsLast"){let n,s;if(Hi(e),2===t.length)n=t[0],s=t[1];else if(-1!==[3,4,5].indexOf(t.length)){if("channelsFirst"===e){const e=sr(t,2);n=t[1]*e,s=t[0]*e}else if("channelsLast"===e){const e=sr(t,0,t.length-2);n=t[t.length-2]*e,s=t[t.length-1]*e}}else{const e=sr(t);n=Math.sqrt(e),s=Math.sqrt(e)}return[n,s]}(t),i=s[0],r=s[1];let a=this.scale;if("fanIn"===this.mode?a/=Math.max(1,i):"fanOut"===this.mode?a/=Math.max(1,r):a/=Math.max(1,(i+r)/2),"normal"===this.distribution){const s=Math.sqrt(a);if("float32"!==(n=n||"float32")&&"int32"!==n)throw new pi(`${this.getClassName()} does not support dType ${n}.`);return e.truncatedNormal(t,0,s,n,this.seed)}{const s=Math.sqrt(3*a);return e.randomUniform(t,-s,s,n)}}getConfig(){return{scale:this.scale,mode:this.mode,distribution:this.distribution,seed:this.seed}}}$r.className="VarianceScaling",e.serialization.registerClass($r);class _r extends $r{constructor(t){super({scale:1,mode:"fanAvg",distribution:"uniform",seed:null==t?null:t.seed})}getClassName(){return $r.className}}_r.className="GlorotUniform",e.serialization.registerClass(_r);class Rr extends $r{constructor(t){super({scale:1,mode:"fanAvg",distribution:"normal",seed:null==t?null:t.seed})}getClassName(){return $r.className}}Rr.className="GlorotNormal",e.serialization.registerClass(Rr);class Mr extends $r{constructor(t){super({scale:2,mode:"fanIn",distribution:"normal",seed:null==t?null:t.seed})}getClassName(){return $r.className}}Mr.className="HeNormal",e.serialization.registerClass(Mr);class Or extends $r{constructor(t){super({scale:2,mode:"fanIn",distribution:"uniform",seed:null==t?null:t.seed})}getClassName(){return $r.className}}Or.className="HeUniform",e.serialization.registerClass(Or);class Br extends $r{constructor(t){super({scale:1,mode:"fanIn",distribution:"normal",seed:null==t?null:t.seed})}getClassName(){return $r.className}}Br.className="LeCunNormal",e.serialization.registerClass(Br);class Pr extends $r{constructor(t){super({scale:1,mode:"fanIn",distribution:"uniform",seed:null==t?null:t.seed})}getClassName(){return $r.className}}Pr.className="LeCunNormal",e.serialization.registerClass(Pr);class Wr extends zr{constructor(t){if(super(),this.DEFAULT_GAIN=1,this.gain=null==t.gain?this.DEFAULT_GAIN:t.gain,this.seed=t.seed,null!=this.seed)throw new pi("Random seed is not implemented for Orthogonal Initializer yet.")}apply(t,n){return e.tidy(()=>{if(t.length<2)throw new pi("Shape must be at least 2D.");t[0]*t[1]>2e3&&console.warn(`Orthogonal initializer is being called on a matrix with more than 2000 (${t[0]*t[1]}) elements: Slowness may result.`);const n=mr(t[0]>t[1]?[t[1],t[0]]:t,0,1,"float32");let s=e.linalg.gramSchmidt(n);return t[0]>t[1]&&(s=s.transpose()),e.mul(this.gain,s)})}getConfig(){return{gain:this.gain,seed:this.seed}}}Wr.className="Orthogonal",e.serialization.registerClass(Wr);const Ur={constant:"Constant",glorotNormal:"GlorotNormal",glorotUniform:"GlorotUniform",heNormal:"HeNormal",heUniform:"HeUniform",identity:"Identity",leCunNormal:"LeCunNormal",leCunUniform:"LeCunUniform",ones:"Ones",orthogonal:"Orthogonal",randomNormal:"RandomNormal",randomUniform:"RandomUniform",truncatedNormal:"TruncatedNormal",varianceScaling:"VarianceScaling",zeros:"Zeros"};function Kr(t,n={}){return Si(t,e.serialization.SerializationMap.getMap().classNameMap,n,"initializer")}function Vr(t){return vi(t)}function jr(t){if("string"==typeof t){const e=t in Ur?Ur[t]:t;if("GlorotNormal"===e)return new Rr;if("GlorotUniform"===e)return new _r;if("HeNormal"===e)return new Mr;if("HeUniform"===e)return new Or;if("LeCunNormal"===e)return new Br;if("LeCunUniform"===e)return new Pr;{const t={};return t.className=e,t.config={},Kr(t)}}return t instanceof zr?t:Kr(t)}var qr=Object.freeze({__proto__:null,zeros:function(){return new Ar},ones:function(){return new Cr},constant:function(t){return new Dr(t)},randomUniform:function(t){return new Tr(t)},randomNormal:function(t){return new Er(t)},truncatedNormal:function(t){return new Fr(t)},identity:function(t){return new Lr(t)},varianceScaling:function(t){return new $r(t)},glorotUniform:function(t){return new _r(t)},glorotNormal:function(t){return new Rr(t)},heNormal:function(t){return new Mr(t)},heUniform:function(t){return new Or(t)},leCunNormal:function(t){return new Br(t)},leCunUniform:function(t){return new Pr(t)},orthogonal:function(t){return new Wr(t)}});let Gr=0;function Hr(){return Gr++}const Jr={};function Zr(t=""){return t in Jr||(Jr[t]=0),Jr[t]+=1,t+Jr[t].toString()}function Xr(t){return Array.isArray(t)&&Array.isArray(t[0])}function Yr(t){return 0===t.length?[]:Array.isArray(t[0])?t:[t]}function Qr(t){let e;if(Array.isArray(t)){if(1!==t.length)throw new ci("Expected Tensor length to be 1; got "+t.length);e=t[0]}else e=t;return e}function ta(t){if(Array.isArray(t)&&Array.isArray(t[0])){if(1===t.length)return(t=t)[0];throw new ci("Expected exactly 1 Shape; got "+t.length)}return t}function ea(t){let e=0;for(const n of t)0===n.shape.length?e+=1:e+=n.shape.reduce((t,e)=>t*e);return e}class na{constructor(t,n="float32",s="Variable",i=!0,r=null){this.dtype=null==n?"float32":n,this.shape=t.shape,this.id=Hr(),s=null==s?"Variable":s,this.originalName=Qi(s),this.name=tr(this.originalName),this.trainable_=i,this.constraint=r,this.val=e.variable(t,this.trainable_,this.name,this.dtype)}read(){return this.assertNotDisposed(),this.val}write(t){return this.assertNotDisposed(),function(t,e){if(t.shape.toString()!==e.shape.toString())throw new Error("Shape mismatch: "+JSON.stringify(t.shape)+" vs. "+JSON.stringify(e.shape))}(this.val,t),this.val.id!==t.id&&(this.val.assign(t),null!=this.constraint&&this.val.assign(this.constraint.apply(this.val))),this}dispose(){this.assertNotDisposed(),this.val.dispose()}assertNotDisposed(){if(this.val.isDisposed)throw new Error(`LayersVariable ${this.name} is already disposed.`)}get trainable(){return this.trainable_}set trainable(t){this.trainable_=t,this.val.trainable=t}}function sa(t){return t.map(t=>t.read())}function ia(t){t.forEach(t=>{t[0].write(t[1])})}class ra{constructor(t){this.dtype=t.dtype,this.shape=t.shape,null!=t.shape?this.ndim=t.shape.length:this.ndim=t.ndim,this.maxNDim=t.maxNDim,this.minNDim=t.minNDim,this.axes=t.axes||{}}}class aa{constructor(t,e,n,s,i,r,a){this.dtype=t,this.shape=e,this.sourceLayer=n,this.inputs=s,this.callArgs=i,this.outputTensorIndex=a,this.id=Hr(),null!=r&&(this.originalName=Qi(r),this.name=tr(this.originalName)),this.rank=e.length}}let oa=0;class la{constructor(t,e){this.callArgs=e,this.id=oa++,this.outboundLayer=t.outboundLayer,this.inboundLayers=t.inboundLayers,this.nodeIndices=t.nodeIndices,this.tensorIndices=t.tensorIndices,this.inputTensors=t.inputTensors,this.outputTensors=t.outputTensors,this.inputMasks=t.inputMasks,this.outputMasks=t.outputMasks,this.inputShapes=t.inputShapes,this.outputShapes=t.outputShapes;for(const e of t.inboundLayers)null!=e&&e.outboundNodes.push(this);t.outboundLayer.inboundNodes.push(this)}getConfig(){const t=[];for(const e of this.inboundLayers)null!=e?t.push(e.name):t.push(null);return{outboundLayer:this.outboundLayer?this.outboundLayer.name:null,inboundLayers:t,nodeIndices:this.nodeIndices,tensorIndices:this.tensorIndices}}}let ua=0;class ha extends e.serialization.Serializable{constructor(t={}){super(),this._callHook=null,this._addedWeightNames=[],this._stateful=!1,this.id=ua++,this.activityRegularizer=null,this.inputSpec=null,this.supportsMasking=!1,this._trainableWeights=[],this._nonTrainableWeights=[],this._losses=[],this._updates=[],this._built=!1,this.inboundNodes=[],this.outboundNodes=[];let e=t.name;if(!e){const t=this.getClassName();e=wi(t)+"_"+Zr(t)}if(this.name=e,this.trainable_=null==t.trainable||t.trainable,null!=t.inputShape||null!=t.batchInputShape){let e;if(null!=t.batchInputShape)e=t.batchInputShape;else if(null!=t.inputShape){let n=null;null!=t.batchSize&&(n=t.batchSize),e=[n].concat(t.inputShape)}this.batchInputShape=e;let n=t.dtype;null==n&&(n=t.inputDType),null==n&&(n="float32"),this.dtype=n}null!=t.weights?this.initialWeights=t.weights:this.initialWeights=null,this._refCount=null,this.fastWeightInitDuringBuild=!1}static nodeKey(t,e){return t.name+"_ib-"+e.toString()}getNodeAtIndex(t,e){if(0===this.inboundNodes.length)throw new hi(`The layer has never been called and thus has no defined ${e}.`);if(this.inboundNodes.length<=t)throw new ci(`Asked to get ${e} at node ${t}, but the layer has only ${this.inboundNodes.length} inbound nodes.`);return this.inboundNodes[t]}getInputAt(t){return yi(this.getNodeAtIndex(t,"input").inputTensors)}getOutputAt(t){return yi(this.getNodeAtIndex(t,"output").outputTensors)}get input(){if(this.inboundNodes.length>1)throw new ui("Layer "+this.name+' has multiple inbound nodes, hence the notion of "layer input" is ill-defined. Use `getInputAt(nodeIndex)` instead.');if(0===this.inboundNodes.length)throw new ui("Layer "+this.name+" is not connected, no input to return.");return yi(this.getNodeAtIndex(0,"input").inputTensors)}get output(){if(0===this.inboundNodes.length)throw new ui("Layer "+this.name+" has no inbound nodes.");if(this.inboundNodes.length>1)throw new ui("Layer "+this.name+' has multiple inbound nodes, hence the notion of "layer output" is ill-defined. Use `getOutputAt(nodeIndex)` instead.');return yi(this.getNodeAtIndex(0,"output").outputTensors)}get losses(){return this._losses}calculateLosses(){return this.losses.map(t=>t())}get updates(){return this._updates}get built(){return this._built}set built(t){this._built=t}get trainable(){return this.trainable_}set trainable(t){this._trainableWeights.forEach(e=>e.trainable=t),this.trainable_=t}get trainableWeights(){return this.trainable_?this._trainableWeights.filter(t=>t.trainable):[]}set trainableWeights(t){this._trainableWeights=t}get nonTrainableWeights(){return this.trainable?this._trainableWeights.filter(t=>!t.trainable).concat(this._nonTrainableWeights):this._trainableWeights.concat(this._nonTrainableWeights)}set nonTrainableWeights(t){this._nonTrainableWeights=t}get weights(){return this.trainableWeights.concat(this.nonTrainableWeights)}get stateful(){return this._stateful}resetStates(){if(!this.stateful)throw new Error("Cannot call the resetStates() method of a non-stateful Layer object.")}assertInputCompatibility(t){if(t=bi(t),null==this.inputSpec||0===this.inputSpec.length)return;const e=bi(this.inputSpec);if(t.length!==e.length)throw new ci(`Layer ${this.name} expects ${e.length} inputs, but it received ${t.length} input tensors. Input received: `+t);for(let n=0;n<t.length;n++){const s=t[n],i=e[n];if(null==i)continue;const r=s.rank;if(null!=i.ndim&&r!==i.ndim)throw new ci(`Input ${n} is incompatible with layer ${this.name}: expected ndim=${i.ndim}, found ndim=${r}`);if(null!=i.maxNDim&&r>i.maxNDim)throw new ci(`Input ${n} is incompatible with layer ${this.name}: expected max_ndim=${i.maxNDim}, found ndim=${r}`);if(null!=i.minNDim&&r<i.minNDim)throw new ci(`Input ${n} is incompatible with layer ${this.name}: expected min_ndim=${i.minNDim}, found ndim=${r}.`);if(null!=i.dtype&&s.dtype!==i.dtype)throw new ci(`Input ${n} is incompatible with layer ${this.name} : expected dtype=${i.dtype}, found dtype=${s.dtype}.`);if(i.axes){const t=s.shape;for(const e in i.axes){const s=Number(e),r=i.axes[e],a=s>=0?t[s]:t[t.length+s];if(null!=r&&-1===[r,null].indexOf(a))throw new ci(`Input ${n} is incompatible with layer ${this.name}: expected axis ${s} of input shape to have value ${r} but got shape ${t}.`)}}if(null!=i.shape)for(let t=0;t<i.shape.length;++t){const e=i.shape[t],r=s.shape[t];if(null!=e&&null!=r&&e!==r)throw new ci(`Input ${n} is incompatible with layer ${this.name}: expected shape=${i.shape}, found shape=${s.shape}.`)}}}call(t,e){return t}invokeCallHook(t,e){null!=this._callHook&&this._callHook(t,e)}setCallHook(t){this._callHook=t}clearCallHook(){this._callHook=null}apply(t,e){e=e||{},this.assertNotDisposed();const n=bi(t);let s=!0;for(const t of n)if(!(t instanceof aa)){s=!1;break}let i=!0;for(const t of n)if(t instanceof aa){i=!1;break}if(s===i)throw new ci("Arguments to apply() must be all SymbolicTensors or all Tensors");return Yi(this.name,()=>{if(!this.built){this.assertInputCompatibility(t);const e=[];for(const n of bi(t))e.push(n.shape);this.build(yi(e)),this.built=!0,this.initialWeights&&this.setWeights(this.initialWeights),null===this._refCount&&i&&(this._refCount=1)}if(this.assertInputCompatibility(t),i){let s=this.call(t,e);const i=bi(s),r=[];for(let t of i)-1!==n.indexOf(t)&&(t=t.clone()),r.push(t);if(s=yi(r),null!=this.activityRegularizer)throw new pi("Layer invocation in the presence of activity regularizer(s) is not supported yet.");return s}{const n=function(t){t=bi(t);const e=[];for(const n of t)e.push(n.shape);return yi(e)}(t),s=this.computeOutputShape(n);let i;const r="float32";if(this.warnOnIncompatibleInputShape(Array.isArray(t)?n[0]:n),i=null!=s&&s.length>0&&Array.isArray(s[0])?s.map((n,s)=>new aa(r,n,this,bi(t),e,this.name,s)):new aa(r,s,this,bi(t),e,this.name),this.addInboundNode(t,i,null,null,n,s,e),this._refCount++,null!=this.activityRegularizer)throw new pi("Layer invocation in the presence of activity regularizer(s) is not supported yet.");return i}})}warnOnIncompatibleInputShape(t){if(null!=this.batchInputShape)if(t.length!==this.batchInputShape.length)console.warn("The rank of the input tensor provided (shape: "+JSON.stringify(t)+") does not match that of the "+`batchInputShape (${JSON.stringify(this.batchInputShape)}) of the layer `+this.name);else{let e=!1;this.batchInputShape.forEach((n,s)=>{null!=n&&null!=t[s]&&t[s]!==n&&(e=!0)}),e&&console.warn(`The shape of the input tensor (${JSON.stringify(t)}) does not match the expectation of layer ${this.name}: `+JSON.stringify(this.batchInputShape))}}get outputShape(){if(null==this.inboundNodes||0===this.inboundNodes.length)throw new ui(`The layer ${this.name} has never been called and thus has no defined output shape.`);const t=[];for(const e of this.inboundNodes){const n=JSON.stringify(e.outputShapes);-1===t.indexOf(n)&&t.push(n)}if(1===t.length){const t=this.inboundNodes[0].outputShapes;return Array.isArray(t)&&Array.isArray(t[0])&&1===t.length?t[0]:t}throw new ui(`The layer ${this.name} has multiple inbound nodes with different output shapes. Hence the notion of "output shape" is ill-defined for the layer.`)}countParams(){if(!this.built)throw new hi(`You tried to call countParams() on ${this.name}, but the layer is not built yet. Build it first by calling build(batchInputShape).`);return ea(this.weights)}build(t){this.built=!0}getWeights(t=!1){return sa(t?this.trainableWeights:this.weights)}setWeights(t){e.tidy(()=>{const n=this.weights;if(n.length!==t.length)throw new ci(`You called setWeights(weights) on layer "${this.name}" with a weight list of length ${t.length}, but the layer was expecting ${n.length} weights. Provided weights: ${t}...`);if(0===n.length)return;const s=[],i=sa(n);for(let r=0;r<i.length;++r){const a=i[r],o=n[r],l=t[r];if(!e.util.arraysEqual(a.shape,l.shape))throw new ci(`Layer weight shape ${a.shape} not compatible with provided weight shape `+l.shape);s.push([o,l])}ia(s)})}addWeight(t,e,n,s,i,r,a){if(-1!==this._addedWeightNames.indexOf(t))throw new ci(`Duplicate weight name ${t} for layer ${this.name}`);this._addedWeightNames.push(t),null==n&&(n="float32"),this.fastWeightInitDuringBuild&&(s=jr("zeros"));const o=s.apply(e,n),l=new na(o,n,t,r,a);return o.dispose(),null!=i&&this.addLoss(()=>i.apply(l.read())),null==r&&(r=!0),r?this._trainableWeights.push(l):this._nonTrainableWeights.push(l),l}setFastWeightInitDuringBuild(t){this.fastWeightInitDuringBuild=t}addLoss(t){null==t||Array.isArray(t)&&0===t.length||(t=bi(t),void 0!==this._losses&&null!==this._losses&&this.losses.push(...t))}computeOutputShape(t){return t}computeMask(t,e){if(!this.supportsMasking){if(null!=e){if(!Array.isArray(e))throw new TypeError(`Layer ${this.name} does not support masking, but was passed an inputMask.`);e.forEach(t=>{if(null!=t)throw new TypeError(`Layer ${this.name} does not support masking, but was passed an inputMask.`)})}return null}return e}addInboundNode(t,e,n,s,i,r,a=null){const o=bi(t);e=bi(e),n=bi(n),s=bi(s),i=Yr(i),r=Yr(r);const l=[],u=[],h=[];for(const t of o)l.push(t.sourceLayer),u.push(t.nodeIndex),h.push(t.tensorIndex);new la({outboundLayer:this,inboundLayers:l,nodeIndices:u,tensorIndices:h,inputTensors:o,outputTensors:e,inputMasks:n,outputMasks:s,inputShapes:i,outputShapes:r},a);for(let t=0;t<e.length;t++)e[t].sourceLayer=this,e[t].nodeIndex=this.inboundNodes.length-1,e[t].tensorIndex=t}getConfig(){const t={name:this.name,trainable:this.trainable};return null!=this.batchInputShape&&(t.batchInputShape=this.batchInputShape),null!=this.dtype&&(t.dtype=this.dtype),t}disposeWeights(){return this.weights.forEach(t=>t.dispose()),this.weights.length}assertNotDisposed(){if(0===this._refCount)throw new Error(`Layer '${this.name}' is already disposed.`)}dispose(){if(!this.built)throw new Error(`Cannot dispose Layer ${this.name} because it has not been built yet.`);if(null===this._refCount)throw new Error(`Cannot dispose Layer ${this.name} because it has not been used yet.`);this.assertNotDisposed();let t=0;return 0==--this._refCount&&(t=this.disposeWeights()),{refCountAfterDispose:this._refCount,numDisposedVariables:t}}}class ca extends ha{constructor(t){if(super({dtype:t.dtype,name:null!=t.name?t.name:Zr("input").toString()}),null==t.batchSize&&(t.batchSize=null),null==t.sparse&&(t.sparse=!1),this.trainable=!1,this.built=!0,this.sparse=t.sparse,null!=t.inputShape&&null!=t.batchInputShape)throw new ci("Only provide the inputShape OR batchInputShape argument to inputLayer, not both at the same time.");let e=t.batchInputShape;if(null==e){if(null==t.inputShape)throw new ci("An InputLayer should be passed either a `batchInputShape` or an `inputShape`.");e=[t.batchSize].concat(t.inputShape)}else if(null!=t.batchSize)throw new ci("Cannot specify batchSize if batchInputShape is specified when creating an InputLayer.");const n=t.dtype||"float32";this.batchInputShape=e,this.dtype=n,this.inputSpec=[{shape:e}];const s=new aa(this.dtype,this.batchInputShape,this,[],{},this.name);s.nodeIndex=0,s.tensorIndex=0,new la({outboundLayer:this,inboundLayers:[],nodeIndices:[],tensorIndices:[],inputTensors:[s],outputTensors:[s],inputMasks:[null],outputMasks:[null],inputShapes:[e],outputShapes:[e]})}apply(t,e){throw new ci("Cannot pass any input to an InputLayer's apply() method. InputLayer name: "+this.name)}dispose(){return{refCountAfterDispose:this._refCount,numDisposedVariables:0}}getConfig(){return{batchInputShape:this.batchInputShape,dtype:this.dtype,sparse:this.sparse,name:this.name}}}function pa(t){if(null==t.batchShape&&null==t.shape)throw new Error("Please provide to Input either a `shape` or a `batchShape` argument. Note that `shape` does not include the batch dimension.");if(null!=t.batchShape&&null!=t.shape)throw new ci("Please provide either a `shape` or `batchShape` argument to Input, but not both.");let e=t.batchShape;null!=t.shape&&null==e&&(e=[null].concat(t.shape));let n=t.dtype;null==n&&(n="float32");return new ca({batchInputShape:e,name:t.name,dtype:n,sparse:t.sparse}).inboundNodes[0].outputTensors[0]}async function da(t){if(null==t)return;const n=[],s=[],i=[];for(const e in t){const r=t[e];if("number"!=typeof r){const t=r;n.push(t.data()),s.push(e),i.push(t)}}if(n.length>0){const r=await Promise.all(n);for(let e=0;e<r.length;++e)t[s[e]]=r[e][0];e.dispose(i)}}function fa(t){if(null!=t)for(const e in t){const n=t[e];"number"!=typeof n&&n.dispose()}}var ga;ca.className="InputLayer",e.serialization.registerClass(ca),function(t){t[t.SILENT=0]="SILENT",t[t.VERBOSE=1]="VERBOSE"}(ga||(ga={}));class ma{constructor(){this.validationData=null}setParams(t){this.params=t}async onEpochBegin(t,e){}async onEpochEnd(t,e){}async onBatchBegin(t,e){}async onBatchEnd(t,e){}async onTrainBegin(t){}async onTrainEnd(t){}setModel(t){}}class ya{constructor(t,e=10){null==t&&(t=[]),this.callbacks=t,this.queueLength=e}append(t){this.callbacks.push(t)}setParams(t){for(const e of this.callbacks)e.setParams(t)}setModel(t){for(const e of this.callbacks)e.setModel(t)}async onEpochBegin(t,e){null==e&&(e={});for(const n of this.callbacks)await n.onEpochBegin(t,e)}async onEpochEnd(t,e){null==e&&(e={});for(const n of this.callbacks)await n.onEpochEnd(t,e)}async onBatchBegin(t,e){null==e&&(e={});for(const n of this.callbacks)await n.onBatchBegin(t,e)}async onBatchEnd(t,e){null==e&&(e={});for(const n of this.callbacks)await n.onBatchEnd(t,e)}async onTrainBegin(t){null==t&&(t={});for(const e of this.callbacks)await e.onTrainBegin(t)}async onTrainEnd(t){null==t&&(t={});for(const e of this.callbacks)await e.onTrainEnd(t)}}class ba extends ma{constructor(){super()}async onEpochBegin(t){this.seen=0,this.totals={}}async onBatchEnd(t,n){null==n&&(n={});const s=null==n.size?0:n.size;this.seen+=s;for(const t in n){const i=n[t];if("number"==typeof i)this.totals.hasOwnProperty(t)||(this.totals[t]=0),this.totals[t]=this.totals[t]+i*s;else{let n;t in this.totals?n=this.totals[t]:this.totals[t]=0;const r=e.tidy(()=>e.add(this.totals[t],e.mul(i,s)));this.totals[t]=r,null!=n&&n.dispose()}}}async onEpochEnd(t,n){if(null!=n)for(const t of this.params.metrics)null!=this.totals[t]&&("number"==typeof this.totals[t]?n[t]=this.totals[t]/this.seen:e.tidy(()=>{const s=e.mul(e.div(1,this.seen),this.totals[t]);n[t]=s,this.totals[t].dispose(),e.keep(n[t])}))}}class wa extends ma{async onTrainBegin(t){this.epoch=[],this.history={}}async onEpochEnd(t,e){null==e&&(e={}),this.epoch.push(t);for(const t in e)null==this.history[t]&&(this.history[t]=[]),this.history[t].push(e[t])}async syncData(){const t=[],e=[],n=[];for(const s in this.history){const i=this.history[s];for(let r=0;r<i.length;++r)if("number"!=typeof i[r]){const a=i[r];t.push(a.data()),e.push(s),n.push(r)}}const s=await Promise.all(t);for(let t=0;t<s.length;++t){this.history[e[t]][n[t]].dispose(),this.history[e[t]][n[t]]=s[t][0]}}}class ka extends ma{constructor(t,n){if(super(),this.currentEpoch=0,this.yieldEvery=n||"auto","auto"===this.yieldEvery&&(this.yieldEvery=125),"never"===this.yieldEvery&&null!=t.onYield)throw new Error("yieldEvery is `never` but you provided an `onYield` callback. Either change `yieldEvery` or remove the callback");e.util.isNumber(this.yieldEvery)&&(this.maybeWait=function(t,n){let s,i=e.util.now();return(...r)=>{const a=e.util.now();return a-i<n||(i=a,s=t(...r)),s}}(this.maybeWait.bind(this),this.yieldEvery)),this.trainBegin=t.onTrainBegin,this.trainEnd=t.onTrainEnd,this.epochBegin=t.onEpochBegin,this.epochEnd=t.onEpochEnd,this.batchBegin=t.onBatchBegin,this.batchEnd=t.onBatchEnd,this.yield=t.onYield}async maybeWait(t,n,s){const i=[];null!=this.yield&&(await da(s),i.push(this.yield(t,n,s))),i.push(e.nextFrame()),await Promise.all(i)}async onEpochBegin(t,e){this.currentEpoch=t,null!=this.epochBegin&&(await da(e),await this.epochBegin(t,e))}async onEpochEnd(t,n){const s=[];null!=this.epochEnd&&(await da(n),s.push(this.epochEnd(t,n))),"epoch"===this.yieldEvery&&s.push(e.nextFrame()),await Promise.all(s)}async onBatchBegin(t,e){null!=this.batchBegin&&(await da(e),await this.batchBegin(t,e))}async onBatchEnd(t,n){const s=[];null!=this.batchEnd&&(await da(n),s.push(this.batchEnd(t,n))),"batch"===this.yieldEvery?s.push(e.nextFrame()):e.util.isNumber(this.yieldEvery)&&s.push(this.maybeWait(this.currentEpoch,t,n)),await Promise.all(s)}async onTrainBegin(t){null!=this.trainBegin&&(await da(t),await this.trainBegin(t))}async onTrainEnd(t){null!=this.trainEnd&&(await da(t),await this.trainEnd(t))}}function xa(t,e){if(null==t&&(t={}),t instanceof ma)return[t];if(Array.isArray(t)&&t[0]instanceof ma)return t;return bi(t).map(t=>new ka(t,e))}class va{constructor(){}static registerCallbackConstructor(t,n){e.util.assert(t>=0&&Number.isInteger(t),()=>"Verbosity level is expected to be an integer >= 0, but got "+t),va.checkForDuplicate(n),null==va.constructors[t]&&(va.constructors[t]=[]),va.constructors[t].push(n)}static checkForDuplicate(t){for(const e in va.constructors){va.constructors[+e].forEach(e=>{if(e===t)throw new ci("Duplicate callback constructor.")})}}static clear(){va.constructors={}}static createCallbacks(t){const e=[];for(const n in va.constructors){const s=+n;t>=s&&e.push(...va.constructors[s])}return e.map(t=>new t)}}function Sa(t,e,n,s,i,r,a,o,l){const u=new wa,h=[new ba,...va.createCallbacks(e)];null!=t&&h.push(...t),h.push(u);const c=new ya(h);return c.setParams({epochs:n,initialEpoch:s,samples:i,steps:r,batchSize:a,verbose:e,doValidation:o,metrics:l}),{callbackList:c,history:u}}function Ia(t,n={},s=!1){return Si(t,e.serialization.SerializationMap.getMap().classNameMap,n,"layer",s)}function Na(t,n){return e.tidy(()=>{"float32"!==t.dtype&&(t=t.asType("float32"));const s=e.sum(wr(t),n,!0),i=e.fill(s.shape,li()),r=e.sqrt(e.maximum(s,i));return e.div(t,r)})}function za(t,n){return e.tidy(()=>e.mean(wr(e.sub(n,t)),-1))}function Aa(t,n){return e.tidy(()=>e.mean(e.abs(e.sub(n,t)),-1))}function Ca(t,n){return e.tidy(()=>{const s=e.sub(t,n),i=e.clipByValue(e.abs(t),li(),Number.MAX_VALUE),r=e.abs(e.div(s,i));return e.mul(100,e.mean(r,-1))})}function Da(t,n,s=!1){return e.tidy(()=>{if(s)n=e.softmax(n);else{const t=e.sum(n,n.shape.length-1,!0);n=e.div(n,t)}return n=e.clipByValue(n,li(),1-li()),e.neg(e.sum(e.mul(t.toFloat(),e.log(n)),n.shape.length-1))})}function Ta(t,n,s=!1){return e.tidy(()=>{const i=e.floor(function(t){const e=[sr(t.shape)];return t.reshape(e)}(t)).toInt(),r=(n=e.clipByValue(n,li(),1-li())).shape;return Da(e.oneHot(i,r[r.length-1]).reshape(r),n,s)})}function Ea(t,n){return e.tidy(()=>{let s;return s=e.clipByValue(n,li(),1-li()),s=e.log(e.div(s,e.sub(1,s))),e.mean(function(t,n){if(!e.util.arraysEqual(t.shape,n.shape))throw new ci(`logits and labels must have the same shape, but got shapes ${JSON.stringify(t.shape)} and ${JSON.stringify(n.shape)}`);return e.tidy(()=>{const e=n.relu(),s=n.abs().neg();return e.sub(n.mul(t)).add(s.exp().log1p())})}(t,s),-1)})}function Fa(t,n){return e.tidy(()=>{const s=Na(t,-1),i=Na(n,-1),r=e.mul(s,i);return e.neg(e.sum(r,-1))})}va.constructors={};const La={meanSquaredError:za,meanAbsoluteError:Aa,meanAbsolutePercentageError:Ca,meanSquaredLogarithmicError:function(t,n){return e.tidy(()=>{const s=e.clipByValue(n,li(),Number.MAX_VALUE),i=e.log(e.add(1,s)),r=e.clipByValue(t,li(),Number.MAX_VALUE),a=e.log(e.add(1,r));return e.mean(wr(e.sub(i,a)),-1)})},squaredHinge:function(t,n){return e.tidy(()=>{const s=e.maximum(0,e.sub(1,e.mul(t,n)));return e.mean(wr(s),-1)})},hinge:function(t,n){return e.tidy(()=>{const s=e.maximum(0,e.sub(1,e.mul(t,n)));return e.mean(s,-1)})},categoricalHinge:function(t,n){return e.tidy(()=>{const s=e.sum(e.mul(t,n),-1),i=e.max(e.mul(e.sub(1,t),n),-1);return e.maximum(0,e.add(1,e.sub(i,s)))})},logcosh:function(t,n){return e.tidy(()=>{const s=Math.log(2),i=e.sub(n,t),r=e.sub(e.add(i,e.softplus(e.mul(-2,i))),s);return e.mean(r,-1)})},categoricalCrossentropy:Da,sparseCategoricalCrossentropy:Ta,binaryCrossentropy:Ea,kullbackLeiblerDivergence:function(t,n){return e.tidy(()=>{const s=e.clipByValue(t,li(),1),i=e.clipByValue(n,li(),1);return e.sum(e.mul(t,e.log(e.div(s,i))),-1)})},poisson:function(t,n){return e.tidy(()=>{const s=e.log(e.add(li(),n));return e.mean(e.sub(n,e.mul(t,s)),-1)})},cosineProximity:Fa};function $a(t){if("string"==typeof t){if(t in La)return La[t];let e="Unknown loss "+t;throw t.toLowerCase().includes("softmaxcrossentropy")&&(e=`Unknown loss ${t}. Use "categoricalCrossentropy" as the string name for tf.losses.softmaxCrossEntropy`),new ci(e)}return t}function _a(t,n){return e.tidy(()=>{const s=e.mul(.5,e.onesLike(n)),i=lr(e.greater(n,s),t.dtype);return e.mean(e.equal(t,i),-1)})}function Ra(t,n){return e.tidy(()=>lr(e.equal(e.argMax(t,-1),e.argMax(n,-1)),"float32"))}function Ma(t,n){return e.tidy(()=>e.logicalAnd(t.equal(1),n.equal(1)).sum().cast("float32"))}function Oa(t,n){return e.tidy(()=>{const s=Ma(t,n),i=function(t,n){return e.tidy(()=>e.logicalAnd(t.equal(0),n.equal(1)).sum().cast("float32"))}(t,n),r=s.add(i);return e.where(e.greater(r,0),s.div(r),0).cast("float32")})}function Ba(t,n){return e.tidy(()=>{const s=Ma(t,n),i=function(t,n){return e.tidy(()=>e.logicalAnd(t.equal(1),n.equal(0)).sum().cast("float32"))}(t,n),r=s.add(i);return e.where(e.greater(r,0),s.div(r),0).cast("float32")})}function Pa(t,e){return Ea(t,e)}function Wa(t,n){return t.rank===n.rank&&(t=t.squeeze([t.rank-1])),(n=n.argMax(-1)).dtype!==t.dtype&&(n=n.asType(t.dtype)),e.equal(t,n).asType("float32")}const Ua=Da,Ka=Ta,Va={binaryAccuracy:_a,categoricalAccuracy:Ra,precision:Oa,categoricalCrossentropy:Ua,sparseCategoricalCrossentropy:Ka,mse:za,MSE:za,mae:Aa,MAE:Aa,mape:Ca,MAPE:Ca,cosine:Fa};function ja(t){if("string"==typeof t&&t in Va)return Va[t];if("string"!=typeof t&&null!=t)return t;throw new ci("Unknown metric "+t)}function qa(t){if(gi(null!==t,"Unknown LossOrMetricFn "+t),"string"==typeof t)return t;{let e;for(const n of Object.keys(La))if(La[n]===t){e=n;break}if(void 0!==e)return e;for(const n of Object.keys(Va))if(Va[n]===t){e=n;break}return void 0!==e?e:t.name}}function Ga(t,e,n=!1){if(null==t||"object"!=typeof t||Object.getPrototypeOf(t)!==Object.prototype||!function t(e){if(null===e)return!0;if("object"==typeof e){if(Object.getPrototypeOf(e)===Object.prototype){const n=Object.keys(e);for(const s of n){if("string"!=typeof s)return!1;if(!t(e[s]))return!1}return!0}if(Array.isArray(e)){for(const n of e)if(!t(n))return!1;return!0}return!1}{const t=typeof e;return"string"===t||"number"===t||"boolean"===t}}(t))throw new Error("User-defined metadata is expected to be a JSON object, but is not.");if(n){const n=JSON.stringify(t);n.length>1048576&&console.warn(`User-defined metadata of model "${e}" is too large in size (length=${n.length} when serialized). It is not recommended to store such large objects in user-defined metadata. Please make sure its serialized length is <= 1048576.`)}}function Ha(t,e,n,s=console.log){const i=function(t){let e=!0;const n=[],s=[];for(const e in t.nodesByDepth)n.push(t.nodesByDepth[e]);for(const t of n){if(t.length>1||1===t.length&&t[0].inboundLayers.length>1){e=!1;break}s.push(...t)}if(e)for(const n of t.layers){let t=!1;for(const i of n.inboundNodes)if(-1!==s.indexOf(i)){if(t){e=!1;break}t=!0}if(!e)break}return e}(t),r=["Layer (type)","Output shape","Param #"];let a;if(i?(e=e||65,n=n||[.45,.85,1]):(e=e||98,n=n||[.33,.55,.67,1]),n[n.length-1]<=1&&(n=n.map(t=>Math.floor(e*t))),!i){r.push("Receives inputs"),a=[];for(const e in t.nodesByDepth)a.push(...t.nodesByDepth[e])}s("_".repeat(e)),Ja(r,n,s),s("=".repeat(e));const o=t.layers;for(let t=0;t<o.length;++t)i?Za(o[t],n,s):Xa(o[t],n,a,s),s((t===o.length-1?"=":"_").repeat(e));t.checkTrainableWeightsConsistency();const l=function(t){let e;e=null!=t.collectedTrainableWeights?ea(t.collectedTrainableWeights):ea(t.trainableWeights);return e}(t),u=ea(t.nonTrainableWeights);s("Total params: "+(l+u)),s("Trainable params: "+l),s("Non-trainable params: "+u),s("_".repeat(e))}function Ja(t,e,n=console.log){let s="";for(let n=0;n<t.length;++n)n>0&&(s=s.slice(0,s.length-1)+" "),s+=t[n],s=s.slice(0,e[n]),s+=" ".repeat(e[n]-s.length);n(s)}function Za(t,e,n){let s;try{s=JSON.stringify(t.outputShape)}catch(t){s="multiple"}Ja([`${t.name} (${t.getClassName()})`,s,t.countParams().toString()],e,n)}function Xa(t,e,n,s){let i;try{i=JSON.stringify(t.outputShape)}catch(t){i="multiple"}const r=[];for(const e of t.inboundNodes)if(!(null!=n&&n.length>0&&-1===n.indexOf(e)))for(let t=0;t<e.inboundLayers.length;++t){const n=e.inboundLayers[t].name,s=e.nodeIndices[t],i=e.tensorIndices[t];r.push(`${n}[${s}][${i}]`)}const a=t.name,o=t.getClassName(),l=0===r.length?"":r[0];Ja([`${a} (${o})`,i,t.countParams().toString(),l],e,s);for(let t=1;t<r.length;++t)Ja(["","","",r[t]],e,s)}function Ya(t,e,n){return("inboundNodes"===t||"outputLayers"===t||"inputLayers"===t)&&0===e&&"string"==typeof n}function Qa(t,e){if(null===t)return null;if("string"==typeof t)return ki(t);if("number"==typeof t||"boolean"==typeof t)return t;if(t instanceof Array){const n=[],s=t.length;for(let i=0;i<s;++i){const s=t[i];Ya(e,i,s)?n.push(s):n.push(Qa(s,e))}return n}{const e={};for(const n of Object.keys(t)){const s=t[n];if("name"===n&&"string"==typeof s)e[n]=s;else{const t=ki(n);e[t]=Qa(s,t)}}return e}}class to{constructor(t){if(this.id2Value={},this.id2Mask={},this.name2Id={},t instanceof to)for(const e in t.id2Value)this.id2Value[e]=t.id2Value[e],e in t.id2Mask&&(this.id2Mask[e]=t.id2Mask[e]);else{if(null==t)return;for(const e of t)this.add(e.key,e.value)}}add(t,n,s){if(null!=this.id2Value[t.id])throw new ci(`Duplicate key: name=${t.name}, id=${t.id}`);return this.id2Value[t.id]=function(t,n){if(null==t.dtype||t.dtype===n.dtype)return n;try{return e.cast(n,t.dtype)}catch(e){throw new ci(`The dtype of the feed (${n.dtype}) can not be cast to the dtype of the key '${t.name}' (${t.dtype}).`)}}(t,n),this.name2Id[t.name]=t.id,null!=s&&(this.id2Mask[t.id]=s),this}addFeed(t){this.add(t.key,t.value)}hasKey(t){return null!=this.id2Value[t.id]}names(){return Object.keys(this.name2Id)}getValue(t){if(t instanceof aa){if(null==this.id2Value[t.id])throw new ci("Nonexistent key: "+t.name);return this.id2Value[t.id]}{const e=this.name2Id[t];if(null==e)throw new ci("Feed dict has no SymbolicTensor name: "+t);return this.id2Value[e]}}getMask(t){if(t instanceof aa){if(null==this.id2Value[t.id])throw new ci("Nonexistent key: "+t.name);return this.id2Mask[t.id]}{const e=this.name2Id[t];if(null==e)throw new ci("Feed dict has no SymbolicTensor name: "+t);return this.id2Mask[e]}}disposeMasks(){null!=this.id2Mask&&e.dispose(this.id2Mask)}}const eo={},no={};function so(t,n,s,i){const r=null!=s&&s.training,a=Array.isArray(t),o=a?t:[t],l=o.map(t=>t.name),u=[],h=n.names();for(const t of l)-1!==h.indexOf(t)?u.push(n.getValue(t)):u.push(null);null!=i&&(i.maxNumTensors=-1/0,i.minNumTensors=1/0);const c=l.join(",")+"|"+n.names().join(",");let p,d;if(null==eo[c]){const t=function(t,n){e.util.assert(null!=t&&t.length>0,()=>"Expected at least one fetch, got none");let s=[],i={};if(1===t.length){const e=ro(t[0],n);s=e.sorted,i=e.recipientMap}else{const e=new Set;for(const r of t){const{sorted:t,recipientMap:a}=ro(r,n);for(const n of t)e.has(n.name)||(s.push(n),e.add(n.name));for(const t in a)null==i[t]&&(i[t]=new Set),a[t].forEach(e=>i[t].add(e))}}return{sorted:s,recipientCounts:io(i)}}(o,n);p=t.sorted,d=t.recipientCounts,eo[c]=p,no[c]=d}p=eo[c],d={},r||Object.assign(d,no[c]);const f=new to(n);for(let t=0;t<p.length;++t){if(null!=i){const t=e.memory().numTensors;t>i.maxNumTensors&&(i.maxNumTensors=t),t<i.minNumTensors&&(i.minNumTensors=t)}const a=p[t],o=a.sourceLayer;if(o instanceof ca)continue;const h=[],c=[],g=[];let m=!1;for(const t of a.inputs){const e=f.getValue(t),s=f.getMask(t);h.push(e),c.push(s),null!=s&&(m=!0),r||(d[t.name]--,0!==d[t.name]||n.hasKey(t)||-1!==l.indexOf(t.name)||e.isDisposed||!0===t.sourceLayer.stateful||g.push(e))}m&&((s=s||{}).mask=c[0]);const y=bi(o.apply(h,s));let b=null;o.supportsMasking&&(b=o.computeMask(h,c));const w=ao(a),k=Array.isArray(w)?w:[w];for(let t=0;t<k.length;++t){f.hasKey(k[t])||f.add(k[t],y[t],Array.isArray(b)?b[0]:b);const e=l.indexOf(k[t].name);-1!==e&&(u[e]=y[t])}r||e.dispose(g)}return f.disposeMasks(),a?u:u[0]}function io(t){const e={};for(const n in t)e[n]=t[n].size;return e}function ro(t,e){const n=new Set,s=[],i={};for(const t of e.names())n.add(t);const r=[],a=[];for(r.push(t);r.length>0;){const t=r[r.length-1];if(n.has(t.name)){r.pop();continue}const e=a[a.length-1]===r.length-1;if(0===t.inputs.length||e)r.pop(),s.push(t),n.add(t.name),e&&a.pop();else{a.push(r.length-1);for(const e of t.inputs)null==i[e.name]&&(i[e.name]=new Set),i[e.name].add(t.name),n.has(e.name)||r.push(e)}}return{sorted:s,recipientMap:i}}function ao(t){let e;if(1===t.sourceLayer.inboundNodes.length)e=t.sourceLayer.output;else{let n=null;for(let e=0;e<t.sourceLayer.inboundNodes.length;++e)for(const s of t.sourceLayer.inboundNodes[e].outputTensors)if(s.id===t.id){n=e;break}e=t.sourceLayer.getOutputAt(n)}return e}class oo extends ha{constructor(t){if(super({}),this.containerNodes=new Set,this.name=t.name,null==this.name){const t=this.getClassName().toLowerCase();this.name=Zr(t)}if(this.supportsMasking=!1,this.trainable_=!0,Array.isArray(t.inputs)?this.inputs=t.inputs.slice():this.inputs=[t.inputs],Array.isArray(t.outputs)?this.outputs=t.outputs.slice():this.outputs=[t.outputs],Ni(this.inputs).length!==this.inputs.length)throw new ci("The list of inputs passed to the model is redundant. All inputs should only appear once. Found: "+this.inputs.map(t=>t.name));Ni(this.outputs).length!==this.outputs.length&&console.warn("The list of outputs passed to the model is redundant. All outputs should only appear once. Found: "+this.outputs.map(t=>t.name)),this.inputLayers=[],this.inputLayersNodeIndices=[],this.inputLayersTensorIndices=[],this.outputLayers=[],this.outputLayersNodeIndices=[],this.outputLayersTensorIndices=[],this.layers=[],this.internalContainerRefs=[];for(const t of this.outputs){const e=t.sourceLayer,n=t.nodeIndex,s=t.tensorIndex;this.outputLayers.push(e),this.outputLayersNodeIndices.push(n),this.outputLayersTensorIndices.push(s)}for(const t of this.inputs){const e=t.sourceLayer,n=t.nodeIndex,s=t.tensorIndex;gi(0===n,"input layer has >1 nodes"),gi(0===s,"input layer has >1 tensors"),this.inputLayers.push(e),this.inputLayersNodeIndices.push(n),this.inputLayersTensorIndices.push(s)}this.inputNames=[],this.outputNames=[],this.feedInputShapes=[],this.feedInputNames=[],this.feedOutputNames=[];for(let e=0;e<this.inputLayers.length;e++){const n=this.inputLayers[e];if(!(n instanceof ca))throw new TypeError(`Input layers to a LayersModel must be InputLayer objects. Received inputs: ${t.inputs}. Input ${e} (0-based) originates from layer type ${n.getClassName()}.`);this.inputNames.push(n.name),this.feedInputShapes.push(n.batchInputShape),this.feedInputNames.push(n.name)}for(const t of this.outputLayers)this.outputNames.push(t.name);this.internalInputShapes=this.inputs.map(t=>t.shape),this.internalOutputShapes=this.outputs.map(t=>t.shape);const e={},n={},s={},i={},r={},a=[],o=(t,e,n,s,i,l)=>{null!=s&&null!=i&&null!=l||(s=t.sourceLayer,i=t.nodeIndex,l=t.tensorIndex);const u=s.inboundNodes[i];if(-1!==n.indexOf(u))throw new hi(`The tensor ${t.name} at layer "${s.name}" is part of a cycle.`);if(-1!==e.indexOf(u))return;this.containerNodes.add(oo.nodeKey(s,i)),s.id in r||(r[s.id]=Object.keys(r).length),-1===n.indexOf(u)&&n.push(u);const h=u.inboundLayers.length;for(let t=0;t<h;t++){const s=u.inputTensors[t],i=u.inboundLayers[t],r=u.nodeIndices[t],a=u.tensorIndices[t];o(s,e,n,i,r,a)}for(e.push(u);n.indexOf(u)>=0;)n.splice(n.indexOf(u),1);a.push(u)},l=[],u=[];for(const t of this.outputs)o(t,l,u);const h=a.slice().reverse();for(const t of h){n[t.id]=t,t.id in e||(e[t.id]=0);let r=e[t.id];const a=null==s[t.outboundLayer.id]?0:s[t.outboundLayer.id];r=Math.max(r,a),s[t.outboundLayer.id]=r,i[t.outboundLayer.id]=t.outboundLayer,e[t.id]=r;for(let s=0;s<t.inboundLayers.length;s++){const i=t.inboundLayers[s],a=t.nodeIndices[s],o=i.inboundNodes[a],l=null==e[o.id]?0:e[o.id];e[o.id]=Math.max(r+1,l),n[o.id]=o}}const c={};for(const t in e){const s=e[t];s in c||(c[s]=[]),c[s].push(n[t])}const p={};for(const t in s){const e=s[t];e in p||(p[e]=[]),p[e].push(i[t])}let d=Object.keys(p).map(t=>parseInt(t,10)).sort(Ii);this.layers=[];for(const t of d){const e=p[t];e.sort((t,e)=>{const n=r[t.id],s=r[e.id];return n<s?-1:n>s?1:0});for(const t of e)t instanceof oo&&this.internalContainerRefs.push(t),this.layers.push(t)}this.layersByDepth=p,d=Object.keys(c).map(t=>parseInt(t,10)).sort(Ii);const f=this.inputs.slice(),g=[];for(const t of d)for(const e of c[t]){const t=e.outboundLayer;if(null!=t){for(const n of e.inputTensors)if(-1===f.indexOf(n))throw new hi("Graph disconnected: cannot obtain value for tensor "+n+` at layer "${t.name}". The following previous layers were accessed without issue: `+g);for(const t of e.outputTensors)f.push(t);g.push(t.name)}}this.nodesByDepth=c;const m=this.layers.map(t=>t.name);for(const t of m){const e=m.filter(e=>e===t).length;if(1!==e)throw new hi(`The name "${t}" is used ${e} times in the model. All layer names should be unique. Layer names: `+JSON.stringify(m))}this.outboundNodes=[],this.inboundNodes=[],new la({outboundLayer:this,inboundLayers:[],nodeIndices:[],tensorIndices:[],inputTensors:this.inputs,outputTensors:this.outputs,inputMasks:this.inputs.map(t=>null),outputMasks:this.outputs.map(t=>null),inputShapes:this.inputs.map(t=>t.shape),outputShapes:this.outputs.map(t=>t.shape)}),this.built=!0,this._refCount=1}assertNotDisposed(){if(0===this._refCount)throw new Error(`Container '${this.name}' is already disposed.`)}dispose(){this.assertNotDisposed();const t={refCountAfterDispose:null,numDisposedVariables:0};if(0==--this._refCount){for(const e of this.layers)t.numDisposedVariables+=e.dispose().numDisposedVariables;for(const e of this.internalContainerRefs)t.numDisposedVariables+=e.dispose().numDisposedVariables}return t.refCountAfterDispose=this._refCount,t}get trainable(){return this.trainable_}set trainable(t){this.layers.forEach(e=>{e._trainableWeights.forEach(e=>e.trainable=t)}),this.trainable_=t}get trainableWeights(){if(this._trainableWeights.length>0)throw new ci("Container instance unexpectedly contains _trainableWeights.The trainable weights of a Container are a union of the trainable weights of its consituent Layers. Its own _trainableWeights must remain an empty Array.");if(!this.trainable)return[];let t=[];for(const e of this.layers)t=t.concat(e.trainableWeights);return t}get nonTrainableWeights(){const t=[];for(const e of this.layers)t.push(...e.nonTrainableWeights);if(!this.trainable){const e=[];for(const t of this.layers)e.push(...t.trainableWeights);return e.concat(t)}return t}get weights(){return this.trainableWeights.concat(this.nonTrainableWeights)}loadWeights(t,e=!0){const n={};let s=0;for(const t of this.layers)for(const e of t.weights){if(null!=n[e.originalName])throw new ci("Duplicate weight name: "+e.originalName);n[e.originalName]=e,s++}const i=[];for(const s in t){let r=s;if(null==n[s]){const t=s.split("/");r=t.slice(0,-2).concat([t[t.length-1]]).join("/")}if(null!=n[r])i.push([n[r],t[s]]);else if(e)throw new ci("Provided weight data has no target variable: "+s);delete n[r]}if(e){const t=[];for(const e in n)t.push(e);if(t.length>0)throw new ci(`${t.length} of ${s} weights are not set: `+t)}ia(i)}updatedConfig(){const t=this.getConfig(),e={};return e.className=this.getClassName(),e.config=t,e.kerasVersion="tfjs-layers 3.0.0",e.backend="TensorFlow.js",e}toJSON(t,e=!0){const n=function t(e,n){if(null==e)return null;if("string"==typeof e)return wi(e);if("number"==typeof e||"boolean"==typeof e)return e;if(e instanceof Array){const s=[],i=e.length;for(let r=0;r<i;++r){const i=e[r];Ya(n,r,i)?s.push(i):s.push(t(i,n))}return s}{const n={};for(const s of Object.keys(e)){const i=e[s],r=wi(s);n[r]="name"!==s&&"className"!==s||"string"!=typeof i?t(i,s):i}return n}}(this.updatedConfig());return e?JSON.stringify(n):n}call(t,n){return e.tidy(()=>{t=bi(t);const e=new to;for(let n=0;n<this.inputs.length;++n)e.add(this.inputs[n],t[n]);return so(this.outputs,e,n)})}computeMask(t,n){return e.tidy(()=>{let e;return t=bi(t),e=null==n?fi(null,t.length):bi(n),this.runInternalGraph(t,e)[1]})}computeOutputShape(t){const e=Yr(t);if(e.length!==this.inputLayers.length)throw new ci(`Invalid inputShape argument ${t}: model has ${this.inputLayers.length} tensor inputs.`);const n={};for(let t=0;t<e.length;t++){const s=this.inputLayers[t],i=e[t];n[s.name+"_0_0"]=i}const s=Object.keys(this.nodesByDepth).map(t=>parseInt(t,10)).sort(Ii);if(s.length>1)for(const t of s){const e=this.nodesByDepth[t];for(const t of e){const e=t.outboundLayer;if(-1!==this.inputLayers.map(t=>t.id).indexOf(e.id))continue;const s=[];for(let e=0;e<t.inboundLayers.length;e++){const i=t.inboundLayers[e],r=t.nodeIndices[e],a=t.tensorIndices[e],o=n[`${i.name}_${r}_${a}`];s.push(o)}const i=Yr(e.computeOutputShape(yi(s))),r=e.inboundNodes.indexOf(t);for(let t=0;t<i.length;t++){n[`${e.name}_${r}_${t}`]=i[t]}}}const i=[],r=[];for(let t=0;t<this.outputLayers.length;t++){const e=this.outputLayers[t],n=this.outputLayersNodeIndices[t],s=this.outputLayersTensorIndices[t],i=`${e.name}_${n}_${s}`;r.push(i)}for(let t=0;t<r.length;t++){const e=r[t];gi(e in n),i.push(n[e])}return yi(i)}runInternalGraph(t,e){null==e&&(e=fi(null,t.length));const n={};for(let s=0;s<this.inputs.length;++s){const i=this.inputs[s],r=t[s],a=e[s];n[i.id]=[r,a]}const s=Object.keys(this.nodesByDepth).map(t=>parseInt(t,10)).sort(Ii);for(const t of s){const e=this.nodesByDepth[t];for(const t of e){const e=t.outboundLayer,s=t.inputTensors,i=t.outputTensors,r=new Array;for(const t of s)t.id in n&&r.push(n[t.id]);if(r.length===s.length){let s,a,o,l,u={};if(null!=t.callArgs&&(u=t.callArgs),1===r.length){const[t,n]=r[0];null==u.mask&&(u.mask=n),o=bi(e.call(t,u)),l=bi(e.computeMask(t,n)),s=[t],a=[n]}else s=r.map(t=>t[0]),a=r.map(t=>t[1]),null==u.mask&&(u.mask=a),o=bi(e.call(s,u)),l=bi(e.computeMask(s,a));if(e.activityRegularizer)throw new pi("LayersModel invocation with concrete Tensor value(s) in the presence of activity regularizer(s) is not supported yet.");for(let t=0;t<i.length;++t){const e=i[t],s=o[t],r=l[t];n[e.id]=[s,r]}}}}const i=[],r=[],a=[];for(const t of this.outputs){gi(t.id in n,`Could not compute output ${t.name} : ${t.id}`);const[e,s]=n[t.id];a.push(e.shape),i.push(e),r.push(s)}return[i,r,a]}buildNodeConversionMap(t){const e={};let n;for(const t of this.layers){n=t instanceof oo?1:0;for(let s=0;s<t.inboundNodes.length;s++){const i=oo.nodeKey(t,s);this.containerNodes.has(i)&&(e[i]=n,n+=1)}}return e}getLayer(t,e){if(null!=e){if(this.layers.length<=e)throw new ci(`Was asked to retrieve layer at index ${e}, but model only has ${this.layers.length} layer(s).`);return this.layers[e]}if(null==t)throw new ci("Provide either a layer name or layer index");for(const e of this.layers)if(e.name===t)return e;throw new ci("No such layer: "+t)}calculateLosses(){return e.tidy(()=>{const t=[];for(const e of this.layers)for(let n=0;n<e.inboundNodes.length;++n){const s=oo.nodeKey(e,n);this.containerNodes.has(s)&&t.push(...e.calculateLosses())}return t})}getConfig(){const t={name:this.name},e=this.buildNodeConversionMap(this.layers),n=[];for(const t of this.layers){const s=t.getClassName(),i=t.getConfig(),r=[];for(let n=0;n<t.inboundNodes.length;n++){const s=t.inboundNodes[n],i=oo.nodeKey(t,n);let a={};if(this.containerNodes.has(i)){if(s.callArgs)try{JSON.stringify(s.callArgs),a=s.callArgs}catch(e){console.warn(`Layer ${t.name} was passed non-serializable keyword arguments: `+s.callArgs+". They will not be included in the serialized model (and thus will be missing at deserialization time)."),a={}}if(s.inboundLayers.length>0){const t=[];for(let n=0;n<s.inboundLayers.length;n++){const i=s.inboundLayers[n],r=s.nodeIndices[n],o=s.tensorIndices[n];let l=e[oo.nodeKey(i,r)];null==l&&(l=0),t.push([i.name,l,o,a])}r.push(t)}}}const a={};a.name=t.name,a.className=s,a.config=i,a.inboundNodes=r,n.push(a)}t.layers=n;const s=[];for(let t=0;t<this.inputLayers.length;t++){const n=this.inputLayers[t],i=this.inputLayersNodeIndices[t],r=oo.nodeKey(n,i);if(!this.containerNodes.has(r))continue;let a=e[r];null==a&&(a=0);const o=this.inputLayersTensorIndices[t];s.push([n.name,a,o])}t.inputLayers=s;const i=[];for(let t=0;t<this.outputLayers.length;t++){const n=this.outputLayers[t],s=this.outputLayersNodeIndices[t],r=oo.nodeKey(n,s);if(!this.containerNodes.has(r))continue;let a=e[r];null==a&&(a=0);const o=this.outputLayersTensorIndices[t];i.push([n.name,a,o])}return t.outputLayers=i,t}static fromConfig(t,e,n={},s=!1){const i={},r={};function a(t,e){t.name in r?r[t.name].push(e):r[t.name]=[e]}function o(t,e){const n=[];let s;for(const r of e){const o=r[0],l=r[1],u=r[2];if(s=null==r[3]?{}:r[3],!(o in i))return void a(t,e);const h=i[o];if(h.inboundNodes.length<=l)return void a(t,e);const c=h.inboundNodes[l];n.push(c.outputTensors[u])}n.length>0&&t.apply(yi(n),s)}function l(t){const n=t.name,r=Ia(t,null!=e.customObjects?e.customObjects:{});r.setFastWeightInitDuringBuild(s),i[n]=r;t.inboundNodes.forEach(t=>{if(!(t instanceof Array))throw new ci("Corrupted configuration, expected array for nodeData: "+t);a(r,t)})}const u=e.name,h=e.layers;for(const t of h)l(t);for(;!zi(r);)for(const t of h){const e=i[t.name];if(e.name in r){const t=r[e.name];delete r[e.name];for(const n of t)o(e,n)}}const c=[],p=[],d=e.inputLayers;for(const t of d){const e=t[0],n=t[1],s=t[2];gi(e in i);const r=i[e].inboundNodes[n].outputTensors;c.push(r[s])}const f=e.outputLayers;for(const t of f){const e=t[0],n=t[1],s=t[2];gi(e in i);const r=i[e].inboundNodes[n].outputTensors;p.push(r[s])}return new t({inputs:c,outputs:p,name:u})}get stateful(){if(this._stateful)throw new ci("Container instance unexpectedly has _stateful = true. The statefulness of a Container is determined by the Layers it contains. Its _stateful property must remain the default false.");for(const t of this.layers)if(t.stateful)return!0;return!1}resetStates(){e.tidy(()=>{this.layers.forEach(t=>{t.stateful&&t.resetStates()})})}}function lo(t,e){return function(t,e,n){const s=e.length;if(null==t||Array.isArray(t)&&0===t.length)return e.map(t=>null);if(1===s)return Array.isArray(t)&&1===t.length?t:"object"==typeof t&&e[0]in t?[t[e[0]]]:[t];if(Array.isArray(t)){if(t.length!==s)throw new Error(`Provided ${n} is an array of ${t.length} element(s), but the model has ${s} outputs. Make sure a set of weights is provided for each model output.`);return t}if("object"==typeof t&&Object.keys(t).length>0&&"object"==typeof t[Object.keys(t)[0]]){const n=[];return e.forEach(e=>{e in t?n.push(t[e]):n.push(null)}),n}throw new Error(`The model has multiple (${s}) outputs, so ${n} must be either an array with ${s} elements or an object with ${e} keys. Provided ${n} not understood: ${JSON.stringify(t)}`)}(t,e,"classWeight")}async function uo(t,n,s,i){if(null!=n||null!=i)throw new Error("Support sampleWeight is not implemented yet");if(null!=s){const n=e.tidy(()=>{if(1===t.shape.length)return t.clone();if(2===t.shape.length){if(t.shape[1]>1){const e=1;return t.argMax(e)}if(1===t.shape[1])return t.reshape([t.shape[0]]);throw new Error(`Encountered unexpected last-dimension size (${t.shape[1]}) during handling of class weights. The size is expected to be >= 1.`)}throw new Error(`Unexpected rank of target (y) tensor (${t.rank}) during handling of class weights. The rank is expected to be 1 or 2.`)}),i=Array.from(await n.data());e.dispose(n);const r=[];return i.forEach(t=>{if(null==s[t])throw new Error(`classWeight must contain all classes in the training data. The class ${t} exists in the data but not in classWeight`);r.push(s[t])}),e.tensor1d(r,"float32")}return null}function ho(t,n){return e.mul(t,n)}function co(t,n){let s,i;const r=n;s=r.xs,i=r.ys,e.util.assert(null!=s&&null!=i,()=>"A Dataset iterator for fitDataset() is expected to generate objects of the form `{xs: xVal, ys: yVal}`, where the two values may be `tf.Tensor`, an array of Tensors, or a map of string to Tensor.  The provided Dataset instead generates "+n);const a=po("input",t.inputNames,s),o=po("output",t.outputNames,i),l=a[0].shape[0];e.util.assert(a.length===t.inputs.length,()=>`LayersModel has ${t.inputs.length} inputs, but the dataset provides ${a.length} inputs.  (Expected input keys: `+JSON.stringify(t.inputNames)+")"),e.util.assert(o.length===t.outputs.length,()=>`LayersModel has ${t.outputs.length} outputs, but the dataset provides ${o.length} outputs.  (Expected output keys: `+JSON.stringify(t.outputNames)+")");for(let n=0;n<a.length;n++)e.util.assert(a[n].shape[0]===l,()=>`Batch size mismatch: input ${t.inputNames[n]} has ${a[n].shape[0]}; expected  ${l} based on input ${t.inputNames[0]}.`);for(let n=0;n<o.length;n++)e.util.assert(o[n].shape[0]===l,()=>`Batch size mismatch: output ${t.outputNames[n]} has ${o[n].shape[0]}; expected  ${l} based on input ${t.inputNames[0]}.`);return{xs:a,ys:o}}function po(t,n,s){if(s instanceof e.Tensor)return[s];if(Array.isArray(s))return e.util.assert(s.length===n.length,()=>`Received an array of ${s.length} Tensors, but expected ${n.length} to match the ${t} keys ${n}.`),s;{const e=[];for(const i of n){if(null==s[i])throw new ci(`The feature data generated by the dataset lacks the required ${t} key '${i}'.`);e.push(s[i])}return e}}async function fo(t,n,s){const i=null!=s.batchesPerEpoch;if(e.util.assert(null!=t.optimizer,()=>"You must compile a model before training/testing. Use LayersModel.compile(modelCompileConfig)."),e.util.assert(null!=s,()=>"For fitDataset(), the 2nd argument (config) is required, but it is not provided in this call."),e.util.assert(null!=s.epochs&&s.epochs>0&&Number.isInteger(s.epochs),()=>"For fitDataset(), config.epochs is expected to be a positive integer, but got "+s.epochs),e.util.assert(!i||s.batchesPerEpoch>0&&Number.isInteger(s.batchesPerEpoch),()=>"For fitDataset(), config.batchesPerEpoch is expected to be a positive integer if specified, but got "+s.batchesPerEpoch),e.util.assert(null==s.validationSplit,()=>"`validationSplit` is not supported by `fitDataset()`. Use validationData instead."),t.isTraining)throw new Error("Cannot start training because another fit() call is ongoing.");t.isTraining=!0;try{const r=null!=s.validationData;let a,o;if(r)if(go(s.validationData))e.util.assert(null==s.validationBatches||s.validationBatches>0&&Number.isInteger(s.validationBatches),()=>"For fitDataset() with dataset-based validation, config.validationBatches is expected not to be provided, or to be a positive integer, but got "+s.validationBatches);else{const t=function(t){if(3===t.length)throw new pi("Validation with sample weights is not implemented yet.");return{xs:t[0],ys:t[1]}}(s.validationData);a=t.xs,o=t.ys}const l=t.makeTrainFunction(),u=t.getDedupedMetricsNames();let h;h=r?u.slice().concat(u.map(t=>"val_"+t)):u.slice();const c=xa(s.callbacks,s.yieldEvery),p=null==s.verbose?1:s.verbose,{callbackList:d,history:f}=Sa(c,p,s.epochs,null,null,function(t,e){let n=null;null!=e.batchesPerEpoch?n=e.batchesPerEpoch:Number.isFinite(t.size)&&(n=t.size);return n}(n,s),null,r,h);d.setModel(t),t.history=f,await d.onTrainBegin(),t.stopTraining_=!1;let g=null==s.initialEpoch?0:s.initialEpoch,m=await n.iterator();for(;g<s.epochs;){const h={};await d.onEpochBegin(g);let c=0,p=0;for(i||(m=await n.iterator());!i||c<s.batchesPerEpoch;){const n=await m.next();if(i&&n.done){console.warn("You provided `batchesPerEpoch` as "+s.batchesPerEpoch+", but your dataset iterator ran out of data after "+c+" batches; interrupting training. Make sure that your dataset can generate at least `batchesPerEpoch * epochs` batches (in this case, "+s.batchesPerEpoch*s.epochs+" batches). You may need to use the repeat() function when building your dataset.");break}if(null!=n.value){const{xs:i,ys:r}=co(t,n.value),a={};a.batch=p,a.size=i[0].shape[0],await d.onBatchBegin(p,a);const o=[];if(null!=s.classWeight){const e=lo(s.classWeight,t.outputNames);for(let t=0;t<e.length;++t)o.push(await uo(r[t],null,e[t]))}const h=i.concat(r).concat(o),f=l(h);e.dispose(h);for(let t=0;t<u.length;++t){const n=u[t],s=f[t];a[n]=s,e.keep(s)}await d.onBatchEnd(p,a),fa(a),p++,c++}if(i?c>=s.batchesPerEpoch:n.done){if(r){let e;e=go(s.validationData)?bi(await t.evaluateDataset(s.validationData,{batches:s.validationBatches})):bi(t.evaluate(a,o,{batchSize:null==s.validationBatchSize?32:s.validationBatchSize,verbose:0}));for(let n=0;n<t.metricsNames.length;++n)h["val_"+t.metricsNames[n]]=e[n]}break}if(t.stopTraining_)break}if(await d.onEpochEnd(g,h),g++,t.stopTraining_)break}return await d.onTrainEnd(),await t.history.syncData(),t.history}finally{t.isTraining=!1}}function go(t){return"function"==typeof t.iterator}function mo(t){e.util.assert(t>0&&Number.isInteger(t),()=>"batchSize is required to be a positive integer, but got "+t)}function yo(t,e,n){return null==t?[null]:Array.isArray(t)?t.map(t=>hr(t,e,n-e)):hr(t,e,n-e)}function bo(t,n){return e.tidy(()=>null==t?null:Array.isArray(t)?t.map(t=>bo(t,n)):br(t,"int32"===n.dtype?n:n.toInt()))}function wo(t,e){const n=[];let s=0,i=null;for(;s<t;)i=s+e,i>=t&&(i=t),n.push([s,i]),s=i;return n}async function ko(t,n,s,i={}){if(t.isTraining)throw new Error("Cannot start training because another fit() call is ongoing.");let r,a,o,l,u,h,c;t.isTraining=!0;try{const p=null==i.batchSize?32:i.batchSize;mo(p);const d=!1,f=await t.standardizeUserData(n,s,i.sampleWeight,i.classWeight,d,p);r=f[0],a=f[1],c=f[2];let g,m=!1;if(null!=i.validationData&&i.validationData.length>0){if(m=!0,2!==i.validationData.length)throw 3===i.validationData.length?new pi("validationData including sample weights is not supported yet."):new ci("When passing validation data, it must contain 2 (valX, valY) or 3 (valX, valY, valSampleWeight) items; "+i.validationData+" is invalid.");o=i.validationData[0],l=i.validationData[1];const e=!0,n=await t.standardizeUserData(o,l,null,null,e,p);u=n[0],h=n[1],g=u.concat(h)}else if(null!=i.validationSplit&&i.validationSplit>0&&i.validationSplit<1){m=!0;const t=Math.floor(r[0].shape[0]*(1-i.validationSplit)),e=r[0].shape[0];u=yo(r,t,e),r=yo(r,0,t),h=yo(a,t,e),a=yo(a,0,t),g=u.concat(h)}else null!=i.validationSteps&&(m=!0);const y=r.concat(a).concat(c);t.checkTrainableWeightsConsistency();const b=t.makeTrainFunction(),w=t.getDedupedMetricsNames();let k,x;m?(t.makeTestFunction(),k=t.testFunction,x=w.slice().concat(w.map(t=>"val_"+t))):(k=null,g=[],x=w.slice());const v=xa(i.callbacks,i.yieldEvery);return await async function(t,n,s,i,r,a,o,l,u,h,c,p,d,f,g){null==r&&(r=32),null==a&&(a=1),null==c&&(c=!0),null==d&&(d=0);let m=!1;if(null!=u&&null!=h&&(m=!0),null!=g&&(m=!0,null==f))throw new ci("Can only use `validationSteps` when doing step-wise training, i.e., `stepsPerEpoch` must be set.");const y=t.checkNumSamples(s,r,f,"steps_per_epoch");let b;null!=y&&(b=or(0,y)),null==o&&(o=1);const{callbackList:w,history:k}=Sa(l,o,a,d,y,f,r,m,p);w.setModel(t),t.history=k,await w.onTrainBegin(),t.stopTraining_=!1;for(let o=d;o<a;++o){await w.onEpochBegin(o);const a={};if(null!=f)throw new pi("stepsPerEpoch mode is not implemented yet.");{if("batch"===c)throw new pi("batch shuffling is not implemneted yet");c&&e.util.shuffle(b);const o=e.tensor1d(b),l=wo(y,r);for(let c=0;c<l.length;++c){const p={};if(await w.onBatchBegin(c,p),e.tidy(()=>{const d=l[c][0],f=l[c][1],g=hr(o,d,f-d);p.batch=c,p.size=f-d;const y=bo(s,g),b=n(y);for(let t=0;t<i.length;++t){const n=i[t],s=b[t];p[n]=s,e.keep(s)}if(c===l.length-1&&m){const n=t.testLoop(u,h,r);for(let t=0;t<i.length;++t){const s=i[t],r=n[t];e.keep(r),a["val_"+s]=r}}}),await w.onBatchEnd(c,p),fa(p),t.stopTraining_)break}o.dispose()}if(await w.onEpochEnd(o,a),t.stopTraining_)break}return await w.onTrainEnd(),await t.history.syncData(),t.history}(t,b,y,w,p,i.epochs,i.verbose,v,k,g,i.shuffle,x,i.initialEpoch,null,null)}finally{t.isTraining=!1,vo(r,n),vo(a,s),vo(u,o),vo(h,l),null!=c&&e.dispose(c)}}function xo(t){const n=[];t instanceof e.Tensor&&(t=[t]);for(let e=0;e<t.length;++e){const s=t[e];if(1===s.rank)n.push(ur(s,1));else{if(0===s.rank)throw new Error("Expected tensor to be at least 1D, but received a 0D tensor (scalar).");n.push(s)}}return n}function vo(t,n){if(null==t)return;const s=[];if(n instanceof e.Tensor)s.push(n.id);else if(Array.isArray(n))n.forEach(t=>s.push(t.id));else if(null!=n)for(const t in n){const e=n[t];s.push(e.id)}const i=[];if(t instanceof e.Tensor)-1===s.indexOf(t.id)&&i.push(t);else if(Array.isArray(t))t.forEach(t=>{-1===s.indexOf(t.id)&&i.push(t)});else if(null!=t)for(const e in t){const n=t[e];-1===s.indexOf(n.id)&&i.push(n)}i.forEach(t=>{t.isDisposed||t.dispose()})}function So(t){return Array.isArray(t)}function Io(t){return!function(t){return t instanceof e.Tensor}(t)&&!So(t)}function No(t,e,n,s=!0,i=""){if(null==e||0===e.length){if(null!=t){let e=!1;if(So(t)&&t.length>0)e=!0;else if(Io(t)){for(const n in t)if(t.hasOwnProperty(n)){e=!0;break}}else e=!0;if(e)throw new ci(`Error when checking model ${i} expected no data, but got `+t)}return[]}if(null==t)return e.map(t=>null);let r;if(Io(t)){t=t,r=[];for(const n of e){if(null==t[n])throw new ci(`No data provided for "${n}". Need data for each key in: `+e);r.push(t[n])}}else if(So(t)){if((t=t).length!==e.length)throw new ci(`Error when checking model ${i}: the Array of Tensors that you are passing to your model is not the size the model expected. Expected to see ${e.length} Tensor(s), but instead got the following list of Tensor(s): `+t);r=t}else{if(t=t,e.length>1)throw new ci(`The model ${i} expects ${e.length} Tensor(s), but only received one Tensor. Found: Tensor with shape `+t.shape);r=[t]}if(r=xo(r),null!=n)for(let t=0;t<e.length;++t){if(null==n[t])continue;const a=r[t];if(a.shape.length!==n[t].length)throw new ci(`Error when checking ${i}: expected ${e[t]} to have ${n[t].length} dimension(s). but got array with shape `+a.shape);for(let r=0;r<n[t].length;++r){if(0===r&&!s)continue;const o=a.shape[r],l=n[t][r];if(null!=l&&l>=0&&o!==l)throw new ci(`Error when checking ${i}: expected ${e[t]} to have shape [${n[t]}], but got array with shape [${a.shape}].`)}}return r}function zo(t,e,n,s=!0,i=""){let r;if(Array.isArray(t)){if(t.length!==e.length)throw new ci(`Error when checking model ${i}: the Array of Tensors that you are passing to your model is not the size the the model expected. Expected to see ${e.length} Tensor(s), but instead got ${t.length} Tensors(s).`);r=t}else{if(e.length>1)throw new ci(`The model expects ${e.length} ${i} Tensors, but only received one Tensor. Found: array with shape `+JSON.stringify(t.shape)+".");r=[t]}if(null!=n)for(let t=0;t<e.length;++t){if(null==n[t])continue;const a=r[t];if(a.shape.length!==n[t].length)throw new ci(`Error when checking ${i}: expected ${e[t]} to have ${n[t].length} dimension(s), but got array with shape `+JSON.stringify(a.shape));for(let r=0;r<n[t].length;++r){if(0===r&&!s)continue;const o=a.shape[r],l=n[t][r];if(null!=l&&l!==o)throw new ci(`Error when checking ${i}: expected ${e[t]} to have shape ${JSON.stringify(n[t])} but got array with shape ${JSON.stringify(a.shape)}.`)}}}class Ao extends oo{constructor(t){super(t),this.isTraining=!1}summary(t,e,n=console.log){if(!this.built)throw new ci("This model has never been called, thus its weights have not been created yet. So no summary can be displayed. Build the model first (e.g., by calling it on some test data).");Ha(this,t,e,n)}compile(t){if(null==t.loss&&(t.loss=[]),this.loss=t.loss,"string"==typeof t.optimizer)this.optimizer_=function(t){const n={Adagrad:()=>e.train.adagrad(.01),Adadelta:()=>e.train.adadelta(1,.95,li()),Adam:()=>e.train.adam(.001,.9,.999,li()),Adamax:()=>e.train.adamax(.002,.9,.999,li(),0),RMSProp:()=>e.train.rmsprop(.001,.9,0,li()),SGD:()=>e.train.sgd(.01)};if(n.adagrad=n.Adagrad,n.adadelta=n.Adadelta,n.adam=n.Adam,n.adamax=n.Adamax,n.rmsprop=n.RMSProp,n.sgd=n.SGD,t in n)return n[t]();throw new ci("Unknown Optimizer "+t)}(t.optimizer),this.isOptimizerOwned=!0;else{if(!(t.optimizer instanceof e.Optimizer))throw new ci("User-defined optimizer must be an instance of tf.Optimizer.");this.optimizer_=t.optimizer,this.isOptimizerOwned=!1}let n=[];if(Array.isArray(t.loss)||"string"==typeof t.loss||"function"==typeof t.loss)if(Array.isArray(t.loss)){if(t.loss.length!==this.outputs.length)throw new ci(`When passing an Array as loss, it should have one entry per model output. The model has ${this.outputs.length} output(s), but you passed loss=${t.loss}.`);const e=t.loss;n=e.map(t=>$a(t))}else{const e=$a(t.loss);this.outputs.forEach(t=>{n.push(e)})}else{t.loss=t.loss;for(const e in t.loss)if(-1===this.outputNames.indexOf(e))throw new ci(`Unknown entry in loss dictionary: "${e}". Only expected the following keys: `+this.outputNames);for(const e of this.outputNames)null==t.loss[e]&&console.warn(`Output "${e}" is missing from loss dictionary. We assume this was done on purpose, and we will not be expecting data to be passed to ${e} during training`),n.push($a(t.loss[e]))}this.lossFunctions=n,this.feedOutputNames=[],this.feedOutputShapes=[],this.feedLossFns=[];for(let t=0;t<this.outputs.length;++t){const e=this.internalOutputShapes[t],n=this.outputNames[t];this.feedOutputNames.push(n),this.feedOutputShapes.push(e),this.feedLossFns.push(this.lossFunctions[t])}const s=[];this.metrics=t.metrics,this.metricsNames=["loss"],this.metricsTensors=[],Yi("loss",()=>{for(let t=0;t<this.outputs.length;++t){if(-1!==s.indexOf(t))continue;const e=this.lossFunctions[t];this.outputs.length>1&&(this.metricsTensors.push([e,t]),this.metricsNames.push(this.outputNames[t]+"_loss"))}});const i=function(t,e){if(null==t||Array.isArray(t)&&0===t.length)return e.map(t=>[]);let n;if("string"==typeof t||"function"==typeof t)n=[t];else{if(!Array.isArray(t)&&"object"!=typeof t)throw new TypeError("Type of metrics argument not understood. Expected an string,function, Array, or Object, found: "+t);n=t}if(Array.isArray(n))return e.map(t=>n);{const t=[];for(const s of e){let e=n.hasOwnProperty(s)?n[s]:[];Array.isArray(e)||(e=[e]),t.push(e)}return t}}(t.metrics,this.outputNames),r=(t,e,n)=>{this.outputNames.length>1&&(e=this.outputNames[t]+"_"+e),this.metricsNames.push(e),this.metricsTensors.push([n,t])};Yi("metric",()=>{for(let t=0;t<this.outputs.length;++t){if(-1!==s.indexOf(t))continue;(e=>{let n,s,i;for(const a of e){if("string"==typeof a&&-1!==["accuracy","acc","crossentropy","ce"].indexOf(a)){const e=this.internalOutputShapes[t];let r;1===e[e.length-1]||this.lossFunctions[t]===Ea?-1!==["accuracy","acc"].indexOf(a)?s=_a:-1!==["crossentropy","ce"].indexOf(a)&&(s=Pa):this.lossFunctions[t]===Ta?-1!==["accuracy","acc"].indexOf(a)?s=Wa:-1!==["crossentropy","ce"].indexOf(a)&&(s=Ka):-1!==["accuracy","acc"].indexOf(a)?s=Ra:-1!==["crossentropy","ce"].indexOf(a)&&(s=Ua),-1!==["accuracy","acc"].indexOf(a)?r="acc":-1!==["crossentropy","ce"].indexOf(a)&&(r="ce"),i=s,n=""+r}else{const t=ja(a);i=t,n=""+qa(a)}let e;Yi(n,()=>{e=i}),r(t,n,e)}})(i[t])}}),this.collectedTrainableWeights=this.trainableWeights}checkTrainableWeightsConsistency(){null!=this.collectedTrainableWeights&&this.trainableWeights.length!==this.collectedTrainableWeights.length&&console.warn("Discrepancy between trainableweights and collected trainable weights. Did you set `model.trainable` without calling `model.compile()` afterwards?")}evaluate(t,e,n={}){const s=null==n.batchSize?32:n.batchSize;mo(s);const i=this.standardizeUserDataXY(t,e,!0,s);try{const r=i[0].concat(i[1]);this.makeTestFunction();const a=this.testFunction;return yi(this.testLoop(a,r,s,n.verbose,n.steps))}finally{vo(i[0],t),vo(i[1],e)}}async evaluateDataset(t,n){return this.makeTestFunction(),async function(t,n,s){const i=null!=(s=s||{}).batches,r=t.testFunction;let a=[];if(s.verbose>0)throw new pi("Verbose mode is not implemented yet.");e.util.assert(!i||s.batches>0&&Number.isInteger(s.batches),()=>"Test loop expects `batches` to be a positive integer, but received "+JSON.stringify(s.batches));const o="function"==typeof n.next?n:await n.iterator();let l=0,u=0;for(;!i||u<s.batches;){const n=await o.next();if(a=e.tidy(()=>{if(n.value){const{xs:s,ys:i}=co(t,n.value),o=s.concat(i),h=e.tidy(()=>r(o));if(e.dispose(o),0===u)for(let t=0;t<h.length;++t)a.push(e.scalar(0));const c=o[0].shape[0];for(let t=0;t<h.length;++t){const n=h[t],s=a[t];a[t]=e.tidy(()=>e.add(a[t],e.mul(c,n))),u>0&&e.dispose(s)}e.dispose(h),l+=c,++u}return a}),n.done){i&&console.warn(`Your dataset iterator ran out of data during evaluateDataset(). Interrupting evalution. Make sure that your dataset can generate at least \`batches\` batches (in this case, ${s.batches} batches). You may need to use the repeat() function when building your dataset.`);break}}for(let t=0;t<a.length;++t){const n=a[t];a[t]=e.div(a[t],l),e.dispose(n)}return yi(a)}(this,t,n)}checkNumSamples(t,e,n,s="steps"){let i;if(null!=n){if(i=null,null!=e)throw new ci(`If ${s} is set, batchSize must be null or undefined.Got batchSize = `+e)}else{if(null==t)throw new ci("Either the input data should have a defined shape, or "+s+" shoud be specified.");i=Array.isArray(t)?t[0].shape[0]:t.shape[0]}return i}execute(t,n){if(Array.isArray(n)&&0===n.length)throw new ci("`outputs` is an empty Array, which is not allowed.");const s=Array.isArray(n),i=s?n:[n],r=this.retrieveSymbolicTensors(i),a=new to;if(t instanceof e.Tensor&&(t=[t]),Array.isArray(t)){if(t.length!==this.inputs.length)throw new ci(`The number of inputs provided (${t.length}) does not match the number of inputs of this model (${this.inputs.length}).`);for(let e=0;e<this.inputs.length;++e)a.add(this.inputs[e],t[e])}else for(const e of this.inputs){const n=t[e.name];if(null==n)throw new ci("No value is provided for the model's input "+e.name);a.add(e,n)}const o=so(r,a);return s?o:o[0]}retrieveSymbolicTensors(t){const e=fi(null,t.length);let n=t.length;for(const s of this.layers){const i=Array.isArray(s.output)?s.output:[s.output],r=i.map(t=>t.name);for(let s=0;s<t.length;++s){const a=r.indexOf(t[s]);if(-1!==a&&(e[s]=i[a],n--),0===n)break}if(0===n)break}if(n>0){const n=[];throw e.forEach((e,s)=>{null==e&&n.push(t[s])}),new ci("Cannot find SymbolicTensors for output name(s): "+JSON.stringify(n))}return e}predictLoop(t,n=32,s=!1){return e.tidy(()=>{const i=this.checkNumSamples(t);if(s)throw new pi("Verbose predictLoop() is not implemented yet.");const r=wo(i,n),a=this.outputs.map(t=>[]);for(let n=0;n<r.length;++n){e.tidy(()=>{const e=r[n][0],s=r[n][1],i=yo(t,e,s),a=[];if(Array.isArray(i))for(let t=0;t<i.length;++t)a.push({key:this.inputs[t],value:i[t]});else a.push({key:this.inputs[0],value:i});const o=new to(a);return so(this.outputs,o)}).forEach((t,e)=>a[e].push(t))}return yi(a.map(t=>e.concat(t,0)))})}predict(t,e={}){const n=xo(t);zo(n,this.inputNames,this.feedInputShapes,!1);try{const s=null==e.batchSize?32:e.batchSize;return mo(s),this.predictLoop(n,s)}finally{vo(n,t)}}predictOnBatch(t){zo(t,this.inputNames,this.feedInputShapes,!0);const e=(Array.isArray(t)?t[0]:t).shape[0];return this.predictLoop(t,e)}standardizeUserDataXY(t,n,s=!0,i){if(null==this.optimizer_)throw new hi("You must compile a model before training/testing. Use LayersModel.compile(modelCompileArgs).");const r=[];for(let t=0;t<this.feedOutputShapes.length;++t){const e=this.feedOutputShapes[t];this.feedLossFns[t]===Ta?r.push(e.slice(0,e.length-1).concat([1])):r.push(e)}if(function(t,n,s){const i=Ni(t.map(t=>t.shape[0]));i.sort();const r=Ni(n.map(t=>t.shape[0]));if(r.sort(),i.length>1)throw new ci("All input Tensors (x) should have the same number of samples. Got array shapes: "+JSON.stringify(t.map(t=>t.shape)));if(r.length>1)throw new ci("All target Tensors (y) should have the same number of samples. Got array shapes: "+JSON.stringify(n.map(t=>t.shape)));if(i.length>0&&r.length>0&&!e.util.arraysEqual(i,r))throw new ci(`Input Tensors should have the same number of samples as target Tensors. Found ${i[0]} input sample(s) and ${r[0]} target sample(s).`)}(t=No(t,this.feedInputNames,this.feedInputShapes,!1,"input"),n=No(n,this.feedOutputNames,r,!1,"target")),function(t,e,n){const s=[za,Ea,Da];for(let i=0;i<t.length;++i){const r=t[i],a=e[i],o=n[i];if(null!=a){if(a===Da&&1===r.shape[r.shape.length-1])throw new ci(`You are passing a target array of shape ${r.shape} while using a loss 'categorical_crossentropy'. 'categorical_crossentropy'expects targets to be binary matrices (1s and 0s) of shape [samples, classes].`);if(-1!==s.indexOf(a)){const t=r.shape.slice(1),e=o.slice(1);for(let n=0;n<t.length;++n){const s=t[n],i=e[n];if(null!=i&&s!==i)throw new ci(`A target Tensor with shape ${r.shape} was passed for an output of shape ${o}, while using a loss function that expects targets to have the same shape as the output.`)}}}}}(n,this.feedLossFns,this.feedOutputShapes),this.stateful&&null!=i&&i>0&&t[0].shape[0]%i!=0)throw new ci(`In a stateful network, you should only pass inputs with a number of samples that is divisible by the batch size ${i}. Found: ${t[0].shape[0]} sample(s).`);return[t,n]}async standardizeUserData(t,e,n,s,i=!0,r){const[a,o]=this.standardizeUserDataXY(t,e,i,r);if(null!=n)throw new Error("sample weight is not supported yet.");let l=null;if(null!=s){const t=lo(s,this.outputNames);l=[];for(let e=0;e<t.length;++e)l.push(await uo(o[e],null,t[e]))}return[a,o,l]}testLoop(t,n,s,i=0,r){return e.tidy(()=>{const a=this.checkNumSamples(n,s,r,"steps"),o=[];if(i>0)throw new pi("Verbose mode is not implemented yet.");if(null!=r)throw new pi("steps mode in testLoop() is not implemented yet");{const i=wo(a,s),r=e.tensor1d(or(0,a));for(let s=0;s<i.length;++s){const a=i[s][0],l=i[s][1],u=hr(r,a,l-a),h=bo(n,u),c=t(h);if(0===s)for(let t=0;t<c.length;++t)o.push(e.scalar(0));for(let t=0;t<c.length;++t){const n=c[t];o[t]=e.add(o[t],e.mul(l-a,n))}}for(let t=0;t<o.length;++t)o[t]=e.div(o[t],a)}return o})}getDedupedMetricsNames(){const t=this.metricsNames,e=[];for(let n=0;n<t.length;++n){const s=t[n];let i=s;if(mi(t,s)>1){i+="_"+mi(t.slice(0,n),s)}e.push(i)}return e}makeTrainFunction(){return t=>{const n=[],s=t.slice(0,this.inputs.length),i=t.slice(this.inputs.length,this.inputs.length+this.outputs.length),r=t.slice(this.inputs.length+this.outputs.length,this.inputs.length+2*this.outputs.length),a=[],o=this.collectedTrainableWeights.map(t=>t.read());return[this.optimizer_.minimize(()=>{const t=[];for(let e=0;e<this.inputs.length;++e)t.push({key:this.inputs[e],value:s[e]});const o=new to(t),l=so(this.outputs,o,{training:!0});let u;for(let t=0;t<this.lossFunctions.length;++t){let s=(0,this.lossFunctions[t])(i[t],l[t]);null!=r[t]&&(s=ho(s,r[t]));const a=e.mean(s);n.push(a),u=0===t?s:e.add(u,s)}for(let t=0;t<this.metricsTensors.length;++t){let s;if(this.outputs.length>1&&t<this.outputs.length)s=n[t];else{const n=this.metricsTensors[t][0],r=this.metricsTensors[t][1];s=e.mean(n(i[r],l[r]))}e.keep(s),a.push(s)}return u=e.mean(u),this.calculateLosses().forEach(t=>{u=e.add(u,t)}),u},!0,o)].concat(a)}}makeTestFunction(){this.testFunction=t=>e.tidy(()=>{const n=[];let s;const i=t.slice(0,this.inputs.length),r=t.slice(this.inputs.length,this.inputs.length+this.outputs.length),a=[];for(let t=0;t<this.inputs.length;++t)a.push({key:this.inputs[t],value:i[t]});const o=new to(a),l=so(this.outputs,o);for(let t=0;t<this.lossFunctions.length;++t){const i=this.lossFunctions[t],a=e.mean(i(r[t],l[t]));s=0===t?a:e.add(s,a),n.push(s)}for(let t=0;t<this.metricsTensors.length;++t){const s=this.metricsTensors[t][0],i=this.metricsTensors[t][1],a=e.mean(s(r[i],l[i]));n.push(a)}return n})}async fit(t,e,n={}){return ko(this,t,e,n)}async fitDataset(t,e){return fo(this,t,e)}async trainOnBatch(t,n){const s=await this.standardizeUserData(t,n),i=s[0],r=s[1],a=this.makeTrainFunction()(i.concat(r)),o=[];for(const t of a){const e=await t.data();o.push(e[0])}return e.dispose(a),yi(o)}getNamedWeights(t){const e=[],n=null!=t&&t.trainableOnly,s=n?this.trainableWeights:this.weights,i=this.getWeights(n);for(let t=0;t<s.length;++t)n&&!s[t].trainable||e.push({name:s[t].originalName,tensor:i[t]});return e}set stopTraining(t){this.stopTraining_=t}get stopTraining(){return this.stopTraining_}get optimizer(){return this.optimizer_}set optimizer(t){this.optimizer_!==t&&(this.optimizer_=t,this.isOptimizerOwned=!1)}dispose(){const t=super.dispose();if(0===t.refCountAfterDispose&&null!=this.optimizer&&this.isOptimizerOwned){const n=e.memory().numTensors;this.optimizer_.dispose(),t.numDisposedVariables+=n-e.memory().numTensors}return t}getLossIdentifiers(){let t;if("string"==typeof this.loss)t=wi(this.loss);else if(Array.isArray(this.loss)){for(const t of this.loss)if("string"!=typeof t)throw new Error("Serialization of non-string loss is not supported.");t=this.loss.map(t=>wi(t))}else{const e=Object.keys(this.loss);t={};const n=this.loss;for(const s of e){if("string"!=typeof n[s])throw new Error("Serialization of non-string loss is not supported.");t[s]=wi(n[s])}}return t}getMetricIdentifiers(){if("string"==typeof this.metrics||"function"==typeof this.metrics)return[wi(qa(this.metrics))];if(Array.isArray(this.metrics))return this.metrics.map(t=>wi(qa(t)));{const t={};for(const e in this.metrics)t[e]=wi(qa(this.metrics[e]));return t}}getTrainingConfig(){return{loss:this.getLossIdentifiers(),metrics:this.getMetricIdentifiers(),optimizer_config:{class_name:this.optimizer.getClassName(),config:this.optimizer.getConfig()}}}loadTrainingConfig(t){if(null!=t.weighted_metrics)throw new Error("Loading weight_metrics is not supported yet.");if(null!=t.loss_weights)throw new Error("Loading loss_weights is not supported yet.");if(null!=t.sample_weight_mode)throw new Error("Loading sample_weight_mode is not supported yet.");const e=Ia(Qa(t.optimizer_config));let n,s;if("string"==typeof t.loss)n=ki(t.loss);else if(Array.isArray(t.loss))n=t.loss.map(t=>ki(t));else if(null!=t.loss){n={};for(const e in t.loss)n[e]=ki(t.loss[e])}if(Array.isArray(t.metrics))s=t.metrics.map(t=>ki(t));else if(null!=t.metrics){s={};for(const e in t.metrics)s[e]=ki(t.metrics[e])}this.compile({loss:n,metrics:s,optimizer:e})}async save(t,n){if("string"==typeof t){const n=e.io.getSaveHandlers(t);if(0===n.length)throw new ci(`Cannot find any save handlers for URL '${t}'`);if(n.length>1)throw new ci(`Found more than one (${n.length}) save handlers for URL '${t}'`);t=n[0]}if(null==t.save)throw new ci("LayersModel.save() cannot proceed because the IOHandler provided does not have the `save` attribute defined.");const s=await e.io.encodeWeights(this.getNamedWeights(n)),i={modelTopology:this.toJSON(null,!1),format:"layers-model",generatedBy:"TensorFlow.js tfjs-layers v3.0.0",convertedBy:null};if(null!=n&&n.includeOptimizer&&null!=this.optimizer){i.trainingConfig=this.getTrainingConfig();const t="optimizer",{data:n,specs:r}=await e.io.encodeWeights(await this.optimizer.getWeights(),t);s.specs.push(...r),s.data=e.io.concatenateArrayBuffers([s.data,n])}if(null!=this.userDefinedMetadata){const t=!0;Ga(this.userDefinedMetadata,this.name,t),i.userDefinedMetadata=this.userDefinedMetadata}return i.weightData=s.data,i.weightSpecs=s.specs,t.save(i)}setUserDefinedMetadata(t){Ga(t,this.name),this.userDefinedMetadata=t}getUserDefinedMetadata(){return this.userDefinedMetadata}}Ao.className="Model",e.serialization.registerClass(Ao);class Co extends Ao{}async function Do(t,n){if(null==n&&(n={}),"string"==typeof t){const s=e.io.getLoadHandlers(t,n);if(0===s.length)s.push(e.io.browserHTTPRequest(t,n));else if(s.length>1)throw new ci(`Found more than one (${s.length}) load handlers for URL '${t}'`);t=s[0]}return async function(t,n,s){null==s&&(s={});if(null==t.load)throw new ci("Cannot proceed with model loading because the IOHandler provided does not have the `load` method implemented.");const i=await t.load();let r=i.modelTopology;null!=r.model_config&&(r=r.model_config);const a=null==s.strict||s.strict,o=null!=i.weightData&&null!=i.weightSpecs&&a,l=Ia(Qa(r),n,o),u=i.trainingConfig;null!=u&&l.loadTrainingConfig(u);null!=i.userDefinedMetadata&&l.setUserDefinedMetadata(i.userDefinedMetadata);if(null!=i.weightData){if(null==i.weightSpecs)throw new ci("LayersModel artifacts contains weight data, but not weight specs. Therefore loading of weights cannot proceed.");const{modelWeights:t,optimizerWeights:n}=function(t,n){const s=e.io.decodeWeights(t,n),i={},r=[];return n.forEach(t=>{"optimizer"===t.group?r.push({name:t.name,tensor:s[t.name]}):i[t.name]=s[t.name]}),{modelWeights:i,optimizerWeights:r}}(i.weightData,i.weightSpecs);l.loadWeights(t,a),null!=l.optimizer&&n.length>0&&await l.optimizer.setWeights(n),e.dispose(t),e.dispose(n.map(t=>t.tensor))}return l}(t,void 0,n)}Co.className="Functional",e.serialization.registerClass(Co);class To extends Ao{constructor(t){if(super({inputs:[],outputs:[]}),t=t||{},this.trainable=!0,this.built=!1,this.name=null!=t.name?t.name:Zr("sequential_"),null!=t.layers)for(const e of t.layers)this.add(e)}checkShape(t){if(t.inboundNodes[0].outputTensors[0].shape.some(t=>t<0))throw new ci("Negative dimension size caused by adding layer "+t.name+" with input shape ["+t.inboundNodes[0].inputTensors[0].shape+"]")}add(t){const e=t instanceof To||t instanceof Ao;let n;if(e){if(n=t,1!==n.outputs.length)throw new ci("All layers in a Sequential model should have a single output tensor. For multi-output layers, use the functional API.");if(1!==n.inputs.length)throw new ci("All layers in a Sequential model should have a single input tensor. For multi-input layers, use the functional API.")}if(0===this.outputs.length){if(0===t.inboundNodes.length){if(null==t.batchInputShape)throw new ci("The first layer in a Sequential model must get an `inputShape` or `batchInputShape` argument.");const e=pa({batchShape:t.batchInputShape,dtype:t.dtype,name:t.name+"_input"});t.apply(e)}if(e)this.outputs=n.outputs,this.inputs=n.inputs;else{if(1!==t.inboundNodes.length)throw new ci(`A layer added to a Sequential model must not already be connected somewhere else. LayersModel received layer ${t.name} which has ${t.inboundNodes.length} pre-existing inbound connections.`);if(1!==t.inboundNodes[0].outputTensors.length)throw new ci("All layers in a Sequential model should have a single output tensor. For multi-output layers, use the functional API.");this.checkShape(t),this.outputs=[t.inboundNodes[0].outputTensors[0]],this.inputs=function t(e,n,s){if((null==n||null!=s&&s>0)&&(n=e.sourceLayer,s=e.nodeIndex),0===n.inboundNodes.length)return[e];{const e=n.inboundNodes[s];if(0===e.inboundLayers.length)return e.inputTensors;{const n=[];for(let s=0;s<e.inboundLayers.length;s++){const i=t(e.inputTensors[s],e.inboundLayers[s],e.nodeIndices[s]);for(const t of i)-1===n.indexOf(t)&&n.push(t)}return n}}}(this.outputs[0])}this.inboundNodes=[],new la({outboundLayer:this,inboundLayers:[],nodeIndices:[],tensorIndices:[],inputTensors:this.inputs,outputTensors:this.outputs,inputMasks:fi(null,this.inputs.length),outputMasks:[null],inputShapes:this.inputs.map(t=>t.shape),outputShapes:this.outputs[0].shape})}else{const e=t.apply(this.outputs[0]);if(Array.isArray(e))throw new TypeError("All layers in a Sequential model should have a single output tensor. For multi-output layers, use the functional API.");this.checkShape(t),this.outputs=[e],this.inboundNodes[0].outputTensors=this.outputs,this.inboundNodes[0].outputShapes=[this.outputs[0].shape]}this.layers.push(t),this.built=!1}pop(){if(0===this.layers.length)throw new TypeError("There are no layers in the model.");if(this.layers.pop(),0===this.layers.length)this.outputs=[],this.inboundNodes=[],this.outboundNodes=[];else{const t=this.layers.length-1;this.layers[t].outboundNodes=[],this.outputs=[this.layers[t].output],this.inboundNodes[0].outputTensors=this.outputs,this.inboundNodes[0].outputShapes=[this.outputs[0].shape]}}call(t,e){return null==this.model&&this.build(),this.model.call(t,e)}build(t){if(ta(t),0===this.inputs.length||0===this.outputs.length)throw new TypeError("Sequential model cannot be built: model is empty. Add some layers first.");this.model=new Ao({inputs:this.inputs,outputs:this.outputs[0],name:this.name+"_model"}),this.model.trainable=this.trainable,this.supportsMasking=this.model.supportsMasking,this.inputLayers=this.model.inputLayers,this.inputLayersNodeIndices=this.model.inputLayersNodeIndices,this.inputLayersTensorIndices=this.model.inputLayersTensorIndices,this.outputLayers=this.model.outputLayers,this.outputLayersNodeIndices=this.model.outputLayersNodeIndices,this.outputLayersTensorIndices=this.model.outputLayersTensorIndices,this.nodesByDepth=this.model.nodesByDepth,this.containerNodes=this.model.containerNodes,this.outputNames=this.model.outputNames,this.inputNames=this.model.inputNames,this.built=!0}countParams(){return this.built||this.build(),super.countParams()}summary(t,e,n=console.log){this.built||this.build(),super.summary(t,e,n)}setWeights(t){null==this.model&&this.build(),this.model.setWeights(t)}evaluate(t,e,n={}){if(!this.built)throw new hi("The model needs to be compiled before being used.");return this.model.evaluate(t,e,n)}async evaluateDataset(t,e){if(!this.built)throw new hi("The model needs to be compiled before being used.");return this.model.evaluateDataset(t,e)}predict(t,e={}){return null==this.model&&this.build(),this.model.predict(t,e)}predictOnBatch(t){return null==this.model&&this.build(),this.model.predictOnBatch(t)}compile(t){this.build(),this.model.compile(t),this.optimizer_=this.model.optimizer,this.isOptimizerOwned=this.model.isOptimizerOwned,this.loss=this.model.loss,this.metrics=this.model.metrics,this.metricsTensors=this.model.metricsTensors,this.metricsNames=this.model.metricsNames}get optimizer(){return null==this.model?void 0:this.model.optimizer}set optimizer(t){this.model.optimizer=t}async fit(t,e,n={}){if(!this.built)throw new hi("The model needs to be compiled before being used.");return this.model.fit(t,e,n)}async fitDataset(t,e){if(!this.built)throw new hi("The model needs to be compiled before being used.");return this.model.fitDataset(t,e)}async trainOnBatch(t,e){return this.model.trainOnBatch(t,e)}static fromConfig(t,n,s={},i=!1){let r,a={};if(n instanceof Array){if(null==n[0].className||"Merge"===n[0].className)throw new ci("Legacy serialization format not supported yet.");r=n}else e.util.assert(null!=n.layers,()=>"When the config data for a Sequential model is not an Array, it must be an Object that contains the 'layers' field."),r=n.layers,delete n.layers,a=n;const o=new t(a);if(!(o instanceof To))throw new pi("Sequential.fromConfig called on non-Sequential input: "+o);for(const t of r){const e=Ia(t,void 0,i);i&&e.setFastWeightInitDuringBuild(!0),o.add(e)}return o}set stopTraining(t){if(null==this.model)throw new ci("Cannot set the stopTraining property of a sequential model before it is compiled.");this.model.stopTraining=t}get stopTraining(){if(null==this.model)throw new ci("Cannot get the stopTraining property of a sequential model before it is compiled.");return this.model.stopTraining}getConfig(){const t=[];for(const e of this.layers){const n={};n.className=e.getClassName(),n.config=e.getConfig(),t.push(n)}return{name:this.name,layers:t}}}function Eo(t){return pa(t)}To.className="Sequential",e.serialization.registerClass(To);class Fo extends e.serialization.Serializable{getConfig(){return{}}}class Lo extends Fo{apply(t,n=1){return function(t,n=1){if(1!==n)throw new pi(`Support for alpha values other than 1 (${n}) is not implemented yet.`);return e.elu(t)}(t,n)}}Lo.className="elu",e.serialization.registerClass(Lo);class $o extends Fo{apply(t){return e.selu(t)}}$o.className="selu",e.serialization.registerClass($o);class _o extends Fo{apply(t){return e.relu(t)}}_o.className="relu",e.serialization.registerClass(_o);class Ro extends Fo{apply(t){return e.tidy(()=>e.minimum(6,e.relu(t)))}}Ro.className="relu6",e.serialization.registerClass(Ro);class Mo extends Fo{apply(t){return t}}Mo.className="linear",e.serialization.registerClass(Mo);class Oo extends Fo{apply(t){return e.sigmoid(t)}}Oo.className="sigmoid",e.serialization.registerClass(Oo);class Bo extends Fo{apply(t){return function(t){return e.tidy(()=>{const n=e.add(.5,e.mul(.2,t));return e.clipByValue(n,0,1)})}(t)}}Bo.className="hardSigmoid",e.serialization.registerClass(Bo);class Po extends Fo{apply(t){return e.softplus(t)}}Po.className="softplus",e.serialization.registerClass(Po);class Wo extends Fo{apply(t){return function(t){return e.tidy(()=>e.div(t,e.abs(t).add(1)))}(t)}}Wo.className="softsign",e.serialization.registerClass(Wo);class Uo extends Fo{apply(t){return e.tanh(t)}}Uo.className="tanh",e.serialization.registerClass(Uo);class Ko extends Fo{apply(t,n=-1){return e.softmax(t,n)}}Ko.className="softmax",e.serialization.registerClass(Ko);class Vo extends Fo{apply(t,n=-1){return e.logSoftmax(t,n)}}Vo.className="logSoftmax",e.serialization.registerClass(Vo);class jo extends Fo{apply(t,n=1){return e.tidy(()=>e.sigmoid(t.mul(n)).mul(t))}}function qo(t){return t.getClassName()}function Go(t,n={}){return Si(t,e.serialization.SerializationMap.getMap().classNameMap,n,"activation")}function Ho(t){if(null==t){const t={className:"linear",config:{}};return Go(t)}if("string"==typeof t){const e={};return e.className=t,e.config={},Go(e)}return t instanceof Fo?t:Go(t)}function Jo(t){if(null!=t&&"object"!=typeof t)throw new Error("Argument to L1L2 regularizer's constructor is expected to be an object, but received: "+t)}jo.className="swish",e.serialization.registerClass(jo);class Zo extends e.serialization.Serializable{}class Xo extends Zo{constructor(t){super(),Jo(t),this.l1=null==t||null==t.l1?.01:t.l1,this.l2=null==t||null==t.l2?.01:t.l2,this.hasL1=0!==this.l1,this.hasL2=0!==this.l2}apply(t){return e.tidy(()=>{let n=e.zeros([1]);return this.hasL1&&(n=e.add(n,e.sum(e.mul(this.l1,e.abs(t))))),this.hasL2&&(n=e.add(n,e.sum(e.mul(this.l2,wr(t))))),n.asScalar()})}getConfig(){return{l1:this.l1,l2:this.l2}}static fromConfig(t,e){return new t({l1:e.l1,l2:e.l2})}}Xo.className="L1L2",e.serialization.registerClass(Xo);const Yo={l1l2:"L1L2"};function Qo(t){return vi(t)}function tl(t,n={}){return Si(t,e.serialization.SerializationMap.getMap().classNameMap,n,"regularizer")}function el(t){if(null==t)return null;if("string"==typeof t){return tl({className:t in Yo?Yo[t]:t,config:{}})}return t instanceof Zo?t:tl(t)}class nl extends ha{constructor(t){super(null==t?{}:t),this.supportsMasking=!0,null!=t&&(this.maxValue=t.maxValue)}call(t,n){t=Qr(t);let s=e.relu(t);return null!=this.maxValue&&(s=e.clipByValue(s,0,this.maxValue)),s}computeOutputShape(t){return t}getConfig(){const t={maxValue:this.maxValue},e=super.getConfig();return Object.assign(t,e),t}}nl.className="ReLU",e.serialization.registerClass(nl);class sl extends ha{constructor(t){super(null==t?{}:t),this.DEFAULT_ALPHA=.3,null==t&&(t={}),this.alpha=null==t.alpha?this.DEFAULT_ALPHA:t.alpha}call(t,n){const s=Qr(t);return e.leakyRelu(s,this.alpha)}computeOutputShape(t){return t}getConfig(){const t={alpha:this.alpha},e=super.getConfig();return Object.assign(t,e),t}}sl.className="LeakyReLU",e.serialization.registerClass(sl);class il extends ha{constructor(t){if(super(null==t?{}:t),this.DEFAULT_ALPHA_INITIALIZER="zeros",null==t&&(t={}),this.supportsMasking=!0,this.alphaInitializer=jr(t.alphaInitializer||this.DEFAULT_ALPHA_INITIALIZER),this.alphaRegularizer=el(t.alphaRegularizer),this.alphaConstraint=Pi(t.alphaConstraint),null==t.sharedAxes)this.sharedAxes=null;else if(Array.isArray(t.sharedAxes))this.sharedAxes=t.sharedAxes;else{if("number"!=typeof t.sharedAxes)throw new ci("Expected sharedAxes to be a number or an array of numbers, but got "+t.sharedAxes);this.sharedAxes=[t.sharedAxes]}}build(t){const e=(t=ta(t)).slice(1);if(null!=this.sharedAxes)for(const t of this.sharedAxes)e[t-1]=1;this.alpha=this.addWeight("alpha",e,"float32",this.alphaInitializer,this.alphaRegularizer,!0,this.alphaConstraint);const n={};if(null!=this.sharedAxes)for(let e=1;e<t.length;++e)n[e]=t[e];this.inputSpec=[new ra({ndim:t.length,axes:n})],this.built=!0}call(t,n){return t=Qr(t),e.prelu(t,this.alpha.read())}getConfig(){const t={alphaInitializer:Vr(this.alphaInitializer),alphaRegularizer:Qo(this.alphaRegularizer),alphaConstraint:Oi(this.alphaConstraint),sharedAxes:this.sharedAxes},e=super.getConfig();return Object.assign(t,e),t}}il.className="PReLU",e.serialization.registerClass(il);class rl extends ha{constructor(t){if(super(null==t?{}:t),this.DEFAULT_ALPHA=1,null==t&&(t={}),null!=t.alpha&&t.alpha!==this.DEFAULT_ALPHA)throw new pi(`Non-default alpha value (${t.alpha}) is not supported by the ELU layer yet.`);this.alpha=null==t.alpha?this.DEFAULT_ALPHA:t.alpha}call(t,n){const s=Qr(t);return e.elu(s)}computeOutputShape(t){return t}getConfig(){const t={alpha:this.alpha},e=super.getConfig();return Object.assign(t,e),t}}rl.className="ELU",e.serialization.registerClass(rl);class al extends ha{constructor(t){super(null==t?{}:t),this.DEFAULT_THETA=1,null==t&&(t={}),this.theta=null==t.theta?this.DEFAULT_THETA:t.theta}call(t,e){const n=Qr(t);return n.mul(lr(n.greater(this.theta),"float32"))}computeOutputShape(t){return t}getConfig(){const t={theta:this.theta},e=super.getConfig();return Object.assign(t,e),t}}al.className="ThresholdedReLU",e.serialization.registerClass(al);class ol extends ha{constructor(t){super(null==t?{}:t),this.DEFAULT_AXIS=1,null==t&&(t={}),this.softmax=(new Ko).apply,this.axis=null==t.axis?this.DEFAULT_AXIS:t.axis}call(t,e){const n=Qr(t);return this.softmax(n,this.axis)}computeOutputShape(t){return t}getConfig(){const t={axis:this.axis},e=super.getConfig();return Object.assign(t,e),t}}function ll(t,e,n){if("number"==typeof t)return fi(t,e);if(t.length!==e)throw new ci(`The ${n} argument must be an integer or tuple of ${e} integers. Received: ${t.length} elements.`);for(let i=0;i<e;++i){const r=t[i];if((s=r)!==parseInt(s.toString(),10))throw new ci(`The ${n} argument must be an integer or tuple of ${e} integers. Received: ${JSON.stringify(t)} including a non-integer number `+r)}return t;var s}function ul(t,e,n,s,i=1){if(null==t)return t;let r;return r="same"===n?t:t-(e+(e-1)*(i-1))+1,Math.floor((r+s-1)/s)}function hl(t,e,n,s){if(null==t)return null;if("valid"===s)t=t*e+ar([n-e,0]);else{if("same"!==s)throw new ci(`Unsupport padding mode: ${s}.`);t*=e}return t}function cl(t,n){return e.tidy(()=>(Hi(n),"channelsFirst"===n?e.transpose(t,[0,2,3,1]):t))}function pl(t,n){return e.tidy(()=>(Hi(n),"channelsFirst"===n?e.transpose(t,[0,2,3,4,1]):t))}function dl(t,n,s,i=[1,1],r="valid",a,o,l=null){return e.tidy(()=>{if(null==a&&(a="channelsLast"),Hi(a),3!==t.rank&&4!==t.rank)throw new ci(`conv2dWithBiasActivation expects input to be of rank 3 or 4, but received ${t.rank}.`);if(3!==n.rank&&4!==n.rank)throw new ci(`conv2dWithBiasActivation expects kernel to be of rank 3 or 4, but received ${t.rank}.`);let u=cl(t,a);if("causal"===r)throw new pi("The support for CAUSAL padding mode in conv1dWithBias is not implemented yet.");return u=e.fused.conv2d({x:u,filter:n,strides:i,pad:"same"===r?"same":"valid",dilations:o,dataFormat:"NHWC",bias:s,activation:l}),"channelsFirst"===a&&(u=e.transpose(u,[0,3,1,2])),u})}ol.className="Softmax",e.serialization.registerClass(ol);class fl extends ha{constructor(t,e){if(super(e),this.bias=null,this.DEFAULT_KERNEL_INITIALIZER="glorotNormal",this.DEFAULT_BIAS_INITIALIZER="zeros",fl.verifyArgs(e),this.rank=t,Di(this.rank,"rank"),1!==this.rank&&2!==this.rank&&3!==this.rank)throw new pi(`Convolution layer for rank other than 1, 2, or 3 (${this.rank}) is not implemented yet.`);if(this.kernelSize=ll(e.kernelSize,t,"kernelSize"),this.strides=ll(null==e.strides?1:e.strides,t,"strides"),this.padding=null==e.padding?"valid":e.padding,Ji(this.padding),this.dataFormat=null==e.dataFormat?"channelsLast":e.dataFormat,Hi(this.dataFormat),this.activation=Ho(e.activation),this.useBias=null==e.useBias||e.useBias,this.biasInitializer=jr(e.biasInitializer||this.DEFAULT_BIAS_INITIALIZER),this.biasConstraint=Pi(e.biasConstraint),this.biasRegularizer=el(e.biasRegularizer),this.activityRegularizer=el(e.activityRegularizer),this.dilationRate=ll(null==e.dilationRate?1:e.dilationRate,t,"dilationRate"),1===this.rank&&Array.isArray(this.dilationRate)&&1!==this.dilationRate.length)throw new ci("dilationRate must be a number or an array of a single number for 1D convolution, but received "+JSON.stringify(this.dilationRate));if(2===this.rank){if("number"==typeof this.dilationRate)this.dilationRate=[this.dilationRate,this.dilationRate];else if(2!==this.dilationRate.length)throw new ci("dilationRate must be a number or array of two numbers for 2D convolution, but received "+JSON.stringify(this.dilationRate))}else if(3===this.rank)if("number"==typeof this.dilationRate)this.dilationRate=[this.dilationRate,this.dilationRate,this.dilationRate];else if(3!==this.dilationRate.length)throw new ci("dilationRate must be a number or array of three numbers for 3D convolution, but received "+JSON.stringify(this.dilationRate))}static verifyArgs(t){if(gi("kernelSize"in t,"required key 'kernelSize' not in config"),"number"!=typeof t.kernelSize&&!Ci(t.kernelSize,"number",1,3))throw new ci(`BaseConv expects config.kernelSize to be number or number[] with length 1, 2, or 3, but received ${JSON.stringify(t.kernelSize)}.`)}getConfig(){const t={kernelSize:this.kernelSize,strides:this.strides,padding:this.padding,dataFormat:this.dataFormat,dilationRate:this.dilationRate,activation:qo(this.activation),useBias:this.useBias,biasInitializer:Vr(this.biasInitializer),biasRegularizer:Qo(this.biasRegularizer),activityRegularizer:Qo(this.activityRegularizer),biasConstraint:Oi(this.biasConstraint)},e=super.getConfig();return Object.assign(t,e),t}}class gl extends fl{constructor(t,e){super(t,e),this.kernel=null,gl.verifyArgs(e),this.filters=e.filters,Di(this.filters,"filters"),this.kernelInitializer=jr(e.kernelInitializer||this.DEFAULT_KERNEL_INITIALIZER),this.kernelConstraint=Pi(e.kernelConstraint),this.kernelRegularizer=el(e.kernelRegularizer)}build(t){t=ta(t);const e="channelsFirst"===this.dataFormat?1:t.length-1;if(null==t[e])throw new ci("The channel dimension of the input should be defined. Found "+t[e]);const n=t[e],s=this.kernelSize.concat([n,this.filters]);this.kernel=this.addWeight("kernel",s,null,this.kernelInitializer,this.kernelRegularizer,!0,this.kernelConstraint),this.useBias&&(this.bias=this.addWeight("bias",[this.filters],null,this.biasInitializer,this.biasRegularizer,!0,this.biasConstraint)),this.inputSpec=[{ndim:this.rank+2,axes:{[e]:n}}],this.built=!0}call(t,n){return e.tidy(()=>{let n;t=Qr(t);const s=null==this.bias?null:this.bias.read(),i=Ti(this.activation.getClassName());if(null!=i&&2===this.rank)n=dl(t,this.kernel.read(),s,this.strides,this.padding,this.dataFormat,this.dilationRate,i);else{if(1===this.rank)n=function(t,n,s,i=1,r="valid",a,o=1){return e.tidy(()=>{if(null==a&&(a="channelsLast"),Hi(a),3!==t.shape.length)throw new ci("The input of a conv1dWithBias operation should be 3, but is "+t.shape.length+" instead.");if(3!==n.shape.length)throw new ci("The kernel for a conv1dWithBias operation should be 3, but is "+n.shape.length+" instead");if(null!=s&&1!==s.shape.length)throw new ci("The bias for a conv1dWithBias operation should be 1, but is "+n.shape.length+" instead");if("channelsFirst"===a&&(t=e.transpose(t,[0,2,1])),"causal"===r)throw new pi("The support for CAUSAL padding mode in conv1dWithBias is not implemented yet.");let l=e.conv1d(t,n,i,"same"===r?"same":"valid","NWC",o);return null!=s&&(l=xr(l,s)),l})}(t,this.kernel.read(),s,this.strides[0],this.padding,this.dataFormat,this.dilationRate[0]);else if(2===this.rank)n=dl(t,this.kernel.read(),s,this.strides,this.padding,this.dataFormat,this.dilationRate);else{if(3!==this.rank)throw new pi("convolutions greater than 3D are not implemented yet.");n=function(t,n,s,i=[1,1,1],r="valid",a,o){return e.tidy(()=>{if(null==a&&(a="channelsLast"),Hi(a),4!==t.rank&&5!==t.rank)throw new ci("conv3dWithBias expects input to be of rank 4 or 5, but received "+t.rank+".");if(4!==n.rank&&5!==n.rank)throw new ci("conv3dWithBias expects kernel to be of rank 4 or 5, but received "+t.rank+".");let l=pl(t,a);if("causal"===r)throw new pi("The support for CAUSAL padding mode in conv3dWithBias is not implemented yet.");return l=e.conv3d(l,n,i,"same"===r?"same":"valid","NDHWC",o),null!=s&&(l=xr(l,s)),"channelsFirst"===a&&(l=e.transpose(l,[0,4,1,2,3])),l})}(t,this.kernel.read(),s,this.strides,this.padding,this.dataFormat,this.dilationRate)}null!=this.activation&&(n=this.activation.apply(n))}return n})}computeOutputShape(t){t=ta(t);const e=[],n="channelsLast"===this.dataFormat?t.slice(1,t.length-1):t.slice(2);for(let t=0;t<n.length;++t){const s=ul(n[t],this.kernelSize[t],this.padding,this.strides[t],"number"==typeof this.dilationRate?this.dilationRate:this.dilationRate[t]);e.push(s)}let s=[t[0]];return"channelsLast"===this.dataFormat?(s=s.concat(e),s.push(this.filters)):(s.push(this.filters),s=s.concat(e)),s}getConfig(){const t={filters:this.filters,kernelInitializer:Vr(this.kernelInitializer),kernelRegularizer:Qo(this.kernelRegularizer),kernelConstraint:Oi(this.kernelConstraint)},e=super.getConfig();return Object.assign(t,e),t}static verifyArgs(t){if(!("filters"in t)||"number"!=typeof t.filters||t.filters<1)throw new ci("Convolution layer expected config.filters to be a 'number' > 0 but got "+JSON.stringify(t.filters))}}class ml extends gl{constructor(t){super(2,t),ml.verifyArgs(t)}getConfig(){const t=super.getConfig();return delete t.rank,t}static verifyArgs(t){if("number"!=typeof t.kernelSize&&!Ci(t.kernelSize,"number",1,2))throw new ci(`Conv2D expects config.kernelSize to be number or number[] with length 1 or 2, but received ${JSON.stringify(t.kernelSize)}.`)}}ml.className="Conv2D",e.serialization.registerClass(ml);class yl extends gl{constructor(t){super(3,t),yl.verifyArgs(t)}getConfig(){const t=super.getConfig();return delete t.rank,t}static verifyArgs(t){if("number"!=typeof t.kernelSize&&(!Array.isArray(t.kernelSize)||1!==t.kernelSize.length&&3!==t.kernelSize.length))throw new ci(`Conv3D expects config.kernelSize to be number or [number, number, number], but received ${JSON.stringify(t.kernelSize)}.`)}}yl.className="Conv3D",e.serialization.registerClass(yl);class bl extends ml{constructor(t){if(super(t),this.inputSpec=[new ra({ndim:4})],"same"!==this.padding&&"valid"!==this.padding)throw new ci("Conv2DTranspose currently supports only padding modes 'same' and 'valid', but received padding mode "+this.padding)}build(t){if(4!==(t=ta(t)).length)throw new ci("Input should have rank 4; Received input shape: "+JSON.stringify(t));const e="channelsFirst"===this.dataFormat?1:t.length-1;if(null==t[e])throw new ci("The channel dimension of the inputs should be defined. Found `None`.");const n=t[e],s=this.kernelSize.concat([this.filters,n]);this.kernel=this.addWeight("kernel",s,"float32",this.kernelInitializer,this.kernelRegularizer,!0,this.kernelConstraint),this.useBias&&(this.bias=this.addWeight("bias",[this.filters],"float32",this.biasInitializer,this.biasRegularizer,!0,this.biasConstraint)),this.inputSpec=[new ra({ndim:4,axes:{[e]:n}})],this.built=!0}call(t,n){return e.tidy(()=>{let n=Qr(t);if(4!==n.shape.length)throw new ci("Conv2DTranspose.call() expects input tensor to be rank-4, but received a tensor of rank-"+n.shape.length);const s=n.shape,i=s[0];let r,a;"channelsFirst"===this.dataFormat?(r=2,a=3):(r=1,a=2);const o=s[r],l=s[a],u=this.kernelSize[0],h=this.kernelSize[1],c=this.strides[0],p=this.strides[1],d=[i,hl(o,c,u,this.padding),hl(l,p,h,this.padding),this.filters];"channelsLast"!==this.dataFormat&&(n=e.transpose(n,[0,2,3,1]));let f=e.conv2dTranspose(n,this.kernel.read(),d,this.strides,this.padding);return"channelsLast"!==this.dataFormat&&(f=e.transpose(f,[0,3,1,2])),null!=this.bias&&(f=xr(f,this.bias.read(),this.dataFormat)),null!=this.activation&&(f=this.activation.apply(f)),f})}computeOutputShape(t){const e=(t=ta(t)).slice();let n,s,i;"channelsFirst"===this.dataFormat?(n=1,s=2,i=3):(n=3,s=1,i=2);const r=this.kernelSize[0],a=this.kernelSize[1],o=this.strides[0],l=this.strides[1];return e[n]=this.filters,e[s]=hl(e[s],o,r,this.padding),e[i]=hl(e[i],l,a,this.padding),e}getConfig(){const t=super.getConfig();return delete t.dilationRate,t}}bl.className="Conv2DTranspose",e.serialization.registerClass(bl);class wl extends gl{constructor(t,e){if(super(t,e),this.DEFAULT_DEPTHWISE_INITIALIZER="glorotUniform",this.DEFAULT_POINTWISE_INITIALIZER="glorotUniform",this.depthwiseKernel=null,this.pointwiseKernel=null,null==e.filters)throw new ci("The `filters` configuration field is required by SeparableConv, but is unspecified.");if(null!=e.kernelInitializer||null!=e.kernelRegularizer||null!=e.kernelConstraint)throw new ci("Fields kernelInitializer, kernelRegularizer and kernelConstraint are invalid for SeparableConv2D. Use depthwiseInitializer, depthwiseRegularizer, depthwiseConstraint, pointwiseInitializer, pointwiseRegularizer and pointwiseConstraint instead.");if(null!=e.padding&&"same"!==e.padding&&"valid"!==e.padding)throw new ci(`SeparableConv${this.rank}D supports only padding modes: 'same' and 'valid', but received `+JSON.stringify(e.padding));this.depthMultiplier=null==e.depthMultiplier?1:e.depthMultiplier,this.depthwiseInitializer=jr(e.depthwiseInitializer||this.DEFAULT_DEPTHWISE_INITIALIZER),this.depthwiseRegularizer=el(e.depthwiseRegularizer),this.depthwiseConstraint=Pi(e.depthwiseConstraint),this.pointwiseInitializer=jr(e.depthwiseInitializer||this.DEFAULT_POINTWISE_INITIALIZER),this.pointwiseRegularizer=el(e.pointwiseRegularizer),this.pointwiseConstraint=Pi(e.pointwiseConstraint)}build(t){if((t=ta(t)).length<this.rank+2)throw new ci(`Inputs to SeparableConv${this.rank}D should have rank `+(this.rank+2)+", but received input shape: "+JSON.stringify(t));const e="channelsFirst"===this.dataFormat?1:t.length-1;if(null==t[e]||t[e]<0)throw new ci("The channel dimension of the inputs should be defined, but found "+JSON.stringify(t[e]));const n=t[e],s=this.kernelSize.concat([n,this.depthMultiplier]),i=[];for(let t=0;t<this.rank;++t)i.push(1);i.push(n*this.depthMultiplier,this.filters);this.depthwiseKernel=this.addWeight("depthwise_kernel",s,"float32",this.depthwiseInitializer,this.depthwiseRegularizer,!0,this.depthwiseConstraint),this.pointwiseKernel=this.addWeight("pointwise_kernel",i,"float32",this.pointwiseInitializer,this.pointwiseRegularizer,!0,this.pointwiseConstraint),this.useBias?this.bias=this.addWeight("bias",[this.filters],"float32",this.biasInitializer,this.biasRegularizer,!0,this.biasConstraint):this.bias=null,this.inputSpec=[new ra({ndim:this.rank+2,axes:{[e]:n}})],this.built=!0}call(t,n){return e.tidy(()=>{let n;if(t=Qr(t),1===this.rank)throw new pi("1D separable convolution is not implemented yet.");return 2===this.rank&&("channelsFirst"===this.dataFormat&&(t=e.transpose(t,[0,2,3,1])),n=e.separableConv2d(t,this.depthwiseKernel.read(),this.pointwiseKernel.read(),this.strides,this.padding,this.dilationRate,"NHWC")),this.useBias&&(n=xr(n,this.bias.read(),this.dataFormat)),null!=this.activation&&(n=this.activation.apply(n)),"channelsFirst"===this.dataFormat&&(n=e.transpose(n,[0,3,1,2])),n})}getConfig(){const t=super.getConfig();return delete t.rank,delete t.kernelInitializer,delete t.kernelRegularizer,delete t.kernelConstraint,t.depthwiseInitializer=Vr(this.depthwiseInitializer),t.pointwiseInitializer=Vr(this.pointwiseInitializer),t.depthwiseRegularizer=Qo(this.depthwiseRegularizer),t.pointwiseRegularizer=Qo(this.pointwiseRegularizer),t.depthwiseConstraint=Oi(this.depthwiseConstraint),t.pointwiseConstraint=Oi(this.pointwiseConstraint),t}}wl.className="SeparableConv";class kl extends wl{constructor(t){super(2,t)}}kl.className="SeparableConv2D",e.serialization.registerClass(kl);class xl extends gl{constructor(t){super(1,t),xl.verifyArgs(t),this.inputSpec=[{ndim:3}]}getConfig(){const t=super.getConfig();return delete t.rank,delete t.dataFormat,t}static verifyArgs(t){if("number"!=typeof t.kernelSize&&!Ci(t.kernelSize,"number",1,1))throw new ci(`Conv1D expects config.kernelSize to be number or number[] with length 1, but received ${JSON.stringify(t.kernelSize)}.`)}}xl.className="Conv1D",e.serialization.registerClass(xl);class vl extends ha{constructor(t){super(t),"number"==typeof t.cropping?this.cropping=[[t.cropping,t.cropping],[t.cropping,t.cropping]]:"number"==typeof t.cropping[0]?this.cropping=[[t.cropping[0],t.cropping[0]],[t.cropping[1],t.cropping[1]]]:this.cropping=t.cropping,this.dataFormat=void 0===t.dataFormat?"channelsLast":t.dataFormat,this.inputSpec=[{ndim:4}]}computeOutputShape(t){return"channelsFirst"===this.dataFormat?[t[0],t[1],t[2]-this.cropping[0][0]-this.cropping[0][1],t[3]-this.cropping[1][0]-this.cropping[1][1]]:[t[0],t[1]-this.cropping[0][0]-this.cropping[0][1],t[2]-this.cropping[1][0]-this.cropping[1][1],t[3]]}call(t,n){return e.tidy(()=>{if(t=Qr(t),"channelsLast"===this.dataFormat){const e=pr(t,this.cropping[0][0],t.shape[1]-this.cropping[0][0]-this.cropping[0][1],2);return pr(e,this.cropping[1][0],t.shape[2]-this.cropping[1][1]-this.cropping[1][0],3)}{const e=pr(t,this.cropping[0][0],t.shape[2]-this.cropping[0][0]-this.cropping[0][1],3);return pr(e,this.cropping[1][0],t.shape[3]-this.cropping[1][1]-this.cropping[1][0],4)}})}getConfig(){const t={cropping:this.cropping,dataFormat:this.dataFormat},e=super.getConfig();return Object.assign(t,e),t}}vl.className="Cropping2D",e.serialization.registerClass(vl);class Sl extends ha{constructor(t){var e;super(t),this.DEFAULT_SIZE=[2,2],this.inputSpec=[{ndim:4}],this.size=null==t.size?this.DEFAULT_SIZE:t.size,this.dataFormat=null==t.dataFormat?"channelsLast":t.dataFormat,Hi(this.dataFormat),this.interpolation=null==t.interpolation?"nearest":t.interpolation,e=this.interpolation,Ai(Ki,"InterpolationFormat",e)}computeOutputShape(t){if("channelsFirst"===this.dataFormat){const e=null==t[2]?null:this.size[0]*t[2],n=null==t[3]?null:this.size[1]*t[3];return[t[0],t[1],e,n]}{const e=null==t[1]?null:this.size[0]*t[1],n=null==t[2]?null:this.size[1]*t[2];return[t[0],e,n,t[3]]}}call(t,n){return e.tidy(()=>{let n=Qr(t);const s=n.shape;if("channelsFirst"===this.dataFormat){n=e.transpose(n,[0,2,3,1]);const t=this.size[0]*s[2],i=this.size[1]*s[3],r="nearest"===this.interpolation?n.resizeNearestNeighbor([t,i]):n.resizeBilinear([t,i]);return e.transpose(r,[0,3,1,2])}{const t=this.size[0]*s[1],e=this.size[1]*s[2];return"nearest"===this.interpolation?n.resizeNearestNeighbor([t,e]):n.resizeBilinear([t,e])}})}getConfig(){const t={size:this.size,dataFormat:this.dataFormat},e=super.getConfig();return Object.assign(t,e),t}}Sl.className="UpSampling2D",e.serialization.registerClass(Sl);class Il extends fl{constructor(t){super(2,t),this.depthwiseKernel=null,this.depthMultiplier=null==t.depthMultiplier?1:t.depthMultiplier,this.depthwiseInitializer=jr(t.depthwiseInitializer||this.DEFAULT_KERNEL_INITIALIZER),this.depthwiseConstraint=Pi(t.depthwiseConstraint),this.depthwiseRegularizer=el(t.depthwiseRegularizer)}build(t){if((t=ta(t)).length<4)throw new ci(`Inputs to DepthwiseConv2D should have rank 4. Received input shape: ${JSON.stringify(t)}.`);const e="channelsFirst"===this.dataFormat?1:3;if(null==t[e]||t[e]<0)throw new ci(`The channel dimension of the inputs to DepthwiseConv2D should be defined, but is not (${t[e]}).`);const n=t[e],s=[this.kernelSize[0],this.kernelSize[1],n,this.depthMultiplier];this.depthwiseKernel=this.addWeight("depthwise_kernel",s,null,this.depthwiseInitializer,this.depthwiseRegularizer,!0,this.depthwiseConstraint),this.useBias?this.bias=this.addWeight("bias",[n*this.depthMultiplier],null,this.biasInitializer,this.biasRegularizer,!0,this.biasConstraint):this.bias=null,this.built=!0}call(t,n){return e.tidy(()=>{let n=function(t,n,s=[1,1],i="valid",r,a){return e.tidy(()=>{null==r&&(r="channelsLast"),Hi(r);let o=cl(t,r);if(4!==t.rank)throw new ci("Input for depthwiseConv2d is required to be 4-D, but is instead "+t.rank+"-D");if(4!==n.rank)throw new ci("depthwiseKernel is required to be 4-D, but is instead "+n.rank+"-D");return o=e.depthwiseConv2d(o,n,s,"same"===i?"same":"valid","NHWC",a),"channelsFirst"===r&&(o=e.transpose(o,[0,3,1,2])),o})}(t=Qr(t),this.depthwiseKernel.read(),this.strides,this.padding,this.dataFormat,null);return this.useBias&&(n=xr(n,this.bias.read(),this.dataFormat)),null!=this.activation&&(n=this.activation.apply(n)),n})}computeOutputShape(t){t=ta(t);const e="channelsFirst"===this.dataFormat?t[2]:t[1],n="channelsFirst"===this.dataFormat?t[3]:t[2],s="channelsFirst"===this.dataFormat?t[1]*this.depthMultiplier:t[3]*this.depthMultiplier,i=ul(e,this.kernelSize[0],this.padding,this.strides[0]),r=ul(n,this.kernelSize[1],this.padding,this.strides[1]);return"channelsFirst"===this.dataFormat?[t[0],s,i,r]:[t[0],i,r,s]}getConfig(){const t=super.getConfig();return t.depthMultiplier=this.depthMultiplier,t.depthwiseInitializer=Vr(this.depthwiseInitializer),t.depthwiseRegularizer=Qo(this.depthwiseRegularizer),t.depthwiseConstraint=Oi(this.depthwiseRegularizer),t}}function Nl(t,e,n,s){if(Array.isArray(t)){if(null!=e||null!=n)throw new ci("When inputs is an array, neither initialState or constants should be provided");null!=s&&(n=t.slice(t.length-s,t.length),t=t.slice(0,t.length-s)),t.length>1&&(e=t.slice(1,t.length)),t=t[0]}function i(t){return null==t||Array.isArray(t)?t:[t]}return{inputs:t,initialState:e=i(e),constants:n=i(n)}}function zl(t,n,s,i=!1,r,a,o=!1,l=!1){return e.tidy(()=>{const u=n.shape.length;if(u<3)throw new ci(`Input should be at least 3D, but is ${u}D.`);const h=[1,0].concat(or(2,u));if(n=e.transpose(n,h),null!=a)throw new pi("The rnn() functoin of the deeplearn.js backend does not support constants yet.");o&&console.warn("Backend rnn(): the unroll = true option is not applicable to the imperative deeplearn.js backend."),null!=r&&((r=r.asType("bool").asType("float32")).rank===u-1&&(r=e.expandDims(r,-1)),r=e.transpose(r,h)),i&&(n=e.reverse(n,0),null!=r&&(r=e.reverse(r,0)));const c=[];let p,d=s;const f=n.shape[0],g=e.unstack(n);let m,y;null!=r&&(m=e.unstack(r));for(let n=0;n<f;++n){const s=g[n],i=e.tidy(()=>t(s,d));if(null==r)p=i[0],d=i[1];else{const t=e.tidy(()=>{const t=m[n],s=e.onesLike(t).sub(t);return{output:i[0].mul(t).add(d[0].mul(s)),newStates:d.map((e,n)=>i[1][n].mul(t).add(e.mul(s)))}});p=t.output,d=t.newStates}l&&c.push(p)}if(l){const t=1;y=e.stack(c,t)}return[p,y,d]})}Il.className="DepthwiseConv2D",e.serialization.registerClass(Il);class Al extends ha{constructor(t){let e;if(super(t),null==t.cell)throw new ci("cell property is missing for the constructor of RNN.");if(e=Array.isArray(t.cell)?new _l({cells:t.cell}):t.cell,null==e.stateSize)throw new ci("The RNN cell should have an attribute `stateSize` (tuple of integers, one integer per RNN state).");this.cell=e,this.returnSequences=null!=t.returnSequences&&t.returnSequences,this.returnState=null!=t.returnState&&t.returnState,this.goBackwards=null!=t.goBackwards&&t.goBackwards,this._stateful=null!=t.stateful&&t.stateful,this.unroll=null!=t.unroll&&t.unroll,this.supportsMasking=!0,this.inputSpec=[new ra({ndim:3})],this.stateSpec=null,this.states_=null,this.numConstants=null,this.keptStates=[]}getStates(){if(null==this.states_){return or(0,Array.isArray(this.cell.stateSize)?this.cell.stateSize.length:1).map(t=>null)}return this.states_}setStates(t){this.states_=t}computeOutputShape(t){Xr(t)&&(t=t[0]),t=t;let e=this.cell.stateSize;Array.isArray(e)||(e=[e]);const n=e[0];let s;if(s=this.returnSequences?[t[0],t[1],n]:[t[0],n],this.returnState){const n=[];for(const s of e)n.push([t[0],s]);return[s].concat(n)}return s}computeMask(t,n){return e.tidy(()=>{Array.isArray(n)&&(n=n[0]);const t=this.returnSequences?n:null;if(this.returnState){const e=this.states.map(t=>null);return[t].concat(e)}return t})}get states(){if(null==this.states_){const t=Array.isArray(this.cell.stateSize)?this.cell.stateSize.length:1,e=[];for(let n=0;n<t;++n)e.push(null);return e}return this.states_}set states(t){this.states_=t}build(t){if(null!=this.numConstants)throw new pi("Constants support is not implemented in RNN yet.");Xr(t)&&(t=t[0]),t=t;const n=this.stateful?t[0]:null,s=t.slice(2);this.inputSpec[0]=new ra({shape:[n,null,...s]});const i=[t[0]].concat(t.slice(2));let r;if(this.cell.build(i),r=Array.isArray(this.cell.stateSize)?this.cell.stateSize:[this.cell.stateSize],null!=this.stateSpec){if(!e.util.arraysEqual(this.stateSpec.map(t=>t.shape[t.shape.length-1]),r))throw new ci(`An initialState was passed that is not compatible with cell.stateSize. Received stateSpec=${this.stateSpec}; However cell.stateSize is `+this.cell.stateSize)}else this.stateSpec=r.map(t=>new ra({shape:[null,t]}));this.stateful&&this.resetStates()}resetStates(t,n=!1){e.tidy(()=>{if(!this.stateful)throw new ui("Cannot call resetStates() on an RNN Layer that is not stateful.");const s=this.inputSpec[0].shape[0];if(null==s)throw new ci("If an RNN is stateful, it needs to know its batch size. Specify the batch size of your input tensors: \n- If using a Sequential model, specify the batch size by passing a `batchInputShape` option to your first layer.\n- If using the functional API, specify the batch size by passing a `batchShape` option to your Input layer.");if(null==this.states_)Array.isArray(this.cell.stateSize)?this.states_=this.cell.stateSize.map(t=>e.zeros([s,t])):this.states_=[e.zeros([s,this.cell.stateSize])];else if(null==t)e.dispose(this.states_),null!=this.keptStates&&(e.dispose(this.keptStates),this.keptStates=[]),Array.isArray(this.cell.stateSize)?this.states_=this.cell.stateSize.map(t=>e.zeros([s,t])):this.states_[0]=e.zeros([s,this.cell.stateSize]);else{if(Array.isArray(t)||(t=[t]),t.length!==this.states_.length)throw new ci(`Layer ${this.name} expects ${this.states_.length} state(s), but it received ${t.length} state value(s). Input received: `+t);!0===n?this.keptStates.push(this.states_.slice()):e.dispose(this.states_);for(let n=0;n<this.states_.length;++n){const i=t[n],r=Array.isArray(this.cell.stateSize)?this.cell.stateSize[n]:this.cell.stateSize,a=[s,r];if(!e.util.arraysEqual(i.shape,a))throw new ci(`State ${n} is incompatible with layer ${this.name}: expected shape=${a}, received shape=${i.shape}`);this.states_[n]=i}}this.states_=this.states_.map(t=>e.keep(t.clone()))})}apply(t,e){let n=null==e?null:e.initialState,s=null==e?null:e.constants;null==e&&(e={});const i=Nl(t,n,s,this.numConstants);t=i.inputs,n=i.initialState,s=i.constants;let r=[],a=[];if(null!=n){e.initialState=n,r=r.concat(n),this.stateSpec=[];for(const t of n)this.stateSpec.push(new ra({shape:t.shape}));a=a.concat(this.stateSpec)}null!=s&&(e.constants=s,r=r.concat(s),this.numConstants=s.length);if(r[0]instanceof aa){const n=[t].concat(r),s=this.inputSpec.concat(a),i=this.inputSpec;this.inputSpec=s;const o=super.apply(n,e);return this.inputSpec=i,o}return super.apply(t,e)}call(t,n){return e.tidy(()=>{const e=null==n?null:n.mask,s=null==n?null:n.training;let i=null==n?null:n.initialState;t=Qr(t),null==i&&(i=this.stateful?this.states_:this.getInitialState(t));const r=Array.isArray(this.cell.stateSize)?this.cell.stateSize.length:1;if(i.length!==r)throw new ci(`RNN Layer has ${r} state(s) but was passed `+i.length+" initial state(s).");this.unroll&&console.warn("Ignoring unroll = true for RNN layer, due to imperative backend.");const a={training:s},o=zl((t,e)=>{const n=this.cell.call([t].concat(e),a);return[n[0],n.slice(1)]},t,i,this.goBackwards,e,null,this.unroll,this.returnSequences),l=o[0],u=o[1],h=o[2];this.stateful&&this.resetStates(h,s);const c=this.returnSequences?u:l;return this.returnState?[c].concat(h):c})}getInitialState(t){return e.tidy(()=>{let n=e.zeros(t.shape);return n=e.sum(n,[1,2]),n=ur(n),Array.isArray(this.cell.stateSize)?this.cell.stateSize.map(t=>t>1?gr(n,[1,t]):n):this.cell.stateSize>1?[gr(n,[1,this.cell.stateSize])]:[n]})}get trainableWeights(){return this.trainable?this.cell.trainableWeights:[]}get nonTrainableWeights(){return this.trainable?this.cell.nonTrainableWeights:this.cell.weights}setFastWeightInitDuringBuild(t){super.setFastWeightInitDuringBuild(t),null!=this.cell&&this.cell.setFastWeightInitDuringBuild(t)}getConfig(){const t=super.getConfig(),e={returnSequences:this.returnSequences,returnState:this.returnState,goBackwards:this.goBackwards,stateful:this.stateful,unroll:this.unroll};null!=this.numConstants&&(e.numConstants=this.numConstants);const n=this.cell.getConfig();return this.getClassName()===Al.className&&(e.cell={className:this.cell.getClassName(),config:n}),Object.assign({},n,t,e)}static fromConfig(t,e,n={}){const s=Ia(e.cell,n);return new t(Object.assign(e,{cell:s}))}}Al.className="RNN",e.serialization.registerClass(Al);class Cl extends ha{}class Dl extends Cl{constructor(t){super(t),this.DEFAULT_ACTIVATION="tanh",this.DEFAULT_KERNEL_INITIALIZER="glorotNormal",this.DEFAULT_RECURRENT_INITIALIZER="orthogonal",this.DEFAULT_BIAS_INITIALIZER="zeros",this.units=t.units,Di(this.units,"units"),this.activation=Ho(null==t.activation?this.DEFAULT_ACTIVATION:t.activation),this.useBias=null==t.useBias||t.useBias,this.kernelInitializer=jr(t.kernelInitializer||this.DEFAULT_KERNEL_INITIALIZER),this.recurrentInitializer=jr(t.recurrentInitializer||this.DEFAULT_RECURRENT_INITIALIZER),this.biasInitializer=jr(t.biasInitializer||this.DEFAULT_BIAS_INITIALIZER),this.kernelRegularizer=el(t.kernelRegularizer),this.recurrentRegularizer=el(t.recurrentRegularizer),this.biasRegularizer=el(t.biasRegularizer),this.kernelConstraint=Pi(t.kernelConstraint),this.recurrentConstraint=Pi(t.recurrentConstraint),this.biasConstraint=Pi(t.biasConstraint),this.dropout=rr([1,ar([0,null==t.dropout?0:t.dropout])]),this.recurrentDropout=rr([1,ar([0,null==t.recurrentDropout?0:t.recurrentDropout])]),this.stateSize=this.units,this.dropoutMask=null,this.recurrentDropoutMask=null}build(t){t=ta(t),this.kernel=this.addWeight("kernel",[t[t.length-1],this.units],null,this.kernelInitializer,this.kernelRegularizer,!0,this.kernelConstraint),this.recurrentKernel=this.addWeight("recurrent_kernel",[this.units,this.units],null,this.recurrentInitializer,this.recurrentRegularizer,!0,this.recurrentConstraint),this.useBias?this.bias=this.addWeight("bias",[this.units],null,this.biasInitializer,this.biasRegularizer,!0,this.biasConstraint):this.bias=null,this.built=!0}call(t,n){return e.tidy(()=>{if(2!==(t=t).length)throw new ci(`SimpleRNNCell expects 2 input Tensors, got ${t.length}.`);let s=t[1];t=t[0];const i=null!=n.training&&n.training;let r;0<this.dropout&&this.dropout<1&&null==this.dropoutMask&&(this.dropoutMask=Rl({ones:()=>e.onesLike(t),rate:this.dropout,training:i})),0<this.recurrentDropout&&this.recurrentDropout<1&&null==this.recurrentDropoutMask&&(this.recurrentDropoutMask=Rl({ones:()=>e.onesLike(s),rate:this.recurrentDropout,training:i}));const a=this.dropoutMask,o=this.recurrentDropoutMask;r=yr(null!=a?e.mul(t,a):t,this.kernel.read()),null!=this.bias&&(r=xr(r,this.bias.read())),null!=o&&(s=e.mul(s,o));let l=e.add(r,yr(s,this.recurrentKernel.read()));return null!=this.activation&&(l=this.activation.apply(l)),[l,l]})}getConfig(){const t=super.getConfig(),e={units:this.units,activation:qo(this.activation),useBias:this.useBias,kernelInitializer:Vr(this.kernelInitializer),recurrentInitializer:Vr(this.recurrentInitializer),biasInitializer:Vr(this.biasInitializer),kernelRegularizer:Qo(this.kernelRegularizer),recurrentRegularizer:Qo(this.recurrentRegularizer),biasRegularizer:Qo(this.biasRegularizer),activityRegularizer:Qo(this.activityRegularizer),kernelConstraint:Oi(this.kernelConstraint),recurrentConstraint:Oi(this.recurrentConstraint),biasConstraint:Oi(this.biasConstraint),dropout:this.dropout,recurrentDropout:this.recurrentDropout};return Object.assign({},t,e)}}Dl.className="SimpleRNNCell",e.serialization.registerClass(Dl);class Tl extends Al{constructor(t){t.cell=new Dl(t),super(t)}call(t,n){return e.tidy(()=>{null!=this.cell.dropoutMask&&(e.dispose(this.cell.dropoutMask),this.cell.dropoutMask=null),null!=this.cell.recurrentDropoutMask&&(e.dispose(this.cell.recurrentDropoutMask),this.cell.recurrentDropoutMask=null);const s=null==n?null:n.mask,i=null==n?null:n.training,r=null==n?null:n.initialState;return super.call(t,{mask:s,training:i,initialState:r})})}static fromConfig(t,e){return new t(e)}}Tl.className="SimpleRNN",e.serialization.registerClass(Tl);class El extends Cl{constructor(t){if(super(t),this.DEFAULT_ACTIVATION="tanh",this.DEFAULT_RECURRENT_ACTIVATION="hardSigmoid",this.DEFAULT_KERNEL_INITIALIZER="glorotNormal",this.DEFAULT_RECURRENT_INITIALIZER="orthogonal",this.DEFAULT_BIAS_INITIALIZER="zeros",t.resetAfter)throw new ci("GRUCell does not support reset_after parameter set to true.");this.units=t.units,Di(this.units,"units"),this.activation=Ho(void 0===t.activation?this.DEFAULT_ACTIVATION:t.activation),this.recurrentActivation=Ho(void 0===t.recurrentActivation?this.DEFAULT_RECURRENT_ACTIVATION:t.recurrentActivation),this.useBias=null==t.useBias||t.useBias,this.kernelInitializer=jr(t.kernelInitializer||this.DEFAULT_KERNEL_INITIALIZER),this.recurrentInitializer=jr(t.recurrentInitializer||this.DEFAULT_RECURRENT_INITIALIZER),this.biasInitializer=jr(t.biasInitializer||this.DEFAULT_BIAS_INITIALIZER),this.kernelRegularizer=el(t.kernelRegularizer),this.recurrentRegularizer=el(t.recurrentRegularizer),this.biasRegularizer=el(t.biasRegularizer),this.kernelConstraint=Pi(t.kernelConstraint),this.recurrentConstraint=Pi(t.recurrentConstraint),this.biasConstraint=Pi(t.biasConstraint),this.dropout=rr([1,ar([0,null==t.dropout?0:t.dropout])]),this.recurrentDropout=rr([1,ar([0,null==t.recurrentDropout?0:t.recurrentDropout])]),this.implementation=t.implementation,this.stateSize=this.units,this.dropoutMask=null,this.recurrentDropoutMask=null}build(t){const e=(t=ta(t))[t.length-1];this.kernel=this.addWeight("kernel",[e,3*this.units],null,this.kernelInitializer,this.kernelRegularizer,!0,this.kernelConstraint),this.recurrentKernel=this.addWeight("recurrent_kernel",[this.units,3*this.units],null,this.recurrentInitializer,this.recurrentRegularizer,!0,this.recurrentConstraint),this.useBias?this.bias=this.addWeight("bias",[3*this.units],null,this.biasInitializer,this.biasRegularizer,!0,this.biasConstraint):this.bias=null,this.built=!0}call(t,n){return e.tidy(()=>{if(2!==(t=t).length)throw new ci("GRUCell expects 2 input Tensors (inputs, h, c), got "+t.length+".");const s=null!=n.training&&n.training;let i=t[1];t=t[0],0<this.dropout&&this.dropout<1&&null==this.dropoutMask&&(this.dropoutMask=Rl({ones:()=>e.onesLike(t),rate:this.dropout,training:s,count:3})),0<this.recurrentDropout&&this.recurrentDropout<1&&null==this.recurrentDropoutMask&&(this.recurrentDropoutMask=Rl({ones:()=>e.onesLike(i),rate:this.recurrentDropout,training:s,count:3}));const r=this.dropoutMask,a=this.recurrentDropoutMask;let o,l,u;0<this.dropout&&this.dropout<1&&(t=e.mul(t,r[0]));let h=yr(t,this.kernel.read());this.useBias&&(h=xr(h,this.bias.read())),0<this.recurrentDropout&&this.recurrentDropout<1&&(i=e.mul(i,a[0]));const c=this.recurrentKernel.read(),[p,d]=e.split(c,[2*this.units,this.units],c.rank-1),f=yr(i,p),[g,m,y]=e.split(h,3,h.rank-1),[b,w]=e.split(f,2,f.rank-1);o=this.recurrentActivation.apply(e.add(g,b)),l=this.recurrentActivation.apply(e.add(m,w));const k=yr(e.mul(l,i),d);u=this.activation.apply(e.add(y,k));const x=e.add(e.mul(o,i),e.mul(e.add(1,e.neg(o)),u));return[x,x]})}getConfig(){const t=super.getConfig(),e={units:this.units,activation:qo(this.activation),recurrentActivation:qo(this.recurrentActivation),useBias:this.useBias,kernelInitializer:Vr(this.kernelInitializer),recurrentInitializer:Vr(this.recurrentInitializer),biasInitializer:Vr(this.biasInitializer),kernelRegularizer:Qo(this.kernelRegularizer),recurrentRegularizer:Qo(this.recurrentRegularizer),biasRegularizer:Qo(this.biasRegularizer),activityRegularizer:Qo(this.activityRegularizer),kernelConstraint:Oi(this.kernelConstraint),recurrentConstraint:Oi(this.recurrentConstraint),biasConstraint:Oi(this.biasConstraint),dropout:this.dropout,recurrentDropout:this.recurrentDropout,implementation:this.implementation,resetAfter:!1};return Object.assign({},t,e)}}El.className="GRUCell",e.serialization.registerClass(El);class Fl extends Al{constructor(t){0===t.implementation&&console.warn("`implementation=0` has been deprecated, and now defaults to `implementation=1`. Please update your layer call."),t.cell=new El(t),super(t)}call(t,n){return e.tidy(()=>{null!=this.cell.dropoutMask&&(e.dispose(this.cell.dropoutMask),this.cell.dropoutMask=null),null!=this.cell.recurrentDropoutMask&&(e.dispose(this.cell.recurrentDropoutMask),this.cell.recurrentDropoutMask=null);const s=null==n?null:n.mask,i=null==n?null:n.training,r=null==n?null:n.initialState;return super.call(t,{mask:s,training:i,initialState:r})})}static fromConfig(t,e){return 0===e.implmentation&&(e.implementation=1),new t(e)}}Fl.className="GRU",e.serialization.registerClass(Fl);class Ll extends Cl{constructor(t){super(t),this.DEFAULT_ACTIVATION="tanh",this.DEFAULT_RECURRENT_ACTIVATION="hardSigmoid",this.DEFAULT_KERNEL_INITIALIZER="glorotNormal",this.DEFAULT_RECURRENT_INITIALIZER="orthogonal",this.DEFAULT_BIAS_INITIALIZER="zeros",this.units=t.units,Di(this.units,"units"),this.activation=Ho(void 0===t.activation?this.DEFAULT_ACTIVATION:t.activation),this.recurrentActivation=Ho(void 0===t.recurrentActivation?this.DEFAULT_RECURRENT_ACTIVATION:t.recurrentActivation),this.useBias=null==t.useBias||t.useBias,this.kernelInitializer=jr(t.kernelInitializer||this.DEFAULT_KERNEL_INITIALIZER),this.recurrentInitializer=jr(t.recurrentInitializer||this.DEFAULT_RECURRENT_INITIALIZER),this.biasInitializer=jr(t.biasInitializer||this.DEFAULT_BIAS_INITIALIZER),this.unitForgetBias=t.unitForgetBias,this.kernelRegularizer=el(t.kernelRegularizer),this.recurrentRegularizer=el(t.recurrentRegularizer),this.biasRegularizer=el(t.biasRegularizer),this.kernelConstraint=Pi(t.kernelConstraint),this.recurrentConstraint=Pi(t.recurrentConstraint),this.biasConstraint=Pi(t.biasConstraint),this.dropout=rr([1,ar([0,null==t.dropout?0:t.dropout])]),this.recurrentDropout=rr([1,ar([0,null==t.recurrentDropout?0:t.recurrentDropout])]),this.implementation=t.implementation,this.stateSize=[this.units,this.units],this.dropoutMask=null,this.recurrentDropoutMask=null}build(t){var e;const n=(t=ta(t))[t.length-1];let s;if(this.kernel=this.addWeight("kernel",[n,4*this.units],null,this.kernelInitializer,this.kernelRegularizer,!0,this.kernelConstraint),this.recurrentKernel=this.addWeight("recurrent_kernel",[this.units,4*this.units],null,this.recurrentInitializer,this.recurrentRegularizer,!0,this.recurrentConstraint),this.useBias){if(this.unitForgetBias){const t=this.biasInitializer,n=this.units;s=new((e=class extends zr{apply(e,s){const i=t.apply([n]),r=(new Cr).apply([n]),a=t.apply([2*n]);return fr(fr(i,r),a)}}).className="CustomInit",e)}else s=this.biasInitializer;this.bias=this.addWeight("bias",[4*this.units],null,s,this.biasRegularizer,!0,this.biasConstraint)}else this.bias=null;this.built=!0}call(t,n){return e.tidy(()=>{const s=null!=n.training&&n.training;if(3!==(t=t).length)throw new ci("LSTMCell expects 3 input Tensors (inputs, h, c), got "+t.length+".");let i=t[1];const r=t[2];t=t[0],0<this.dropout&&this.dropout<1&&null==this.dropoutMask&&(this.dropoutMask=Rl({ones:()=>e.onesLike(t),rate:this.dropout,training:s,count:4})),0<this.recurrentDropout&&this.recurrentDropout<1&&null==this.recurrentDropoutMask&&(this.recurrentDropoutMask=Rl({ones:()=>e.onesLike(i),rate:this.recurrentDropout,training:s,count:4}));const a=this.dropoutMask,o=this.recurrentDropoutMask;let l,u,h,c;0<this.dropout&&this.dropout<1&&(t=e.mul(t,a[0]));let p=yr(t,this.kernel.read());0<this.recurrentDropout&&this.recurrentDropout<1&&(i=e.mul(i,o[0])),p=e.add(p,yr(i,this.recurrentKernel.read())),this.useBias&&(p=xr(p,this.bias.read()));const[d,f,g,m]=e.split(p,4,p.rank-1);l=this.recurrentActivation.apply(d),u=this.recurrentActivation.apply(f),h=e.add(e.mul(u,r),e.mul(l,this.activation.apply(g))),c=this.recurrentActivation.apply(m);const y=e.mul(c,this.activation.apply(h));return[y,y,h]})}getConfig(){const t=super.getConfig(),e={units:this.units,activation:qo(this.activation),recurrentActivation:qo(this.recurrentActivation),useBias:this.useBias,kernelInitializer:Vr(this.kernelInitializer),recurrentInitializer:Vr(this.recurrentInitializer),biasInitializer:Vr(this.biasInitializer),unitForgetBias:this.unitForgetBias,kernelRegularizer:Qo(this.kernelRegularizer),recurrentRegularizer:Qo(this.recurrentRegularizer),biasRegularizer:Qo(this.biasRegularizer),activityRegularizer:Qo(this.activityRegularizer),kernelConstraint:Oi(this.kernelConstraint),recurrentConstraint:Oi(this.recurrentConstraint),biasConstraint:Oi(this.biasConstraint),dropout:this.dropout,recurrentDropout:this.recurrentDropout,implementation:this.implementation};return Object.assign({},t,e)}}Ll.className="LSTMCell",e.serialization.registerClass(Ll);class $l extends Al{constructor(t){0===t.implementation&&console.warn("`implementation=0` has been deprecated, and now defaults to `implementation=1`. Please update your layer call."),t.cell=new Ll(t),super(t)}call(t,n){return e.tidy(()=>{null!=this.cell.dropoutMask&&(e.dispose(this.cell.dropoutMask),this.cell.dropoutMask=null),null!=this.cell.recurrentDropoutMask&&(e.dispose(this.cell.recurrentDropoutMask),this.cell.recurrentDropoutMask=null);const s=null==n?null:n.mask,i=null==n?null:n.training,r=null==n?null:n.initialState;return super.call(t,{mask:s,training:i,initialState:r})})}static fromConfig(t,e){return 0===e.implmentation&&(e.implementation=1),new t(e)}}$l.className="LSTM",e.serialization.registerClass($l);class _l extends Cl{constructor(t){super(t),this.cells=t.cells}get stateSize(){const t=[];for(const e of this.cells.slice().reverse())Array.isArray(e.stateSize)?t.push(...e.stateSize):t.push(e.stateSize);return t}call(t,n){return e.tidy(()=>{let e=(t=t).slice(1);const s=[];for(const t of this.cells.slice().reverse())Array.isArray(t.stateSize)?s.push(e.splice(0,t.stateSize.length)):s.push(e.splice(0,1));s.reverse();const i=[];let r;for(let a=0;a<this.cells.length;++a){const o=this.cells[a];e=s[a],r=0===a?[t[0]].concat(e):[r[0]].concat(e),r=o.call(r,n),i.push(r.slice(1))}e=[];for(const t of i.slice().reverse())e.push(...t);return[r[0]].concat(e)})}build(t){let e;Xr(t)&&(t=t[0]),t=t,this.cells.forEach((n,s)=>{Yi("RNNCell_"+s,()=>{n.build(t),e=Array.isArray(n.stateSize)?n.stateSize[0]:n.stateSize,t=[t[0],e]})}),this.built=!0}getConfig(){const t=super.getConfig(),e={cells:this.cells.map(t=>({className:t.getClassName(),config:t.getConfig()}))};return Object.assign({},t,e)}static fromConfig(t,e,n={}){const s=[];for(const t of e.cells)s.push(Ia(t,n));return new t({cells:s})}get trainableWeights(){if(!this.trainable)return[];const t=[];for(const e of this.cells)t.push(...e.trainableWeights);return t}get nonTrainableWeights(){const t=[];for(const e of this.cells)t.push(...e.nonTrainableWeights);if(!this.trainable){const e=[];for(const t of this.cells)e.push(...t.trainableWeights);return e.concat(t)}return t}getWeights(){const t=[];for(const e of this.cells)t.push(...e.weights);return sa(t)}setWeights(t){const e=[];for(const n of this.cells){const s=n.weights.length,i=t.splice(s);for(let t=0;t<n.weights.length;++t)e.push([n.weights[t],i[t]])}ia(e)}}function Rl(t){const{ones:n,rate:s,training:i=!1,count:r=1}=t,a=()=>vr(n(),s),o=()=>Sr(a,n,i);if(!r||r<=1)return e.keep(o().clone());return Array(r).fill(void 0).map(o).map(t=>e.keep(t.clone()))}_l.className="StackedRNNCells",e.serialization.registerClass(_l);class Ml extends Al{constructor(t){if(t.unroll)throw new pi("Unrolling is not possible with convolutional RNNs.");if(Array.isArray(t.cell))throw new pi("It is not possible at the moment to stack convolutional cells.");super(t),this.inputSpec=[new ra({ndim:5})]}call(t,n){return e.tidy(()=>{if(null!=this.cell.dropoutMask&&(e.dispose(this.cell.dropoutMask),this.cell.dropoutMask=null),null!=this.cell.recurrentDropoutMask&&(e.dispose(this.cell.recurrentDropoutMask),this.cell.recurrentDropoutMask=null),n&&n.constants)throw new ci("ConvRNN2D cell does not support constants");const s=null==n?null:n.mask,i=null==n?null:n.training,r=null==n?null:n.initialState;return super.call(t,{mask:s,training:i,initialState:r})})}computeOutputShape(t){let e=this.computeSingleOutputShape(t);return this.returnSequences||(e=[e[0],...e.slice(2)]),this.returnState&&(e=[e,...Array(2).fill([t[0],...e.slice(-3)])]),e}getInitialState(t){return e.tidy(()=>{const{stateSize:n}=this.cell,s=t.shape,i=this.computeSingleOutputShape(s),r=[i[0],...i.slice(2)],a=e.zeros(r);return Array.isArray(n)?Array(n.length).fill(a):[a]})}resetStates(t,n=!1){e.tidy(()=>{if(!this.stateful)throw new ui("Cannot call resetStates() on an RNN Layer that is not stateful.");const s=this.inputSpec[0].shape,i=this.computeSingleOutputShape(s),r=[i[0],...i.slice(2)];if(null==s[0])throw new ci("If an RNN is stateful, it needs to know its batch size. Specify the batch size of your input tensors: \n- If using a Sequential model, specify the batch size by passing a `batchInputShape` option to your first layer.\n- If using the functional API, specify the batch size by passing a `batchShape` option to your Input layer.");if(null==this.getStates())Array.isArray(this.cell.stateSize)?this.states_=this.cell.stateSize.map(()=>e.zeros(r)):this.states_=[e.zeros(r)];else if(null==t)e.dispose(this.states_),null!=this.keptStates&&(e.dispose(this.keptStates),this.keptStates=[]),Array.isArray(this.cell.stateSize)?this.states_=this.cell.stateSize.map(()=>e.zeros(r)):this.states_[0]=e.zeros(r);else{if(Array.isArray(t)||(t=[t]),t.length!==this.states_.length)throw new ci(`Layer ${this.name} expects ${this.states_.length} state(s), but it received ${t.length} state value(s). Input received: `+t);n?this.keptStates.push(this.states_.slice()):e.dispose(this.states_);for(let n=0;n<this.states_.length;++n){const s=t[n],i=r;if(!e.util.arraysEqual(s.shape,i))throw new ci(`State ${n} is incompatible with layer ${this.name}: expected shape=${i}, received shape=${s.shape}`);this.states_[n]=s}}this.states_=this.states_.map(t=>e.keep(t.clone()))})}computeSingleOutputShape(t){const{dataFormat:e,filters:n,kernelSize:s,padding:i,strides:r,dilationRate:a}=this.cell,o="channelsFirst"===e,l=t[o?3:2],u=t[o?4:3],h=ul(l,s[0],i,r[0],a[0]),c=ul(u,s[1],i,r[1],a[1]);return[...t.slice(0,2),...o?[n,h,c]:[h,c,n]]}}Ml.className="ConvRNN2D";class Ol extends Ll{constructor(t){const{filters:e,kernelSize:n,strides:s,padding:i,dataFormat:r,dilationRate:a}=t;super(Object.assign({},t,{units:e})),this.filters=e,Di(this.filters,"filters"),this.kernelSize=ll(n,2,"kernelSize"),this.kernelSize.forEach(t=>Di(t,"kernelSize")),this.strides=ll(s||1,2,"strides"),this.strides.forEach(t=>Di(t,"strides")),this.padding=i||"valid",Ji(this.padding),this.dataFormat=r||"channelsLast",Hi(this.dataFormat),this.dilationRate=ll(a||1,2,"dilationRate"),this.dilationRate.forEach(t=>Di(t,"dilationRate"))}build(t){var n;t=ta(t);const s="channelsFirst"===this.dataFormat?1:t.length-1;if(null==t[s])throw new ci("The channel dimension of the input should be defined. Found "+t[s]);const i=t[s],r=this.kernelSize.concat([i,4*this.filters]);this.kernel=this.addWeight("kernel",r,null,this.kernelInitializer,this.kernelRegularizer,!0,this.kernelConstraint);const a=this.kernelSize.concat([this.filters,4*this.filters]);if(this.recurrentKernel=this.addWeight("recurrent_kernel",a,null,this.recurrentInitializer,this.recurrentRegularizer,!0,this.recurrentConstraint),this.useBias){let t;if(this.unitForgetBias){const s=this.biasInitializer,i=this.filters;t=new((n=class extends zr{apply(t,n){return dr([s.apply([i]),e.ones([i]),s.apply([2*i])])}}).className="CustomInit",n)}else t=this.biasInitializer;this.bias=this.addWeight("bias",[4*this.filters],null,t,this.biasRegularizer,!0,this.biasConstraint)}this.built=!0}call(t,n){return e.tidy(()=>{if(3!==t.length)throw new ci("ConvLSTM2DCell expects 3 input Tensors (inputs, h, c), got "+t.length+".");const s=n.training||!1,i=t[0],r=t[1],a=t[2];0<this.dropout&&this.dropout<1&&null==this.dropoutMask&&(this.dropoutMask=Rl({ones:()=>e.onesLike(i),rate:this.dropout,training:s,count:4}));const o=this.dropoutMask,l=(t,n,s)=>n&&n[s]?e.mul(n[s],t):t;let u=l(i,o,0),h=l(i,o,1),c=l(i,o,2),p=l(i,o,3);0<this.recurrentDropout&&this.recurrentDropout<1&&null==this.recurrentDropoutMask&&(this.recurrentDropoutMask=Rl({ones:()=>e.onesLike(r),rate:this.recurrentDropout,training:s,count:4}));const d=this.recurrentDropoutMask;let f=l(r,d,0),g=l(r,d,1),m=l(r,d,2),y=l(r,d,3);const[b,w,k,x]=e.split(this.kernel.read(),4,3),[v,S,I,N]=this.useBias?e.split(this.bias.read(),4):[null,null,null,null];u=this.inputConv(u,b,v,this.padding),h=this.inputConv(h,w,S,this.padding),c=this.inputConv(c,k,I,this.padding),p=this.inputConv(p,x,N,this.padding);const[z,A,C,D]=e.split(this.recurrentKernel.read(),4,3);f=this.recurrentConv(f,z),g=this.recurrentConv(g,A),m=this.recurrentConv(m,C),y=this.recurrentConv(y,D);const T=this.recurrentActivation.apply(e.add(u,f)),E=this.recurrentActivation.apply(e.add(h,g)),F=e.add(e.mul(E,a),e.mul(T,this.activation.apply(e.add(c,m)))),L=e.mul(this.recurrentActivation.apply(e.add(p,y)),this.activation.apply(F));return[L,L,F]})}getConfig(){const t=function(t,e){var n={};for(var s in t)Object.prototype.hasOwnProperty.call(t,s)&&e.indexOf(s)<0&&(n[s]=t[s]);if(null!=t&&"function"==typeof Object.getOwnPropertySymbols){var i=0;for(s=Object.getOwnPropertySymbols(t);i<s.length;i++)e.indexOf(s[i])<0&&Object.prototype.propertyIsEnumerable.call(t,s[i])&&(n[s[i]]=t[s[i]])}return n}(super.getConfig(),["units"]),e={filters:this.filters,kernelSize:this.kernelSize,padding:this.padding,dataFormat:this.dataFormat,dilationRate:this.dilationRate,strides:this.strides};return Object.assign({},t,e)}inputConv(t,n,s,i){const r=e.conv2d(t,n,this.strides,i||"valid","channelsFirst"===this.dataFormat?"NCHW":"NHWC",this.dilationRate);return s?xr(r,s,this.dataFormat):r}recurrentConv(t,n){return e.conv2d(t,n,1,"same","channelsFirst"===this.dataFormat?"NCHW":"NHWC")}}Ol.className="ConvLSTM2DCell",e.serialization.registerClass(Ol);class Bl extends Ml{constructor(t){const e=new Ol(t);super(Object.assign({},t,{cell:e}))}static fromConfig(t,e){return new t(e)}}Bl.className="ConvLSTM2D",e.serialization.registerClass(Bl);class Pl extends ha{constructor(t){super(t),this.rate=Math.max(Math.min(t.rate,1),0),this.noiseShape=t.noiseShape,this.seed=t.seed,this.supportsMasking=!0}getNoiseShape(t){if(null==this.noiseShape)return this.noiseShape;const e=t.shape,n=[];for(let t=0;t<this.noiseShape.length;++t)n.push(null==this.noiseShape[t]?e[t]:this.noiseShape[t]);return n}call(t,n){return e.tidy(()=>{this.invokeCallHook(t,n);const e=Qr(t);if(0<this.rate&&this.rate<1){const t=null!=n.training&&n.training,s=this.getNoiseShape(e);return Sr(()=>vr(e,this.rate,s,this.seed),()=>e,t)}return t})}getConfig(){const t={rate:this.rate,noiseShape:this.noiseShape,seed:this.seed},e=super.getConfig();return Object.assign(t,e),t}dispose(){return super.dispose()}}Pl.className="Dropout",e.serialization.registerClass(Pl);class Wl extends Pl{constructor(t){super(t),this.inputSpec=[{ndim:3}]}getNoiseShape(t){const e=t.shape;return[e[0],1,e[2]]}}Wl.className="SpatialDropout1D",e.serialization.registerClass(Wl);class Ul extends ha{constructor(t){if(super(t),this.activation=null,this.useBias=!0,this.kernel=null,this.bias=null,this.DEFAULT_KERNEL_INITIALIZER="glorotNormal",this.DEFAULT_BIAS_INITIALIZER="zeros",null==t.batchInputShape&&null==t.inputShape&&null!=t.inputDim){let e=null;null!=t.batchSize&&(e=t.batchSize),this.batchInputShape=[e,t.inputDim]}this.units=t.units,Di(this.units,"units"),this.activation=Ho(t.activation),null!=t.useBias&&(this.useBias=t.useBias),this.kernelInitializer=jr(t.kernelInitializer||this.DEFAULT_KERNEL_INITIALIZER),this.biasInitializer=jr(t.biasInitializer||this.DEFAULT_BIAS_INITIALIZER),this.kernelConstraint=Pi(t.kernelConstraint),this.biasConstraint=Pi(t.biasConstraint),this.kernelRegularizer=el(t.kernelRegularizer),this.biasRegularizer=el(t.biasRegularizer),this.activityRegularizer=el(t.activityRegularizer),this.supportsMasking=!0,this.inputSpec=[{minNDim:2}]}build(t){const e=(t=ta(t))[t.length-1];null==this.kernel&&(this.kernel=this.addWeight("kernel",[e,this.units],null,this.kernelInitializer,this.kernelRegularizer,!0,this.kernelConstraint),this.useBias&&(this.bias=this.addWeight("bias",[this.units],null,this.biasInitializer,this.biasRegularizer,!0,this.biasConstraint))),this.inputSpec=[{minNDim:2,axes:{[-1]:e}}],this.built=!0}computeOutputShape(t){const e=(t=ta(t)).slice();return e[e.length-1]=this.units,e}call(t,n){return e.tidy(()=>{this.invokeCallHook(t,n);const e=Qr(t),s=Ti(this.activation.getClassName());let i;return null!=s?i=yr(e,this.kernel.read(),s,this.bias?this.bias.read():null):(i=yr(e,this.kernel.read()),null!=this.bias&&(i=xr(i,this.bias.read())),null!=this.activation&&(i=this.activation.apply(i))),i})}getConfig(){const t={units:this.units,activation:qo(this.activation),useBias:this.useBias,kernelInitializer:Vr(this.kernelInitializer),biasInitializer:Vr(this.biasInitializer),kernelRegularizer:Qo(this.kernelRegularizer),biasRegularizer:Qo(this.biasRegularizer),activityRegularizer:Qo(this.activityRegularizer),kernelConstraint:Oi(this.kernelConstraint),biasConstraint:Oi(this.biasConstraint)},e=super.getConfig();return Object.assign(t,e),t}}Ul.className="Dense",e.serialization.registerClass(Ul);class Kl extends ha{constructor(t){super(t=t||{}),this.inputSpec=[{minNDim:3}],this.dataFormat=t.dataFormat}computeOutputShape(t){t=ta(t);for(const e of t.slice(1))if(null==e)throw new ci(`The shape of the input to "Flatten" is not fully defined (got ${t.slice(1)}). Make sure to pass a complete "input_shape" or "batch_input_shape" argument to the first layer in your model.`);return[t[0],sr(t,1)]}call(t,n){return e.tidy(()=>{this.invokeCallHook(t,n);let e=Qr(t);if("channelsFirst"===this.dataFormat&&e.rank>1){const t=[0];for(let n=2;n<e.rank;++n)t.push(n);t.push(1),e=e.transpose(t)}return function(t){if(t.rank<=1)throw new ci(`batchFlatten requires a minimum rank of 2. Got rank: ${t.rank}.`);const e=[t.shape[0],sr(t.shape,1)];return t.reshape(e)}(e)})}getConfig(){const t={};null!=this.dataFormat&&(t.dataFormat=this.dataFormat);const e=super.getConfig();return Object.assign(t,e),t}}Kl.className="Flatten",e.serialization.registerClass(Kl);class Vl extends ha{constructor(t){super(t),this.supportsMasking=!0,this.activation=Ho(t.activation)}call(t,n){return e.tidy(()=>{this.invokeCallHook(t,n);const e=Qr(t);return this.activation.apply(e)})}getConfig(){const t={activation:qo(this.activation)},e=super.getConfig();return Object.assign(t,e),t}}Vl.className="Activation",e.serialization.registerClass(Vl);class jl extends ha{constructor(t){super(t),this.n=t.n,this.inputSpec=[{ndim:2}]}computeOutputShape(t){return[t[0],this.n,t[1]]}call(t,n){return e.tidy(()=>{return t=Qr(t),n=t,s=this.n,e.tidy(()=>{if(2!==n.shape.length)throw new ci(`repeat() expects a rank-2 tensor, but received a rank-${n.shape.length} tensor.`);return gr(ur(n,1),[1,s,1])});var n,s})}getConfig(){const t={n:this.n},e=super.getConfig();return Object.assign(t,e),t}}jl.className="RepeatVector",e.serialization.registerClass(jl);class ql extends ha{constructor(t){super(t),this.targetShape=t.targetShape;for(let t=0;t<this.targetShape.length;++t)this.isUnknown(this.targetShape[t])&&(this.targetShape[t]=null)}isUnknown(t){return t<0||null==t}fixUnknownDimension(t,e){const n="Total size of new array must be unchanged.",s=e.slice();let i=1,r=null;for(let t=0;t<s.length;++t){const e=s[t];if(this.isUnknown(e)){if(null!==r)throw new ci("Can only specifiy one unknown dimension.");r=t}else i*=e}const a=sr(t);if(null!==r){if(0===i||a%i!=0)throw new ci(n);s[r]=a/i}else if(a!==i)throw new ci(n);return s}computeOutputShape(t){let e=!1;for(let n=0;n<t.length;++n)if(this.isUnknown(t[n])){e=!0;break}return e?t.slice(0,1).concat(this.targetShape):t.slice(0,1).concat(this.fixUnknownDimension(t.slice(1),this.targetShape))}call(t,n){return e.tidy(()=>{this.invokeCallHook(t,n);const e=Qr(t),s=e.shape,i=s.slice(0,1).concat(this.fixUnknownDimension(s.slice(1),this.targetShape));return e.reshape(i)})}getConfig(){const t={targetShape:this.targetShape},e=super.getConfig();return Object.assign(t,e),t}}ql.className="Reshape",e.serialization.registerClass(ql);class Gl extends ha{constructor(t){if(super(t),null==t.dims)throw new Error("Required configuration field `dims` is missing during Permute constructor call.");if(!Array.isArray(t.dims))throw new Error("Permute constructor requires `dims` to be an Array, but received "+t.dims+" instead.");const n=or(1,t.dims.length+1);if(!e.util.arraysEqual(t.dims.slice().sort(),n))throw new Error("Invalid permutation `dims`: "+JSON.stringify(t.dims)+" `dims` must contain consecutive integers starting from 1.");this.dims=t.dims,this.dimsIncludingBatch=[0].concat(this.dims),this.inputSpec=[new ra({ndim:this.dims.length+1})]}computeOutputShape(t){const e=(t=ta(t)).slice();return this.dims.forEach((n,s)=>{e[s+1]=t[n]}),e}call(t,n){return e.transpose(Qr(t),this.dimsIncludingBatch)}getConfig(){const t={dims:this.dims},e=super.getConfig();return Object.assign(t,e),t}}Gl.className="Permute",e.serialization.registerClass(Gl);class Hl extends ha{constructor(t){super(null==t?{}:t),this.supportsMasking=!0,this.maskValue=null!=t?null==t.maskValue?0:t.maskValue:0}computeOutputShape(t){return t}getConfig(){const t=super.getConfig(),e={maskValue:this.maskValue};return Object.assign(e,t),e}computeMask(t,n){const s=Qr(t);return e.any(e.notEqual(s,this.maskValue),-1)}call(t,n){return e.tidy(()=>{this.invokeCallHook(t,n);const s=Qr(t),i=e.any(e.notEqual(s,this.maskValue),-1,!0);return s.mul(i.asType(s.dtype))})}}Hl.className="Masking",e.serialization.registerClass(Hl);class Jl extends ha{constructor(t){if(super(t),this.embeddings=null,this.DEFAULT_EMBEDDINGS_INITIALIZER="randomUniform",null==t.batchInputShape&&null==t.inputShape){let e=null;null!=t.batchSize&&(e=t.batchSize),null==t.inputLength?this.batchInputShape=[e,null]:this.batchInputShape=[e].concat(bi(t.inputLength))}this.inputDim=t.inputDim,Di(this.inputDim,"inputDim"),this.outputDim=t.outputDim,Di(this.outputDim,"outputDim"),this.embeddingsInitializer=jr(t.embeddingsInitializer||this.DEFAULT_EMBEDDINGS_INITIALIZER),this.embeddingsRegularizer=el(t.embeddingsRegularizer),this.activityRegularizer=el(t.activityRegularizer),this.embeddingsConstraint=Pi(t.embeddingsConstraint),this.maskZero=t.maskZero,this.supportsMasking=t.maskZero,this.inputLength=t.inputLength}build(t){this.embeddings=this.addWeight("embeddings",[this.inputDim,this.outputDim],this.dtype,this.embeddingsInitializer,this.embeddingsRegularizer,!0,this.embeddingsConstraint),this.built=!0}warnOnIncompatibleInputShape(t){}computeMask(t,n){return e.tidy(()=>this.maskZero?(t=Qr(t),e.notEqual(t,e.zerosLike(t))):null)}computeOutputShape(t){if(t=ta(t),null==this.inputLength)return[...t,this.outputDim];const e=bi(this.inputLength);if(e.length!==t.length-1)throw new ci(`"inputLength" is ${this.inputLength}, but received input shape has shape `+t);{let n=0;for(let s=0;s<e.length;++s){const i=e[s],r=t[s+1];if(null!=i&&null!=r&&i!==r)throw new ci(`"inputLength" is ${this.inputLength}, but received input shape has shape `+t);null==i&&(e[n]=r),n++}}return[t[0],...e,this.outputDim]}call(t,n){return e.tidy(()=>{this.invokeCallHook(t,n);let e=Qr(t);"int32"!==e.dtype&&(e=lr(e,"int32"));return br(this.embeddings.read(),e.as1D()).reshape(ta(this.computeOutputShape(e.shape)))})}getConfig(){const t={inputDim:this.inputDim,outputDim:this.outputDim,embeddingsInitializer:Vr(this.embeddingsInitializer),embeddingsRegularizer:Qo(this.embeddingsRegularizer),activityRegularizer:Qo(this.activityRegularizer),embeddingsConstraint:Oi(this.embeddingsConstraint),maskZero:this.maskZero,inputLength:this.inputLength},e=super.getConfig();return Object.assign(t,e),t}}Jl.className="Embedding",e.serialization.registerClass(Jl);class Zl extends ha{constructor(t){super(t||{}),this.supportsMasking=!0}mergeFunction(t){throw new pi}computeElementwiseOpOutputShape(t,e){if(null==t||null==e)return null;if(t.length<e.length)return this.computeElementwiseOpOutputShape(e,t);if(0===e.length)return t;const n=t.slice(0,t.length-e.length);for(let s=0;s<e.length;++s){const i=t[t.length-e.length+s],r=e[s];if(null==i||null==r||i<0||r<0)n.push(null);else if(1===i)n.push(r);else if(1===r)n.push(i);else{if(i!==r)throw new ci("Operands could not be broadcast together with shapes "+JSON.stringify(t)+" "+JSON.stringify(e));n.push(i)}}return n}build(t){if(Array.isArray(t)&&!Array.isArray(t[0])&&(t=[ta(t)]),(t=t).length<2)throw new ci(`A merge layer should be called on an Array of at least 2 inputs. Got ${t.length} input(s).`);let e=[];for(const n of t)null!=n&&null!==n[0]&&e.push(n[0]);if(e=Ni(e),e.length>1)throw new ci(`Can not merge tensors with different batch sizes. Got tensors with shapes: ${JSON.stringify(t)}.`);let n=null==t[0]?null:t[0].slice(1);for(let e=1;e<t.length;++e){const s=null==t[e]?null:t[e].slice(1);n=this.computeElementwiseOpOutputShape(n,s)}const s=t.map(t=>t.length);-1===t.indexOf(null)&&1===Ni(s).length?this.reshapeRequired=!1:this.reshapeRequired=!0}call(t,n){return e.tidy(()=>{if(t=t,this.reshapeRequired){const n=[],s=t.map(t=>t.rank);if(-1===s.indexOf(null)){const e=ar(s);for(let s of t){const t=s.rank;for(let n=0;n<e-t;++n)s=ur(s,1);n.push(s)}return this.mergeFunction(n)}{let s=!1;for(const i of t){const t=i.rank;if(null==t){const t=i.shape,r=t[0],a=t.slice(1).concat([r]);let o=i.reshape([r].concat(sr(t.slice(1))));o=e.transpose(o,[1,0]),o=o.reshape(a),n.push(o),s=!0}else if(t>1){const r=or(1,t).concat([0]);n.push(e.transpose(i,r)),s=!0}else n.push(i)}let i=this.mergeFunction(n);const r=i.rank;if(s)if(null==r){const t=i.shape,n=t[t.length-1],s=[n].concat(t.slice(0,t.length-1));i=e.transpose(i.reshape([-1,n]),[1,0]).reshape(s)}else if(r>1){const t=[r-1].concat(or(0,r-1));i=e.transpose(i,t)}return i}}return this.mergeFunction(t)})}computeOutputShape(t){let e;e=null==(t=t)[0]?null:t[0].slice(1);for(let n=1;n<t.length;++n){const s=null==t[n]?null:t[n].slice(1);e=this.computeElementwiseOpOutputShape(e,s)}let n=[];for(const e of t)null!=e&&null!==e[0]&&n.push(e[0]);return n=Ni(n),e=1===n.length?n.concat(e):[null].concat(e),e}computeMask(t,n){return e.tidy(()=>{if(null==n)return null;if(!Array.isArray(n))throw new ci("`mask` should be an Array");if(!Array.isArray(t))throw new ci("`inputs` should be an Array");if(n.length!==t.length)throw new ci(`The Array 'inputs' and 'mask' are expected to have the same length, but have different lengths (${t.length} vs ${n.length})`);if(n.every(t=>null==t))return null;let s=(n=n.map(t=>null==t?t:e.expandDims(t,0)))[0];for(let t=1;t<n.length-1;++t)s=e.logicalAnd(s,n[t]);return s})}}class Xl extends Zl{constructor(t){super(t)}mergeFunction(t){return e.tidy(()=>{let n=t[0].clone();for(let s=1;s<t.length;++s)n=e.add(n,t[s]);return n})}}Xl.className="Add",e.serialization.registerClass(Xl);class Yl extends Zl{constructor(t){super(t)}mergeFunction(t){return e.tidy(()=>{let n=t[0].clone();for(let s=1;s<t.length;++s)n=e.mul(n,t[s]);return n})}}Yl.className="Multiply",e.serialization.registerClass(Yl);class Ql extends Zl{constructor(t){super(t)}mergeFunction(t){return e.tidy(()=>{let n=t[0].clone();for(let s=1;s<t.length;++s)n=e.add(n,t[s]);return e.mul(1/t.length,n)})}}Ql.className="Average",e.serialization.registerClass(Ql);class tu extends Zl{constructor(t){super(t)}mergeFunction(t){return e.tidy(()=>{let n=t[0];for(let s=1;s<t.length;++s)n=e.maximum(n,t[s]);return n})}}tu.className="Maximum",e.serialization.registerClass(tu);class eu extends Zl{constructor(t){super(t)}mergeFunction(t){return e.tidy(()=>{let n=t[0];for(let s=1;s<t.length;++s)n=e.minimum(n,t[s]);return n})}}eu.className="Minimum",e.serialization.registerClass(eu);class nu extends Zl{constructor(t){super(t),this.DEFAULT_AXIS=-1,null==t&&(t={}),this.axis=null==t.axis?this.DEFAULT_AXIS:t.axis,this.supportsMasking=!0,this.reshapeRequired=!1}build(t){if(!Array.isArray(t)||!Array.isArray(t[0])||1===t.length)throw new ci("A `Concatenate` layer should be called on a list of at least 2 inputs");t=t;let n=!0;for(const e of t)if(null!=e){n=!1;break}if(n)return;const s=[];for(let n=0;n<t.length;++n){const i=t[n].slice();i.splice(this.axis,1);let r=!1;for(const t of s)if(e.util.arraysEqual(t,i)){r=!0;break}r||s.push(i)}if(s.length>1)throw new ci("A `Concatenate` layer requires inputs with matching shapes except for the concat axis. Got input shapes: "+JSON.stringify(t))}mergeFunction(t){return e.tidy(()=>dr(t,this.axis))}computeOutputShape(t){if(!Array.isArray(t)||!Array.isArray(t[0]))throw new ci("A `Concatenate` layer should be called on a list of inputs.");const e=t,n=e[0].slice(),s=this.axis<0?n.length+this.axis:this.axis;for(const t of e.slice(1)){if(null==n[s]||null==t[s]){n[s]=null;break}n[s]+=t[s]}return n}computeMask(t,n){if(null==n)return null;if(!Array.isArray(n))throw new ci("`mask` should be an array for Concatenate");if(!Array.isArray(t))throw new ci("`inputs` should be an array for Concatenate");if(n.length!==t.length)throw new ci(`Mismatch in the length of mask (${n.length}) and the legnth of inputs (${t.length})`);return e.tidy(()=>{let s=!0;if(n.forEach(t=>{null==t||(s=!1)}),s)return null;const i=[];for(let s=0;s<t.length;++s)null==n[s]?i.push(e.onesLike(t[s]).asType("bool")):n[s].rank<t[s].rank?i.push(e.expandDims(n[s],-1)):i.push(n[s]);const r=e.concat(i,this.axis);return e.all(r,-1,!1)})}getConfig(){const t={axis:this.axis},e=super.getConfig();return Object.assign(t,e),t}}function su(t,e){for(;t<0;)t+=e;return t}nu.className="Concatenate",e.serialization.registerClass(nu);class iu extends Zl{constructor(t){super(t),this.axes=t.axes,this.normalize=null!=t.normalize&&t.normalize,this.supportsMasking=!0,this.reshapeRequired=!1}build(t){e.util.assert(Array.isArray(t)&&2===t.length&&Array.isArray(t[0])&&Array.isArray(t[1]),()=>"A `Dot` layer should be called on a list of exactly 2 inputs.");const n=t[0],s=t[1];if(n.length>3||s.length>3)throw new pi("Dot layer does not support tensors of 4D or higher rank yet.");const i=this.interpretAxes(n,s);if(n[i[0]]!==s[i[1]])throw new ci(`Dimension incompatibility: ${n[i[0]]} !== ${s[i[1]]}`)}mergeFunction(t){if(2!==t.length)throw new ci(`A \`Dot\` layer must be called on exactly 2 inputs, but received ${t.length} input(s).`);let n,s=t[0],i=t[1];return n=Array.isArray(this.axes)?this.axes.map((e,n)=>su(e,t[n].shape.length)):[su(this.axes,s.shape.length),su(this.axes,i.shape.length)],this.normalize&&(s=Na(s,n[0]),i=Na(i,n[1])),function(t,n,s){if(t.shape.length>3||n.shape.length>3)throw new pi("batchDot is not implemented for tensors of 4D or higher rank yet");if(e.util.assert(t.shape.length>=2,()=>"batchDot requires the rank of x to be >= 2, but got "+t.shape.length),e.util.assert(t.shape.length>=2,()=>"batchDot requires the rank of y to be >= 2, but got "+n.shape.length),"number"==typeof s&&(s=[s,s]),"complex64"===t.dtype||"complex64"===n.dtype)throw new pi("batchDot is not implemented for complex64-type Tensors yet.");const i=t.shape.length,r=n.shape.length;null==s&&(s=[i-1,r-2]);const a=s;return e.tidy(()=>{let e,s;if(i>r){e=i-r;const t=[];for(let n=0;n<e;++n)t.push(1);n=n.reshape(n.shape.concat(t))}else if(r>i){e=r-i;const n=[];for(let t=0;t<e;++t)n.push(1);t=t.reshape(t.shape.concat(n))}else e=0;if(2===t.shape.length&&2===n.shape.length)s=a[0]===a[1]?t.mul(n).sum(a[0]):t.transpose([1,0]).mul(n).sum(a[1]);else{const e=a[0]!==t.shape.length-1,i=a[1]===n.shape.length-1;s=t.matMul(n,e,i)}if(e>0){let t;t=i>r?i+r-3:i-1;const n=[];for(let s=t;s<t+e;++s)n.push(s);s=s.squeeze(n)}return 1===s.shape.length&&(s=s.expandDims(1)),s})}(s,i,n)}interpretAxes(t,e){let n;return n=Array.isArray(this.axes)?this.axes:[su(this.axes,t.length),su(this.axes,e.length)],n}computeOutputShape(t){e.util.assert(Array.isArray(t)&&2===t.length&&Array.isArray(t[0])&&Array.isArray(t[1]),()=>"A `Dot` layer should be called on a list of exactly 2 inputs.");const n=t[0].slice(),s=t[1].slice();if(n.length>3||s.length>3)throw new pi("Dot layer does not support tensors of 4D or higher rank yet.");const i=this.interpretAxes(n,s);n.splice(i[0],1),s.splice(i[1],1),s.splice(0,1);const r=n.concat(s);return 1===r.length&&r.push(1),r}computeMask(t,e){return null}getConfig(){const t={axes:this.axes,normalize:this.normalize},e=super.getConfig();return Object.assign(t,e),t}}iu.className="Dot",e.serialization.registerClass(iu);class ru extends ha{constructor(t){super(t),this.supportsMasking=!0,this.stddev=t.stddev}computeOutputShape(t){return t}getConfig(){const t=super.getConfig(),e={stddev:this.stddev};return Object.assign(e,t),e}call(t,n){return e.tidy(()=>{this.invokeCallHook(t,n);const e=Qr(t);return Sr(()=>mr(e.shape,0,this.stddev).add(e),()=>e,n.training||!1)})}}ru.className="GaussianNoise",e.serialization.registerClass(ru);class au extends ha{constructor(t){super(t),this.supportsMasking=!0,this.rate=t.rate}computeOutputShape(t){return t}getConfig(){const t=super.getConfig(),e={rate:this.rate};return Object.assign(e,t),e}call(t,n){return e.tidy(()=>{this.invokeCallHook(t,n);const e=Qr(t);if(this.rate>0&&this.rate<1){return Sr(()=>{const t=Math.sqrt(this.rate/(1-this.rate));return e.mul(mr(e.shape,1,t))},()=>e,n.training||!1)}return e})}}au.className="GaussianDropout",e.serialization.registerClass(au);class ou extends ha{constructor(t){super(t),this.supportsMasking=!0,this.rate=t.rate,this.noiseShape=t.noiseShape}_getNoiseShape(t){return this.noiseShape||Qr(t).shape}computeOutputShape(t){return t}getConfig(){const t=super.getConfig(),e={rate:this.rate};return Object.assign(e,t),e}call(t,n){return e.tidy(()=>{if(this.rate<1&&this.rate>0){const s=this._getNoiseShape(t);return Sr(()=>{const n=Qr(t),i=-1.7580993408473766;let r=e.greaterEqual(e.randomUniform(s),this.rate);r=lr(r,"float32");const a=((1-this.rate)*(1+this.rate*i**2))**-.5,o=-a*i*this.rate;return n.mul(r).add(r.add(-1).mul(i)).mul(a).add(o)},()=>Qr(t),n.training||!1)}return t})}}function lu(t,n,s,i,r,a=.001){let o;if(2===t.rank)o=e.batchNorm2d(t,n,s,i,r,a);else if(3===t.rank)o=e.batchNorm3d(t,n,s,i,r,a);else{if(4!==t.rank)throw new pi(`batchNormalization is not implemented for array of rank ${t.rank} yet`);o=e.batchNorm4d(t,n,s,i,r,a)}return o}function uu(t,n,s,i,r=.001){return e.util.arraysEqual(i.slice().sort(),or(0,t.rank-1))?function(t,n,s,i,r=.001){return e.tidy(()=>{const a=e.moments(t,i),o=a.mean,l=a.variance;return[lu(t,o,l,s,n,r),o,l]})}(t,n,s,i,r):function(t,n,s,i,r=.001){return e.tidy(()=>{const a=e.moments(t,i),o=a.mean,l=a.variance,u=[];for(const e of or(0,t.rank))-1!==i.indexOf(e)?u.push(1):u.push(t.shape[e]);const h=o.reshape(u),c=l.reshape(u),p=null==n?null:n.reshape(u),d=null==s?null:s.reshape(u);return[lu(t,h,c,d,p,r),o,l]})}(t,n,s,i,r)}ou.className="AlphaDropout",e.serialization.registerClass(ou);class hu extends ha{constructor(t){null==t&&(t={}),super(t),this.supportsMasking=!0,this.axis=null==t.axis?-1:t.axis,this.momentum=null==t.momentum?.99:t.momentum,this.epsilon=null==t.epsilon?.001:t.epsilon,this.center=null==t.center||t.center,this.scale=null==t.scale||t.scale,this.betaInitializer=jr(t.betaInitializer||"zeros"),this.gammaInitializer=jr(t.gammaInitializer||"ones"),this.movingMeanInitializer=jr(t.movingMeanInitializer||"zeros"),this.movingVarianceInitializer=jr(t.movingVarianceInitializer||"ones"),this.betaConstraint=Pi(t.betaConstraint),this.gammaConstraint=Pi(t.gammaConstraint),this.betaRegularizer=el(t.betaRegularizer),this.gammaRegularizer=el(t.gammaRegularizer)}build(t){t=ta(t);const e=this.axis>=0?this.axis:this.axis+t.length,n=t[e];if(null==n)throw new ci(`Axis ${e} of input tensor should have a defined dimension but the layer received an input with shape `+JSON.stringify(t)+".");this.inputSpec=[new ra({ndim:t.length,axes:{[e]:n}})];const s=[n];this.scale&&(this.gamma=this.addWeight("gamma",s,null,this.gammaInitializer,this.gammaRegularizer,!0,this.gammaConstraint)),this.center&&(this.beta=this.addWeight("beta",s,null,this.betaInitializer,this.betaRegularizer,!0,this.betaConstraint)),this.movingMean=this.addWeight("moving_mean",s,null,this.movingMeanInitializer,null,!1),this.movingVariance=this.addWeight("moving_variance",s,null,this.movingVarianceInitializer,null,!1),this.built=!0}call(t,n){return e.tidy(()=>{const s=null!=n.training&&n.training,i=Qr(t),r=i.shape,a=r.length,o=or(0,a),l=this.axis>=0?this.axis:this.axis+a;o.splice(l,1);const u=fi(1,a);u[l]=r[l];const h=o.slice();h.sort();const c=!e.util.arraysEqual(h,or(0,a).slice(0,a-1));if(!s)return(()=>{if(c){const t=this.movingMean.read().reshape(u),e=this.movingVariance.read().reshape(u),n=this.center?this.beta.read().reshape(u):null,s=this.scale?this.gamma.read().reshape(u):null;return lu(i,t,e,n,s,this.epsilon)}return lu(i,this.movingMean.read(),this.movingVariance.read(),null==this.beta?null:this.beta.read(),null==this.gamma?null:this.gamma.read(),this.epsilon)})();const[p,d,f]=uu(i,this.gamma.read(),this.beta.read(),o,this.epsilon),g=(t,n,s)=>{e.tidy(()=>{const e=1-s,i=t.read(),r=i.sub(n).mul(e);t.write(i.sub(r))})};return(()=>{g(this.movingMean,d,this.momentum),g(this.movingVariance,f,this.momentum)})(),p})}getConfig(){const t={axis:this.axis,momentum:this.momentum,epsilon:this.epsilon,center:this.center,scale:this.scale,betaInitializer:Vr(this.betaInitializer),gammaInitializer:Vr(this.gammaInitializer),movingMeanInitializer:Vr(this.movingMeanInitializer),movingVarianceInitializer:Vr(this.movingVarianceInitializer),betaRegularizer:Qo(this.betaRegularizer),gammaRegularizer:Qo(this.gammaRegularizer),betaConstraint:Oi(this.betaConstraint),gammaConstraint:Oi(this.gammaConstraint)},e=super.getConfig();return Object.assign(t,e),t}}hu.className="BatchNormalization",e.serialization.registerClass(hu);class cu extends ha{constructor(t){if(null==t&&(t={}),super(t),this.axis=null==t.axis?-1:t.axis,"number"==typeof this.axis){if(!Number.isInteger(this.axis))throw new Error("Expected axis to be an integer, but received "+this.axis)}else{if(!Array.isArray(this.axis))throw new Error("Expected axis to be an integer or an array of integers, but received "+JSON.stringify(this.axis));for(const t of this.axis)if(!Number.isInteger(t))throw new Error("Expected axis to be an array of integers, but received "+JSON.stringify(this.axis))}this.epsilon=null==t.epsilon?.001:t.epsilon,this.center=null==t.center||t.center,this.scale=null==t.scale||t.scale,this.betaInitializer=jr(t.betaInitializer||"zeros"),this.gammaInitializer=jr(t.gammaInitializer||"ones"),this.betaRegularizer=el(t.betaRegularizer),this.gammaRegularizer=el(t.gammaRegularizer),this.supportsMasking=!0}build(t){const e=(t=ta(t)).length;"number"==typeof this.axis&&(this.axis=[this.axis]);for(let t=0;t<this.axis.length;++t)this.axis[t]<0&&(this.axis[t]+=e);for(const t of this.axis)if(t<0||t>=e)throw new Error("Invalid axis: "+t);if(this.axis.length!==Ni(this.axis).length)throw new Error("Found duplicate axes in: "+this.axis);const n=this.axis.map(e=>t[e]);this.scale?this.gamma=this.addWeight("gamma",n,"float32",this.gammaInitializer,this.gammaRegularizer,!0):this.gamma=null,this.center?this.beta=this.addWeight("beta",n,"float32",this.betaInitializer,this.betaRegularizer,!0):this.beta=null,this.built=!0}call(t,n){const s=Qr(t),i=s.shape,r=i.length;return e.tidy(()=>{let{mean:t,variance:n}=e.moments(s,this.axis,!0);const a=fi(1,r);for(const t of this.axis)a[t]=i[t];const o=t=>null!=t&&t.shape.length!==r&&this.axis!==[r-1]?t.reshape(a):t;let l=o(this.gamma.read()),u=o(this.beta.read());const h=[],c=[];for(let t=0;t<r;++t)-1!==this.axis.indexOf(t)?(h.push(i[t]),c.push(1)):(h.push(1),c.push(i[t]));return t=t.tile(h),n=n.tile(h),l=l.tile(c),u=u.tile(c),lu(s,t,n,u,l,this.epsilon)})}getConfig(){const t={axis:this.axis,epsilon:this.epsilon,center:this.center,scale:this.scale,betaInitializer:Vr(this.betaInitializer),gammaInitializer:Vr(this.gammaInitializer),betaRegularizer:Qo(this.betaRegularizer),gammaRegularizer:Qo(this.gammaRegularizer)},e=super.getConfig();return Object.assign(t,e),t}}cu.className="LayerNormalization",e.serialization.registerClass(cu);class pu extends ha{constructor(t){if(null==t&&(t={}),super(t),this.dataFormat=null==t.dataFormat?"channelsLast":t.dataFormat,null==t.padding)this.padding=[[1,1],[1,1]];else if("number"==typeof t.padding)this.padding=[[t.padding,t.padding],[t.padding,t.padding]];else{if(t.padding=t.padding,2!==t.padding.length)throw new ci(`ZeroPadding2D expects padding to be a length-2 array, but received a length-${t.padding.length} array.`);let e,n;if("number"==typeof t.padding[0])e=[t.padding[0],t.padding[0]],n=[t.padding[1],t.padding[1]];else{if(t.padding=t.padding,2!==t.padding[0].length)throw new ci(`ZeroPadding2D expects height padding to be a length-2 array, but received a length-${t.padding[0].length} array.`);if(e=t.padding[0],2!==t.padding[1].length)throw new ci(`ZeroPadding2D expects width padding to be a length-2 array, but received a length-${t.padding[1].length} array.`);n=t.padding[1]}this.padding=[e,n]}this.inputSpec=[new ra({ndim:4})]}computeOutputShape(t){let e,n;return t=ta(t),"channelsFirst"===this.dataFormat?(e=null!=t[2]&&t[2]>=0?t[2]+this.padding[0][0]+this.padding[0][1]:null,n=null!=t[3]&&t[3]>=0?t[3]+this.padding[1][0]+this.padding[1][1]:null,[t[0],t[1],e,n]):(e=null!=t[1]&&t[1]>=0?t[1]+this.padding[0][0]+this.padding[0][1]:null,n=null!=t[2]&&t[2]>=0?t[2]+this.padding[1][0]+this.padding[1][1]:null,[t[0],e,n,t[3]])}call(t,n){return e.tidy(()=>{return n=Qr(t),s=this.padding,i=this.dataFormat,e.tidy(()=>{if(4!==n.rank)throw new ci("temporalPadding expects input tensor to be 4-D, but received a "+n.rank+"-D tensor.");if(null==s&&(s=[[1,1],[1,1]]),2!==s.length||2!==s[0].length||2!==s[1].length)throw new ci("spatial2dPadding expects `padding` to be an Array of two Arrays, each of which is an Array of two integers.");if(null==i&&(i="channelsLast"),"channelsLast"!==i&&"channelsFirst"!==i)throw new ci(`Unknown data format: ${i}. Supported data formats are 'channelsLast' and 'channelsFirst.`);let t;return t="channelsFirst"===i?[[0,0],[0,0],s[0],s[1]]:[[0,0],s[0],s[1],[0,0]],e.pad(n,t)});var n,s,i})}getConfig(){const t={padding:this.padding,dataFormat:this.dataFormat},e=super.getConfig();return Object.assign(t,e),t}}function du(t,n,s,i,r,a){return e.tidy(()=>{let o;Hi(r),Zi(a),Ji(i),null==s&&(s=[1,1]),null==i&&(i="valid"),null==r&&(r="channelsLast"),null==a&&(a="max"),t=cl(t,r);const l="same"===i?"same":"valid";return o="max"===a?e.maxPool(t,n,s,l):e.avgPool(t,n,s,l),"channelsFirst"===r&&(o=e.transpose(o,[0,3,1,2])),o})}function fu(t,n,s,i,r,a){return e.tidy(()=>{let o;Hi(r),Zi(a),Ji(i),null==s&&(s=[1,1,1]),null==i&&(i="valid"),null==r&&(r="channelsLast"),null==a&&(a="max"),t=pl(t,r);const l="same"===i?"same":"valid";return o="max"===a?e.maxPool3d(t,n,s,l):e.avgPool3d(t,n,s,l),"channelsFirst"===r&&(o=e.transpose(o,[0,4,1,2,3])),o})}pu.className="ZeroPadding2D",e.serialization.registerClass(pu);class gu extends ha{constructor(t){if(null==t.poolSize&&(t.poolSize=2),super(t),"number"==typeof t.poolSize)this.poolSize=[t.poolSize];else{if(!Array.isArray(t.poolSize)||1!==t.poolSize.length||"number"!=typeof t.poolSize[0])throw new ci("poolSize for 1D convolutional layer must be a number or an Array of a single number, but received "+JSON.stringify(t.poolSize));this.poolSize=t.poolSize}if(Di(this.poolSize,"poolSize"),null==t.strides)this.strides=this.poolSize;else if("number"==typeof t.strides)this.strides=[t.strides];else{if(!Array.isArray(t.strides)||1!==t.strides.length||"number"!=typeof t.strides[0])throw new ci("strides for 1D convolutional layer must be a number or an Array of a single number, but received "+JSON.stringify(t.strides));this.strides=t.strides}Di(this.strides,"strides"),this.padding=null==t.padding?"valid":t.padding,Ji(this.padding),this.inputSpec=[new ra({ndim:3})]}computeOutputShape(t){const e=ul((t=ta(t))[1],this.poolSize[0],this.padding,this.strides[0]);return[t[0],e,t[2]]}call(t,n){return e.tidy(()=>{this.invokeCallHook(t,n),t=ur(Qr(t),2);const s=this.poolingFunction(Qr(t),[this.poolSize[0],1],[this.strides[0],1],this.padding,"channelsLast");return e.squeeze(s,[2])})}getConfig(){const t={poolSize:this.poolSize,padding:this.padding,strides:this.strides},e=super.getConfig();return Object.assign(t,e),t}}class mu extends gu{constructor(t){super(t)}poolingFunction(t,e,n,s,i){return Hi(i),Ji(s),du(t,e,n,s,i,"max")}}mu.className="MaxPooling1D",e.serialization.registerClass(mu);class yu extends gu{constructor(t){super(t)}poolingFunction(t,e,n,s,i){return Hi(i),Ji(s),du(t,e,n,s,i,"avg")}}yu.className="AveragePooling1D",e.serialization.registerClass(yu);class bu extends ha{constructor(t){if(null==t.poolSize&&(t.poolSize=[2,2]),super(t),this.poolSize=Array.isArray(t.poolSize)?t.poolSize:[t.poolSize,t.poolSize],null==t.strides)this.strides=this.poolSize;else if(Array.isArray(t.strides)){if(2!==t.strides.length)throw new ci("If the strides property of a 2D pooling layer is an Array, it is expected to have a length of 2, but received length "+t.strides.length+".");this.strides=t.strides}else this.strides=[t.strides,t.strides];Di(this.poolSize,"poolSize"),Di(this.strides,"strides"),this.padding=null==t.padding?"valid":t.padding,this.dataFormat=null==t.dataFormat?"channelsLast":t.dataFormat,Hi(this.dataFormat),Ji(this.padding),this.inputSpec=[new ra({ndim:4})]}computeOutputShape(t){t=ta(t);let e="channelsFirst"===this.dataFormat?t[2]:t[1],n="channelsFirst"===this.dataFormat?t[3]:t[2];return e=ul(e,this.poolSize[0],this.padding,this.strides[0]),n=ul(n,this.poolSize[1],this.padding,this.strides[1]),"channelsFirst"===this.dataFormat?[t[0],t[1],e,n]:[t[0],e,n,t[3]]}call(t,n){return e.tidy(()=>(this.invokeCallHook(t,n),this.poolingFunction(Qr(t),this.poolSize,this.strides,this.padding,this.dataFormat)))}getConfig(){const t={poolSize:this.poolSize,padding:this.padding,strides:this.strides,dataFormat:this.dataFormat},e=super.getConfig();return Object.assign(t,e),t}}class wu extends bu{constructor(t){super(t)}poolingFunction(t,e,n,s,i){return Hi(i),Ji(s),du(t,e,n,s,i,"max")}}wu.className="MaxPooling2D",e.serialization.registerClass(wu);class ku extends bu{constructor(t){super(t)}poolingFunction(t,e,n,s,i){return Hi(i),Ji(s),du(t,e,n,s,i,"avg")}}ku.className="AveragePooling2D",e.serialization.registerClass(ku);class xu extends ha{constructor(t){if(null==t.poolSize&&(t.poolSize=[2,2,2]),super(t),this.poolSize=Array.isArray(t.poolSize)?t.poolSize:[t.poolSize,t.poolSize,t.poolSize],null==t.strides)this.strides=this.poolSize;else if(Array.isArray(t.strides)){if(3!==t.strides.length)throw new ci("If the strides property of a 3D pooling layer is an Array, it is expected to have a length of 3, but received length "+t.strides.length+".");this.strides=t.strides}else this.strides=[t.strides,t.strides,t.strides];Di(this.poolSize,"poolSize"),Di(this.strides,"strides"),this.padding=null==t.padding?"valid":t.padding,this.dataFormat=null==t.dataFormat?"channelsLast":t.dataFormat,Hi(this.dataFormat),Ji(this.padding),this.inputSpec=[new ra({ndim:5})]}computeOutputShape(t){t=ta(t);let e="channelsFirst"===this.dataFormat?t[2]:t[1],n="channelsFirst"===this.dataFormat?t[3]:t[2],s="channelsFirst"===this.dataFormat?t[4]:t[3];return e=ul(e,this.poolSize[0],this.padding,this.strides[0]),n=ul(n,this.poolSize[1],this.padding,this.strides[1]),s=ul(s,this.poolSize[2],this.padding,this.strides[2]),"channelsFirst"===this.dataFormat?[t[0],t[1],e,n,s]:[t[0],e,n,s,t[4]]}call(t,n){return e.tidy(()=>(this.invokeCallHook(t,n),this.poolingFunction(Qr(t),this.poolSize,this.strides,this.padding,this.dataFormat)))}getConfig(){const t={poolSize:this.poolSize,padding:this.padding,strides:this.strides,dataFormat:this.dataFormat},e=super.getConfig();return Object.assign(t,e),t}}class vu extends xu{constructor(t){super(t)}poolingFunction(t,e,n,s,i){return Hi(i),Ji(s),fu(t,e,n,s,i,"max")}}vu.className="MaxPooling3D",e.serialization.registerClass(vu);class Su extends xu{constructor(t){super(t)}poolingFunction(t,e,n,s,i){return Hi(i),Ji(s),fu(t,e,n,s,i,"avg")}}Su.className="AveragePooling3D",e.serialization.registerClass(Su);class Iu extends ha{constructor(t){super(t),this.inputSpec=[new ra({ndim:3})]}computeOutputShape(t){return[t[0],t[2]]}call(t,e){throw new pi}}class Nu extends Iu{constructor(t){super(t||{})}call(t,n){return e.tidy(()=>{const n=Qr(t);return e.mean(n,1)})}}Nu.className="GlobalAveragePooling1D",e.serialization.registerClass(Nu);class zu extends Iu{constructor(t){super(t||{})}call(t,n){return e.tidy(()=>{const n=Qr(t);return e.max(n,1)})}}zu.className="GlobalMaxPooling1D",e.serialization.registerClass(zu);class Au extends ha{constructor(t){super(t),this.dataFormat=null==t.dataFormat?"channelsLast":t.dataFormat,Hi(this.dataFormat),this.inputSpec=[new ra({ndim:4})]}computeOutputShape(t){return t=t,"channelsLast"===this.dataFormat?[t[0],t[3]]:[t[0],t[1]]}call(t,e){throw new pi}getConfig(){const t={dataFormat:this.dataFormat},e=super.getConfig();return Object.assign(t,e),t}}class Cu extends Au{call(t,n){return e.tidy(()=>{const n=Qr(t);return"channelsLast"===this.dataFormat?e.mean(n,[1,2]):e.mean(n,[2,3])})}}Cu.className="GlobalAveragePooling2D",e.serialization.registerClass(Cu);class Du extends Au{call(t,n){return e.tidy(()=>{const n=Qr(t);return"channelsLast"===this.dataFormat?e.max(n,[1,2]):e.max(n,[2,3])})}}Du.className="GlobalMaxPooling2D",e.serialization.registerClass(Du);class Tu extends ha{constructor(t){super(t),this.layer=t.layer}build(t){this.built=!0}get trainable(){return null!=this.layer&&this.layer.trainable}set trainable(t){null!=this.layer&&(this.layer.trainable=t)}get trainableWeights(){return this.layer.trainableWeights}get nonTrainableWeights(){return this.layer.nonTrainableWeights}get updates(){return this.layer._updates}get losses(){return this.layer.losses}getWeights(){return this.layer.getWeights()}setWeights(t){this.layer.setWeights(t)}getConfig(){const t={layer:{className:this.layer.getClassName(),config:this.layer.getConfig()}},e=super.getConfig();return Object.assign(t,e),t}setFastWeightInitDuringBuild(t){super.setFastWeightInitDuringBuild(t),null!=this.layer&&this.layer.setFastWeightInitDuringBuild(t)}static fromConfig(t,e,n={}){const s=Ia(e.layer,n);delete e.layer;const i={layer:s};return Object.assign(i,e),new t(i)}}class Eu extends Tu{constructor(t){super(t),this.supportsMasking=!0}build(t){if((t=ta(t)).length<3)throw new ci("TimeDistributed layer expects an input shape >= 3D, but received input shape "+JSON.stringify(t));this.inputSpec=[{shape:t}];const e=[t[0]].concat(t.slice(2));this.layer.built||(this.layer.build(e),this.layer.built=!0),super.build(t)}computeOutputShape(t){const e=[(t=ta(t))[0]].concat(t.slice(2)),n=this.layer.computeOutputShape(e),s=t[1];return[n[0],s].concat(n.slice(1))}call(t,n){return e.tidy(()=>zl((t,e)=>[Qr(this.layer.call(t,n)),[]],t=Qr(t),[],!1,null,null,!1,!0)[1])}}Eu.className="TimeDistributed",e.serialization.registerClass(Eu);class Fu extends Tu{constructor(t){super(t);const e=t.layer.getConfig(),n={};n.className=t.layer.getClassName(),n.config=e,this.forwardLayer=Ia(n),e.goBackwards=!0!==e.goBackwards;const s={};var i;if(s.className=t.layer.getClassName(),s.config=e,this.backwardLayer=Ia(s),this.forwardLayer.name="forward_"+this.forwardLayer.name,this.backwardLayer.name="backward_"+this.backwardLayer.name,this.mergeMode=void 0===t.mergeMode?"concat":t.mergeMode,i=this.mergeMode,Ai(qi,"BidirectionalMergeMode",i),t.weights)throw new pi("weights support is not implemented for Bidirectional layer yet.");this._stateful=t.layer.stateful,this.returnSequences=t.layer.returnSequences,this.returnState=t.layer.returnState,this.supportsMasking=!0,this._trainable=!0,this.inputSpec=t.layer.inputSpec,this.numConstants=null}get trainable(){return this._trainable}set trainable(t){this._trainable=t,null!=this.forwardLayer&&(this.forwardLayer.trainable=t),null!=this.backwardLayer&&(this.backwardLayer.trainable=t)}getWeights(){return this.forwardLayer.getWeights().concat(this.backwardLayer.getWeights())}setWeights(t){const e=t.length,n=Math.floor(e/2);this.forwardLayer.setWeights(t.slice(0,n)),this.backwardLayer.setWeights(t.slice(n))}computeOutputShape(t){let e,n,s,i=this.forwardLayer.computeOutputShape(t);return Array.isArray(i)&&Array.isArray(i[0])||(i=[i]),i=i,this.returnState?(s=i.slice(1),e=i[0]):e=i[0],e=e,"concat"===this.mergeMode?(e[e.length-1]*=2,n=[e]):n=null==this.mergeMode?[e,e.slice()]:[e],this.returnState?null==this.mergeMode?n.concat(s).concat(s.slice()):[e].concat(s).concat(s.slice()):yi(n)}apply(t,e){let n=null==e?null:e.initialState,s=null==e?null:e.constants;null==e&&(e={});const i=Nl(t,n,s,this.numConstants);if(t=i.inputs,n=i.initialState,s=i.constants,Array.isArray(t)&&(n=t.slice(1),t=t[0]),(null==n||0===n.length)&&null==s)return super.apply(t,e);const r=[],a=[];if(null!=n){const t=n.length;if(t%2>0)throw new ci("When passing `initialState` to a Bidrectional RNN, the state should be an Array containing the states of the underlying RNNs.");e.initialState=n,r.push(...n);const s=n.map(t=>new ra({shape:t.shape}));this.forwardLayer.stateSpec=s.slice(0,t/2),this.backwardLayer.stateSpec=s.slice(t/2),a.push(...s)}if(null!=s)throw new pi("Support for constants in Bidirectional layers is not implemented yet.");const o=r[0]instanceof aa;for(const t of r)if(t instanceof aa!==o)throw new ci("The initial state of a Bidirectional layer cannot be specified as a mix of symbolic and non-symbolic tensors");if(o){const n=[t].concat(r),s=this.inputSpec.concat(a),i=this.inputSpec;this.inputSpec=s;const o=super.apply(n,e);return this.inputSpec=i,o}return super.apply(t,e)}call(t,n){return e.tidy(()=>{const s=n.initialState;let i,r,a,o;if(null==s)i=this.forwardLayer.call(t,n),r=this.backwardLayer.call(t,n);else{const e=s.slice(0,s.length/2),a=s.slice(s.length/2);i=this.forwardLayer.call(t,Object.assign(n,{initialState:e})),r=this.backwardLayer.call(t,Object.assign(n,{initialState:a}))}return this.returnState&&(Array.isArray(i)&&(a=i.slice(1).concat(r.slice(1))),i=i[0],r=r[0]),this.returnSequences&&(r=e.reverse(r,1)),"concat"===this.mergeMode?o=dr([i,r]):"sum"===this.mergeMode?o=e.add(i,r):"ave"===this.mergeMode?o=e.mul(.5,e.add(i,r)):"mul"===this.mergeMode?o=e.mul(i,r):null==this.mergeMode&&(o=[i,r]),this.returnState?null==this.mergeMode?o.concat(a):[o].concat(a):o})}resetStates(t){this.forwardLayer.resetStates(),this.backwardLayer.resetStates()}build(t){Yi(this.forwardLayer.name,()=>{this.forwardLayer.build(t)}),Yi(this.backwardLayer.name,()=>{this.backwardLayer.build(t)}),this.built=!0}computeMask(t,e){let n;if(Array.isArray(e)&&(e=e[0]),n=this.returnSequences?null==this.mergeMode?[e,e]:e:null==this.mergeMode?[null,null]:null,this.returnState){const t=this.forwardLayer.states.map(t=>null);return Array.isArray(n)?n.concat(t).concat(t):[n].concat(t).concat(t)}return n}get trainableWeights(){return this.forwardLayer.trainableWeights.concat(this.backwardLayer.trainableWeights)}get nonTrainableWeights(){return this.forwardLayer.nonTrainableWeights.concat(this.backwardLayer.nonTrainableWeights)}setFastWeightInitDuringBuild(t){super.setFastWeightInitDuringBuild(t),null!=this.forwardLayer&&this.forwardLayer.setFastWeightInitDuringBuild(t),null!=this.backwardLayer&&this.backwardLayer.setFastWeightInitDuringBuild(t)}getConfig(){const t={mergeMode:this.mergeMode},e=super.getConfig();return Object.assign(t,e),t}static fromConfig(t,e){const n=Ia(e.layer);if(delete e.layer,null!=e.numConstants)throw new pi("Deserialization of a Bidirectional layer with numConstants present is not supported yet.");const s=e;return s.layer=n,new t(s)}}function Lu(t){return new yu(t)}function $u(t){return new ku(t)}function _u(t){return new Su(t)}function Ru(t){return new zu(t)}function Mu(t){return new Du(t)}function Ou(t){return new mu(t)}function Bu(t){return new wu(t)}Fu.className="Bidirectional",e.serialization.registerClass(Fu);const Pu=Ru,Wu=Mu,Uu=Ou,Ku=Bu;var Vu=Object.freeze({__proto__:null,inputLayer:function(t){return new ca(t)},elu:function(t){return new rl(t)},reLU:function(t){return new nl(t)},leakyReLU:function(t){return new sl(t)},prelu:function(t){return new il(t)},softmax:function(t){return new ol(t)},thresholdedReLU:function(t){return new al(t)},conv1d:function(t){return new xl(t)},conv2d:function(t){return new ml(t)},conv2dTranspose:function(t){return new bl(t)},conv3d:function(t){return new yl(t)},separableConv2d:function(t){return new kl(t)},cropping2D:function(t){return new vl(t)},upSampling2d:function(t){return new Sl(t)},depthwiseConv2d:function(t){return new Il(t)},activation:function(t){return new Vl(t)},dense:function(t){return new Ul(t)},dropout:function(t){return new Pl(t)},spatialDropout1d:function(t){return new Wl(t)},flatten:function(t){return new Kl(t)},repeatVector:function(t){return new jl(t)},reshape:function(t){return new ql(t)},permute:function(t){return new Gl(t)},embedding:function(t){return new Jl(t)},add:function(t){return new Xl(t)},average:function(t){return new Ql(t)},concatenate:function(t){return new nu(t)},maximum:function(t){return new tu(t)},minimum:function(t){return new eu(t)},multiply:function(t){return new Yl(t)},dot:function(t){return new iu(t)},batchNormalization:function(t){return new hu(t)},layerNormalization:function(t){return new cu(t)},zeroPadding2d:function(t){return new pu(t)},averagePooling1d:Lu,avgPool1d:function(t){return Lu(t)},avgPooling1d:function(t){return Lu(t)},averagePooling2d:$u,avgPool2d:function(t){return $u(t)},avgPooling2d:function(t){return $u(t)},averagePooling3d:_u,avgPool3d:function(t){return _u(t)},avgPooling3d:function(t){return _u(t)},globalAveragePooling1d:function(t){return new Nu(t)},globalAveragePooling2d:function(t){return new Cu(t)},globalMaxPooling1d:Ru,globalMaxPooling2d:Mu,maxPooling1d:Ou,maxPooling2d:Bu,maxPooling3d:function(t){return new vu(t)},gru:function(t){return new Fl(t)},gruCell:function(t){return new El(t)},lstm:function(t){return new $l(t)},lstmCell:function(t){return new Ll(t)},simpleRNN:function(t){return new Tl(t)},simpleRNNCell:function(t){return new Dl(t)},convLstm2d:function(t){return new Bl(t)},convLstm2dCell:function(t){return new Ol(t)},rnn:function(t){return new Al(t)},stackedRNNCells:function(t){return new _l(t)},bidirectional:function(t){return new Fu(t)},timeDistributed:function(t){return new Eu(t)},globalMaxPool1d:Pu,globalMaxPool2d:Wu,maxPool1d:Uu,maxPool2d:Ku,Layer:ha,RNN:Al,RNNCell:Cl,input:Eo,gaussianNoise:function(t){return new ru(t)},gaussianDropout:function(t){return new au(t)},alphaDropout:function(t){return new ou(t)},masking:function(t){return new Hl(t)}});var ju=Object.freeze({__proto__:null,binaryAccuracy:function(t,e){return _a(t,e)},binaryCrossentropy:function(t,e){return Pa(t,e)},sparseCategoricalAccuracy:function(t,e){return Wa(t,e)},categoricalAccuracy:function(t,e){return Ra(t,e)},categoricalCrossentropy:function(t,e){return Ua(t,e)},precision:function(t,e){return Oa(t,e)},recall:function(t,e){return Ba(t,e)},cosineProximity:function(t,e){return Fa(t,e)},meanAbsoluteError:function(t,e){return Aa(t,e)},meanAbsolutePercentageError:function(t,e){return Ca(t,e)},MAPE:function(t,e){return Ca(t,e)},mape:function(t,e){return Ca(t,e)},meanSquaredError:function(t,e){return za(t,e)},MSE:function(t,e){return za(t,e)},mse:function(t,e){return za(t,e)}}),qu=Object.freeze({__proto__:null,modelFromJSON:async function(t,n){"modelTopology"in t||(t={modelTopology:t});let s=(t=t).modelTopology;null!=s.model_config&&(s=s.model_config);const i=Ia(Qa(s),n);if(null!=t.weightsManifest){const n=await e.io.loadWeights(t.weightsManifest,t.pathPrefix,i.weights.map(t=>t.originalName)),s={};for(const t of i.weights)s[t.originalName]=n[t.originalName];i.loadWeights(s),e.dispose(n)}return i}});var Gu=Object.freeze({__proto__:null,l1l2:function(t){return new Xo(t)},l1:function(t){return Jo(e=t),new Xo({l1:null!=e?e.l1:null,l2:0});var e},l2:function(t){return Jo(e=t),new Xo({l2:null!=e?e.l2:null,l1:0});var e}});class Hu extends ma{constructor(){super(...arguments),this.model=null}setModel(t){if(!(t instanceof Ao))throw new Error("model must be a LayersModel, not some other Container");this.model=t}}function Ju(t,e){return t<e}function Zu(t,e){return t>e}class Xu extends Hu{constructor(t){if(super(),null==t&&(t={}),t.restoreBestWeights)throw new pi("restoreBestWeights = True is not implemented in EarlyStopping yet.");this.monitor=t.monitor||"val_loss",this.minDelta=Math.abs(t.minDelta||0),this.patience=t.patience||0,this.verbose=t.verbose||0,this.mode=t.mode||"auto",this.baseline=t.baseline,-1===["auto","min","max"].indexOf(this.mode)&&(console.warn(`EarlyStopping mode '${this.mode}' is invalid. Falling back to mode 'auto'.`),this.mode="auto"),"min"===this.mode?this.monitorFunc=Ju:"max"===this.mode||-1!==this.monitor.indexOf("acc")?this.monitorFunc=Zu:this.monitorFunc=Ju,this.monitorFunc===Ju&&(this.minDelta*=-1)}async onTrainBegin(t){this.wait=0,this.stoppedEpoch=0,null!=this.baseline?this.best=this.baseline:this.best=this.monitorFunc===Ju?1/0:-1/0}async onEpochEnd(t,e){await da(e);const n=this.getMonitorValue(e);null!=n&&(this.monitorFunc(n-this.minDelta,this.best)?(this.best=n,this.wait=0):(this.wait++,this.wait>=this.patience&&(this.stoppedEpoch=t,this.model.stopTraining=!0)))}async onTrainEnd(t){this.stoppedEpoch>0&&this.verbose&&console.log(`Epoch ${this.stoppedEpoch}: early stopping.`)}getMonitorValue(t){null==t&&(t={});const e=t[this.monitor];return null==e&&console.warn(`Metric for EarlyStopping ${this.monitor} is not available. Available metrics are: `+Object.keys(t)),e}}const Yu={earlyStopping:function(t){return new Xu(t)}};t.Callback=Hu,t.CallbackList=ya,t.CustomCallback=ka,t.EarlyStopping=Xu,t.History=wa,t.InputSpec=ra,t.LayerVariable=na,t.LayersModel=Ao,t.RNN=Al,t.Sequential=To,t.SymbolicTensor=aa,t.callbacks=Yu,t.constraints=Wi,t.initializers=qr,t.input=Eo,t.layers=Vu,t.loadLayersModel=function(t,e){return null==e&&(e={}),Do(t,e)},t.metrics=ju,t.model=function(t){return new Ao(t)},t.models=qu,t.registerCallbackConstructor=function(t,e){va.registerCallbackConstructor(t,e)},t.regularizers=Gu,t.sequential=function(t){return new To(t)},t.version_layers="3.0.0",Object.defineProperty(t,"__esModule",{value:!0})}));
//# sourceMappingURL=tf-layers.es2017.min.js.map
